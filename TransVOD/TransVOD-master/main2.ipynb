{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "\n",
    "import datasets.samplers as samplers\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "#from models import build_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('CUDA_PATH'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.dataset_file, 11111111)\n",
    "        \n",
    "    print(args.dataset_file)\n",
    "    device = torch.device(args.device)\n",
    "    utils.init_distributed_mode(args)\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "    print(args)\n",
    "\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    model, criterion, postprocessors = build_model(args)\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('number of params:', n_parameters)\n",
    "\n",
    "    dataset_train = build_dataset(image_set='train_joint', args=args)\n",
    "    dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                   collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                   pin_memory=True)\n",
    "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    # lr_backbone_names = [\"backbone.0\", \"backbone.neck\", \"input_proj\", \"transformer.encoder\"]\n",
    "    def match_name_keywords(n, name_keywords):\n",
    "        out = False\n",
    "        for b in name_keywords:\n",
    "            if b in n:\n",
    "                out = True\n",
    "                break\n",
    "        return out\n",
    "\n",
    "    for n, p in model_without_ddp.named_parameters():\n",
    "        print(n)\n",
    "\n",
    "    param_dicts = [\n",
    "        {\n",
    "            \"params\":\n",
    "                [p for n, p in model_without_ddp.named_parameters()\n",
    "                 if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "            \"lr\": args.lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "            \"lr\": args.lr_backbone,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "            \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "        }\n",
    "    ]\n",
    "    if args.sgd:\n",
    "        optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                      weight_decay=args.weight_decay)\n",
    "    print(args.lr_drop_epochs)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, args.lr_drop_epochs)\n",
    "\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n",
    "        model_without_ddp = model.module\n",
    "\n",
    "    if args.dataset_file == \"coco_panoptic\":\n",
    "        # We also evaluate AP during panoptic training, on original coco DS\n",
    "        coco_val = datasets.coco.build(\"val\", args)\n",
    "        base_ds = get_coco_api_from_dataset(coco_val)\n",
    "    else:\n",
    "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "\n",
    "        if args.eval:\n",
    "            missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "        else:\n",
    "            tmp_dict = model_without_ddp.state_dict().copy()\n",
    "            if args.coco_pretrain: # singleBaseline\n",
    "                for k, v in checkpoint['model'].items():\n",
    "                    if ('class_embed' not in k) :\n",
    "                        tmp_dict[k] = v \n",
    "                    else:\n",
    "                        print('k', k)\n",
    "            \"\"\"\n",
    "            else:\n",
    "                tmp_dict = checkpoint['model']\n",
    "                for name, param in model_without_ddp.named_parameters():\n",
    "\n",
    "\t                if ('temp' in name):\n",
    "\t                    param.requires_grad = True\n",
    "                    else:\n",
    "\t                    param.requires_grad = False\n",
    "        \n",
    "            missing_keys, unexpected_keys = model_without_ddp.load_state_dict(tmp_dict, strict=False)\n",
    "            \"\"\"\n",
    "        unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "        if len(missing_keys) > 0:\n",
    "            print('Missing Keys: {}'.format(missing_keys))\n",
    "        if len(unexpected_keys) > 0:\n",
    "            print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                              data_loader_val, base_ds, device, args.output_dir)\n",
    "        if args.output_dir:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "        return\n",
    "\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            sampler_train.set_epoch(epoch)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)\n",
    "        lr_scheduler.step()\n",
    "        print('args.output_dir', args.output_dir)\n",
    "        if args.output_dir:\n",
    "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "            # extra checkpoint before LR drop and every 5 epochs\n",
    "            # if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 1 == 0:\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                utils.save_on_master({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'args': args,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        #test_stats, coco_evaluator = evaluate(\n",
    "         #   model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "        #)\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     'epoch': epoch,\n",
    "                     'n_parameters': n_parameters}\n",
    "\n",
    "        if args.output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
