{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "#Model\n",
    "from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "\n",
    "import MultiScaleDeformableAttention as MSDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "args=dict(\n",
    "lr= 2e-4, #learning rate\n",
    "lr_backbone_names=[\"backbone.0\"], #Backbone name such as resnet\n",
    "lr_backbone=2e-5, #learning rate for backbone\n",
    "lr_linear_proj_names=['reference_points', 'sampling_offsets'],\n",
    "lr_linear_proj_mult=0.1,\n",
    "batch_size=1, #video per batch\n",
    "weight_decay=1e-4, #\n",
    "epochs=7,\n",
    "lr_drop=5,\n",
    "lr_drop_epochs=[5,6],    \n",
    "clip_max_norm=0.1, #gradient clipping max norm\n",
    "\n",
    "#parameters of model\n",
    "num_ref_frames=3,\n",
    "num_frames=12,\n",
    "\n",
    "sgd=False,\n",
    "gap=2,\n",
    "\n",
    "#Variants of Deformable DETR\n",
    "with_box_refine=True,\n",
    "two_stage=False,\n",
    "\n",
    "#pretrained model\n",
    "frozen_weights=None, #use pretrained model to fine tune it, only mask head will be trained, give the path to the model\n",
    "pretrained=None, #if resume from a checkpoint\n",
    "\n",
    "#Backbone\n",
    "backbone='swin_b_p4w7',#Name of the convolutional backbone to use\n",
    "dilation=True,#If true, we replace stride with dilation in the last convolutional block (DC5)\n",
    "position_embedding='sine',#choices=('sine', 'learned'), Type of positional embedding to use on top of the image features\n",
    "position_embedding_scale=2 * math.pi, #position / size * scale\n",
    "num_feature_levels=1, #number of feature levels\n",
    "checkpoint=False, #store a checkpoint if true\n",
    "\n",
    "#Transformers \n",
    "enc_layers=6, #Number of encoding layers in the transformer\n",
    "dec_layers=6, #Number of decoding layers in the transformer\n",
    "dim_feedforward=1024, #Intermediate size of the feedforward layers in the transformer blocks\n",
    "hidden_dim=256, #Size of the embeddings (dimension of the transformer)\n",
    "dropout=0.1, #Dropout applied in the transformer\n",
    "nheads=8, #Number of attention heads inside the transformer's attentions\n",
    "num_queries=100, #Number of query slots\n",
    "dec_n_points=4,\n",
    "enc_n_points=4,\n",
    "n_temporal_decoder_layers=1,\n",
    "interval1=20,\n",
    "interval2=60,\n",
    "fixed_pretrained_model=False,\n",
    "is_shuffle=False,\n",
    "\n",
    "# * Segmentation\n",
    "masks=False, #Train segmentation head if the flag is provided\n",
    "\n",
    "# Loss\n",
    "aux_loss=False,\n",
    "\n",
    "# * Matcher\n",
    "set_cost_class=2, #Class coefficient in the matching cost\n",
    "set_cost_bbox=5, #L1 box coefficient in the matching cost\n",
    "set_cost_giou=2, #giou box coefficient in the matching cost\n",
    "\n",
    "# * Loss coefficients\n",
    "mask_loss_coef=1,\n",
    "dice_loss_coef=1,\n",
    "cls_loss_coef=2,\n",
    "bbox_loss_coef=5,\n",
    "giou_loss_coef=2,\n",
    "focal_alpha=0.25,\n",
    "\n",
    "# dataset parameters\n",
    "dataset_file='vid_multi',\n",
    "coco_path='./data/coco',\n",
    "vid_path='./data/vid',\n",
    "coco_pretrain=False,\n",
    "coco_panoptic_path=\"\",\n",
    "remove_difficult=False,\n",
    "\n",
    "output_dir='Final_output', #path where to save, empty for no saving\n",
    "device='cuda', #device to use for training / testing\n",
    "seed=42,\n",
    "resume='./exps/exps_single/swinb_88.3/checkpoint0006.pth', #resume from checkpoint\n",
    "start_epoch=0, #metavar='N', start epoch)\n",
    "eval=False,\n",
    "num_workers=0,\n",
    "cache_mode=False) #whether to cache images on memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Multiscale DeformableAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSDeformAttnFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step):\n",
    "        ctx.im2col_step = im2col_step\n",
    "        output = MSDA.ms_deform_attn_forward(\n",
    "            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, ctx.im2col_step)\n",
    "        ctx.save_for_backward(value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_output):\n",
    "        value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights = ctx.saved_tensors\n",
    "        grad_value, grad_sampling_loc, grad_attn_weight = \\\n",
    "            MSDA.ms_deform_attn_backward(\n",
    "                value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, grad_output, ctx.im2col_step)\n",
    "\n",
    "        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n",
    "\n",
    "\n",
    "def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n",
    "    # for debug and test only,\n",
    "    # need to use cuda version instead\n",
    "    N_, S_, M_, D_ = value.shape\n",
    "    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n",
    "    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n",
    "    sampling_grids = 2 * sampling_locations - 1\n",
    "    sampling_value_list = []\n",
    "    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n",
    "        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_\n",
    "        value_l_ = value_list[lid_].flatten(2).transpose(1, 2).reshape(N_*M_, D_, H_, W_)\n",
    "        # N_, Lq_, M_, P_, 2 -> N_, M_, Lq_, P_, 2 -> N_*M_, Lq_, P_, 2\n",
    "        sampling_grid_l_ = sampling_grids[:, :, :, lid_].transpose(1, 2).flatten(0, 1)\n",
    "        # N_*M_, D_, Lq_, P_\n",
    "        sampling_value_l_ = F.grid_sample(value_l_, sampling_grid_l_,\n",
    "                                          mode='bilinear', padding_mode='zeros', align_corners=False)\n",
    "        sampling_value_list.append(sampling_value_l_)\n",
    "    # (N_, Lq_, M_, L_, P_) -> (N_, M_, Lq_, L_, P_) -> (N_, M_, 1, Lq_, L_*P_)\n",
    "    attention_weights = attention_weights.transpose(1, 2).reshape(N_*M_, 1, Lq_, L_*P_)\n",
    "    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights).sum(-1).view(N_, M_*D_, Lq_)\n",
    "    return output.transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "def _is_power_of_2(n):\n",
    "    if (not isinstance(n, int)) or (n < 0):\n",
    "        raise ValueError(\"invalid input for _is_power_of_2: {} (type: {})\".format(n, type(n)))\n",
    "    return (n & (n-1) == 0) and n != 0\n",
    "\n",
    "class MSDeformAttn(nn.Module):\n",
    "    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n",
    "        \"\"\"\n",
    "        Multi-Scale Deformable Attention Module\n",
    "        :param d_model      hidden dimension\n",
    "        :param n_levels     number of feature levels\n",
    "        :param n_heads      number of attention heads\n",
    "        :param n_points     number of sampling points per attention head per feature level\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if d_model % n_heads != 0:\n",
    "            raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n",
    "        _d_per_head = d_model // n_heads\n",
    "        # you'd better set _d_per_head to a power of 2 which is more efficient in our CUDA implementation\n",
    "        if not _is_power_of_2(_d_per_head):\n",
    "            warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 \"\n",
    "                          \"which is more efficient in our CUDA implementation.\")\n",
    "\n",
    "        self.im2col_step = 64\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_levels = n_levels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_points = n_points\n",
    "\n",
    "        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n",
    "        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.output_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        constant_(self.sampling_offsets.weight.data, 0.)\n",
    "        thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n",
    "        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n",
    "        grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n",
    "        for i in range(self.n_points):\n",
    "            grid_init[:, :, i, :] *= i + 1\n",
    "        with torch.no_grad():\n",
    "            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n",
    "        constant_(self.attention_weights.weight.data, 0.)\n",
    "        constant_(self.attention_weights.bias.data, 0.)\n",
    "        xavier_uniform_(self.value_proj.weight.data)\n",
    "        constant_(self.value_proj.bias.data, 0.)\n",
    "        xavier_uniform_(self.output_proj.weight.data)\n",
    "        constant_(self.output_proj.bias.data, 0.)\n",
    "\n",
    "    def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n",
    "        \"\"\"\n",
    "        :param query                       (N, Length_{query}, C)\n",
    "        :param reference_points            (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0), bottom-right (1, 1), including padding area\n",
    "                                        or (N, Length_{query}, n_levels, 4), add additional (w, h) to form reference boxes\n",
    "        :param input_flatten               (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l, C)\n",
    "        :param input_spatial_shapes        (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n",
    "        :param input_level_start_index     (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1, H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\n",
    "        :param input_padding_mask          (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l), True for padding elements, False for non-padding elements\n",
    "\n",
    "        :return output                     (N, Length_{query}, C)\n",
    "        \"\"\"\n",
    "        N, Len_q, _ = query.shape\n",
    "        N, Len_in, _ = input_flatten.shape\n",
    "        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
    "\n",
    "        value = self.value_proj(input_flatten)\n",
    "        if input_padding_mask is not None:\n",
    "            value = value.masked_fill(input_padding_mask[..., None], float(0))\n",
    "        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n",
    "        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n",
    "        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n",
    "        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n",
    "        # N, Len_q, n_heads, n_levels, n_points, 2\n",
    "        if reference_points.shape[-1] == 2:\n",
    "            offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n",
    "            # print(\"shape122\", offset_normalizer.shape)\n",
    "            # print(sampling_offsets.shape)\n",
    "            sampling_locations = reference_points[:, :, None, :, None, :] \\\n",
    "                                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n",
    "        elif reference_points.shape[-1] == 4:\n",
    "            sampling_locations = reference_points[:, :, None, :, None, :2] \\\n",
    "                                 + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n",
    "        output = MSDeformAttnFunction.apply(\n",
    "            value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-5):\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1/x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part 1 : Deformable DETR (DEtection TRansformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for model part 1, deformable transformer multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableTransformer(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1,\n",
    "                 activation=\"relu\", return_intermediate_dec=False,\n",
    "                 num_feature_levels=4, dec_n_points=4,  enc_n_points=4,\n",
    "                 two_stage=False, two_stage_num_proposals=300, n_temporal_decoder_layers = 1,\n",
    "                 num_frames= 3, fixed_pretrained_model = False, args=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.two_stage = two_stage\n",
    "        self.num_frames = num_frames\n",
    "        self.two_stage_num_proposals = two_stage_num_proposals\n",
    "        self.fixed_pretrained_model = fixed_pretrained_model\n",
    "        self.n_temporal_query_layers = 3\n",
    "\n",
    "        encoder_layer = DeformableTransformerEncoderLayer(d_model, dim_feedforward,\n",
    "                                                          dropout, activation,\n",
    "                                                          num_feature_levels, nhead, enc_n_points)\n",
    "        self.encoder = DeformableTransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "        decoder_layer = DeformableTransformerDecoderLayer(d_model, dim_feedforward,\n",
    "                                                          dropout, activation,\n",
    "                                                          num_feature_levels, nhead, dec_n_points)\n",
    "        self.decoder = DeformableTransformerDecoder(decoder_layer, num_decoder_layers, return_intermediate_dec)\n",
    "\n",
    "        self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))\n",
    "                                                          \n",
    "        self.temporal_query_layer1 = TemporalQueryEncoderLayer(d_model, dim_feedforward, dropout, activation, nhead)\n",
    "        self.temporal_query_layer2 = TemporalQueryEncoderLayer(d_model, dim_feedforward, dropout, activation, nhead)\n",
    "        self.temporal_query_layer3 = TemporalQueryEncoderLayer(d_model, dim_feedforward, dropout, activation, nhead)\n",
    "        # self.temporal_query_encoder = TemporalQueryEncoder(self.temporal_query_layer, self.n_temporal_query_layers) \n",
    "        self.temporal_decoder1 = TemporalDeformableTransformerDecoder(decoder_layer, n_temporal_decoder_layers, False)\n",
    "        self.temporal_decoder2 = TemporalDeformableTransformerDecoder(decoder_layer, n_temporal_decoder_layers, False)\n",
    "        self.temporal_decoder3 = TemporalDeformableTransformerDecoder(decoder_layer, n_temporal_decoder_layers, False)\n",
    "\n",
    "        if two_stage:\n",
    "            self.enc_output = nn.Linear(d_model, d_model)\n",
    "            self.enc_output_norm = nn.LayerNorm(d_model)\n",
    "            self.pos_trans = nn.Linear(d_model * 2, d_model * 2)\n",
    "            self.pos_trans_norm = nn.LayerNorm(d_model * 2)\n",
    "        else:\n",
    "            self.reference_points = nn.Linear(d_model, 2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, MSDeformAttn):\n",
    "                m._reset_parameters()\n",
    "        if not self.two_stage:\n",
    "            xavier_uniform_(self.reference_points.weight.data, gain=1.0)\n",
    "            constant_(self.reference_points.bias.data, 0.)\n",
    "        normal_(self.level_embed)\n",
    "\n",
    "    def get_proposal_pos_embed(self, proposals):\n",
    "        num_pos_feats = 128\n",
    "        temperature = 10000\n",
    "        scale = 2 * math.pi\n",
    "\n",
    "        dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)\n",
    "        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n",
    "        # N, L, 4\n",
    "        proposals = proposals.sigmoid() * scale\n",
    "        # N, L, 4, 128\n",
    "        pos = proposals[:, :, :, None] / dim_t\n",
    "        # N, L, 4, 64, 2\n",
    "        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()), dim=4).flatten(2)\n",
    "        return pos\n",
    "\n",
    "    def gen_encoder_output_proposals(self, memory, memory_padding_mask, spatial_shapes):\n",
    "        N_, S_, C_ = memory.shape\n",
    "        base_scale = 4.0\n",
    "        proposals = []\n",
    "        _cur = 0\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, 1)\n",
    "            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)\n",
    "            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)\n",
    "\n",
    "            grid_y, grid_x = torch.meshgrid(torch.linspace(0, H_ - 1, H_, dtype=torch.float32, device=memory.device),\n",
    "                                            torch.linspace(0, W_ - 1, W_, dtype=torch.float32, device=memory.device))\n",
    "            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)\n",
    "\n",
    "            scale = torch.cat([valid_W.unsqueeze(-1), valid_H.unsqueeze(-1)], 1).view(N_, 1, 1, 2)\n",
    "            grid = (grid.unsqueeze(0).expand(N_, -1, -1, -1) + 0.5) / scale\n",
    "            wh = torch.ones_like(grid) * 0.05 * (2.0 ** lvl)\n",
    "            proposal = torch.cat((grid, wh), -1).view(N_, -1, 4)\n",
    "            proposals.append(proposal)\n",
    "            _cur += (H_ * W_)\n",
    "        output_proposals = torch.cat(proposals, 1)\n",
    "        output_proposals_valid = ((output_proposals > 0.01) & (output_proposals < 0.99)).all(-1, keepdim=True)\n",
    "        output_proposals = torch.log(output_proposals / (1 - output_proposals))\n",
    "        output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(-1), float('inf'))\n",
    "        output_proposals = output_proposals.masked_fill(~output_proposals_valid, float('inf'))\n",
    "\n",
    "        output_memory = memory\n",
    "        output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(-1), float(0))\n",
    "        output_memory = output_memory.masked_fill(~output_proposals_valid, float(0))\n",
    "        output_memory = self.enc_output_norm(self.enc_output(output_memory))\n",
    "        return output_memory, output_proposals\n",
    "\n",
    "    def get_valid_ratio(self, mask):\n",
    "        _, H, W = mask.shape\n",
    "        valid_H = torch.sum(~mask[:, :, 0], 1)\n",
    "        valid_W = torch.sum(~mask[:, 0, :], 1)\n",
    "        valid_ratio_h = valid_H.float() / H\n",
    "        valid_ratio_w = valid_W.float() / W\n",
    "        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n",
    "        return valid_ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
    "        reference_points_list = []\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "\n",
    "            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
    "                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n",
    "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
    "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
    "            ref = torch.stack((ref_x, ref_y), -1)\n",
    "            reference_points_list.append(ref)\n",
    "        reference_points = torch.cat(reference_points_list, 1)\n",
    "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
    "        return reference_points\n",
    "\n",
    "    def forward(self, srcs, masks, pos_embeds, query_embed=None, class_embed = None, temp_class_embed_list = None, temp_bbox_embed_list = None ):\n",
    "        assert self.two_stage or query_embed is not None\n",
    "\n",
    "        # prepare input for encoder\n",
    "        src_flatten = []\n",
    "        mask_flatten = []\n",
    "        lvl_pos_embed_flatten = []\n",
    "        spatial_shapes = []\n",
    "        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):\n",
    "            bs, c, h, w = src.shape\n",
    "            spatial_shape = (h, w)\n",
    "            spatial_shapes.append(spatial_shape)\n",
    "            src = src.flatten(2).transpose(1, 2)\n",
    "            mask = mask.flatten(1)\n",
    "\n",
    "            pos_embed = pos_embed.flatten(2).transpose(1, 2) \n",
    "            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)\n",
    "            lvl_pos_embed_flatten.append(lvl_pos_embed)\n",
    "            src_flatten.append(src)\n",
    "            mask_flatten.append(mask)\n",
    "        src_flatten = torch.cat(src_flatten, 1) \n",
    "        mask_flatten = torch.cat(mask_flatten, 1)\n",
    "        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n",
    "        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
    "        level_start_index = torch.cat((spatial_shapes.new_zeros((1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))\n",
    "        valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n",
    "\n",
    "        # encoder\n",
    "        memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)\n",
    "\n",
    "        # prepare input for decoder:\n",
    "        bs, _, c = memory.shape\n",
    "        if self.two_stage:\n",
    "            output_memory, output_proposals = self.gen_encoder_output_proposals(memory, mask_flatten, spatial_shapes)\n",
    "\n",
    "            # hack implementation for two-stage Deformable DETR\n",
    "            enc_outputs_class = self.decoder.class_embed[self.decoder.num_layers](output_memory)\n",
    "            enc_outputs_coord_unact = self.decoder.bbox_embed[self.decoder.num_layers](output_memory) + output_proposals\n",
    "\n",
    "            topk = self.two_stage_num_proposals\n",
    "            topk_proposals = torch.topk(enc_outputs_class[..., 0], topk, dim=1)[1]\n",
    "            topk_coords_unact = torch.gather(enc_outputs_coord_unact, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4))\n",
    "            topk_coords_unact = topk_coords_unact.detach()\n",
    "            reference_points = topk_coords_unact.sigmoid()\n",
    "            init_reference_out = reference_points\n",
    "            pos_trans_out = self.pos_trans_norm(self.pos_trans(self.get_proposal_pos_embed(topk_coords_unact)))\n",
    "            query_embed, tgt = torch.split(pos_trans_out, c, dim=2)\n",
    "        else:\n",
    "            query_embed, tgt = torch.split(query_embed, c, dim=1)\n",
    "            query_embed = query_embed.unsqueeze(0).expand(bs, -1, -1)\n",
    "            tgt = tgt.unsqueeze(0).expand(bs, -1, -1)\n",
    "            reference_points = self.reference_points(query_embed).sigmoid()\n",
    "            init_reference_out = reference_points\n",
    "\n",
    "        # decoder\n",
    "        hs, inter_references = self.decoder(tgt, reference_points, memory,\n",
    "                                            spatial_shapes, level_start_index, valid_ratios, query_embed, mask_flatten)\n",
    "\n",
    "        inter_references_out = inter_references\n",
    "        if self.two_stage:\n",
    "            return hs, init_reference_out, inter_references_out, enc_outputs_class, enc_outputs_coord_unact\n",
    "        \n",
    "        if self.fixed_pretrained_model:\n",
    "            print(\"fixed\")\n",
    "            memory = memory.detach()\n",
    "            hs = hs.detach()\n",
    "            inter_references = inter_references.detach()\n",
    "\n",
    "        self.SeqHQM = True\n",
    "        # Implementation of Sequential Hard Query Mining (SeqHQM)\n",
    "        if self.SeqHQM:\n",
    "            out = {}\n",
    "            last_reference_out  = inter_references_out[-1]\n",
    "            #print(\"11\", last_reference_out.shape)\n",
    "            last_hs = hs[-1]\n",
    "            new_hs, last_reference_out = update_QFH(class_embed, last_hs, last_reference_out, 80)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames, dim = 0)\n",
    "            new_hs = torch.cat(new_hs_list, 1) # 1, 300 * 4 , 128\n",
    "            new_hs = self.temporal_query_layer1(new_hs, new_hs)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames , dim = 1)\n",
    "            new_hs = torch.cat(new_hs_list , 0) # 4, 300, 128\n",
    "            new_hs, last_references_out = self.temporal_decoder1(new_hs, last_reference_out, memory,\n",
    "                                                                spatial_shapes, level_start_index, valid_ratios, None, None)\n",
    "            \n",
    "            reference1 = inverse_sigmoid(last_references_out)\n",
    "            output_class1 = temp_class_embed_list[0](new_hs)\n",
    "            tmp1 = temp_bbox_embed_list[0](new_hs)\n",
    "            if reference1.shape[-1] == 4:\n",
    "                tmp1 += reference1\n",
    "            else:\n",
    "                assert reference1.shape[-1] == 2\n",
    "                tmp1[..., :2] += reference1\n",
    "            output_coord1 = tmp1.sigmoid()\n",
    "            out['aux_outputs'] = [{\"pred_logits\":output_class1, \"pred_boxes\":output_coord1}]\n",
    "\n",
    "            # loss: new_hs [4, 50, 128] \n",
    "            # self.temp_class_embed\n",
    "            new_hs, last_reference_out = update_QFH(temp_class_embed_list[0], new_hs, last_reference_out, 50)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames, dim = 0)\n",
    "            new_hs = torch.cat(new_hs_list, 1) #1, 30 * 4 ,128\n",
    "            new_hs = self.temporal_query_layer2(new_hs, new_hs)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames , dim = 1)\n",
    "            new_hs = torch.cat(new_hs_list , 0)\n",
    "            new_hs, last_references_out = self.temporal_decoder2(new_hs, last_reference_out, memory,\n",
    "                                                                spatial_shapes, level_start_index, valid_ratios, None, None)\n",
    "            \n",
    "            \n",
    "            reference2 = inverse_sigmoid(last_references_out)\n",
    "            output_class2 = temp_class_embed_list[1](new_hs)\n",
    "            tmp2 = temp_bbox_embed_list[1](new_hs)\n",
    "            if reference2.shape[-1] == 4:\n",
    "                tmp2 += reference2\n",
    "            else:\n",
    "                assert reference2.shape[-1] == 2\n",
    "                tmp2[..., :2] += reference2\n",
    "            output_coord2 = tmp2.sigmoid()\n",
    "            out['aux_outputs'].append({\"pred_logits\":output_class2, \"pred_boxes\":output_coord2})\n",
    "            # loss: [4, 30, 128]\n",
    "\n",
    "            new_hs, last_reference_out = update_QFH(temp_class_embed_list[1], new_hs, last_reference_out, 30)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames, dim = 0)\n",
    "            new_hs = torch.cat(new_hs_list, 1)\n",
    "            new_hs = self.temporal_query_layer3(new_hs, new_hs)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames , dim = 1)\n",
    "            new_hs = torch.cat(new_hs_list , 0)\n",
    "            final_hs, final_references_out = self.temporal_decoder3(new_hs, last_reference_out, memory,\n",
    "                                            spatial_shapes, level_start_index, valid_ratios, None, None)\n",
    "\n",
    "            return hs, init_reference_out, inter_references_out, None, None, final_hs, final_references_out, out\n",
    "\n",
    "            \n",
    "        return hs[:,0:1,:,:], init_reference_out[0:1], inter_references_out[:,0:1,:,:], None, None, final_hs, final_references_out\n",
    "\n",
    "def update_QFH(class_embed, hs, last_reference_out, topk):\n",
    "    num_frames = hs.shape[0]\n",
    "    hs_logits = class_embed(hs)\n",
    "    prob = hs_logits.sigmoid()\n",
    "    prob = torch.max(prob, dim = -1)\n",
    "    topk_values, topk_indexes = torch.topk(prob[0], topk, dim = 1)\n",
    "    hs = torch.gather(hs, 1, topk_indexes.unsqueeze(-1).repeat(1,1,hs.shape[-1]))\n",
    "    last_reference_out = torch.gather(last_reference_out, 1, topk_indexes.unsqueeze(-1).repeat(1,1,last_reference_out.shape[-1]))\n",
    "    return hs, last_reference_out\n",
    "\n",
    "class TemporalQueryEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 256, d_ffn = 1024, dropout=0.1, activation=\"relu\", n_heads = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        # self attention \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # cross attention \n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # ffn \n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model) \n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "    \n",
    "    def forward(self, query , ref_query, query_pos = None, ref_query_pos = None):\n",
    "        # self.attention\n",
    "        q = k = self.with_pos_embed(query, query_pos)\n",
    "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), query.transpose(0, 1))[0].transpose(0, 1)\n",
    "        tgt = query + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # cross attention \n",
    "        tgt2 = self.cross_attn(\n",
    "            self.with_pos_embed(tgt, query_pos).transpose(0, 1), \n",
    "            self.with_pos_embed(ref_query, ref_query_pos).transpose(0, 1),\n",
    "            ref_query.transpose(0,1)\n",
    "        )[0].transpose(0,1)\n",
    "\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "class TemporalQueryEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, query , ref_query, query_pos = None, ref_query_pos = None):\n",
    "        output = query\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            output = layer(output, ref_query, query_pos, ref_query_pos)\n",
    "        return output\n",
    "\n",
    "class TemporalDeformableTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 256, d_ffn=1024, dropout=0.1, \n",
    "                 activation='relu', num_ref_frames = 3, n_heads = 8, n_points=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # cross attention \n",
    "        self.cross_attn = MSDeformAttn(d_model, num_ref_frames, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, query_pos, reference_points, src, src_spatial_shapes, frame_start_index, src_padding_mask=None):\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1))[0].transpose(0, 1)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "    \n",
    "        # cross attention\n",
    "        tgt2 = self.cross_attn(self.with_pos_embed(tgt, query_pos),\n",
    "                               reference_points,\n",
    "                               src, src_spatial_shapes, frame_start_index, src_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class DeformableTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model=256, d_ffn=1024,\n",
    "                 dropout=0.1, activation=\"relu\",\n",
    "                 n_levels=4, n_heads=8, n_points=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, src):\n",
    "        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout3(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask=None):\n",
    "        # self attention\n",
    "        src2 = self.self_attn(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # ffn\n",
    "        src = self.forward_ffn(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class DeformableTransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
    "        reference_points_list = []\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "\n",
    "            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
    "                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n",
    "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
    "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
    "            ref = torch.stack((ref_x, ref_y), -1)\n",
    "            reference_points_list.append(ref)\n",
    "        reference_points = torch.cat(reference_points_list, 1)\n",
    "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
    "        return reference_points\n",
    "\n",
    "    def forward(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None):\n",
    "        output = src\n",
    "        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            # print(str(_) + \"deformable_transformer_\", [reference_points.shape, level_start_index, spatial_shapes] )\n",
    "            output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class DeformableTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=256, d_ffn=1024,\n",
    "                 dropout=0.1, activation=\"relu\",\n",
    "                 n_levels=4, n_heads=8, n_points=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # cross attention\n",
    "        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, query_pos, reference_points, src, src_spatial_shapes, level_start_index, src_padding_mask=None):\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        # \n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"q tran shape\", q.transpose(0,1).shape)\n",
    "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1))[0].transpose(0, 1)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # cross attention\n",
    "        # print(\"tgt\", tgt.shape)\n",
    "        # print(\"ref\", reference_points.shape)\n",
    "        # print(\"src_spatial_shapes\", src_spatial_shapes)\n",
    "        # print(\"mask\", src_padding_mask)\n",
    "        tgt2 = self.cross_attn(self.with_pos_embed(tgt, query_pos),\n",
    "                               reference_points,\n",
    "                               src, src_spatial_shapes, level_start_index, src_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class TemporalDeformableTransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.return_intermediate = return_intermediate\n",
    "        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n",
    "        self.bbox_embed = None\n",
    "        self.class_embed = None\n",
    "\n",
    "    def forward(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,\n",
    "                query_pos=None, src_padding_mask=None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        intermediate_reference_points = []\n",
    "        for lid, layer in enumerate(self.layers):\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            if reference_points.shape[-1] == 4:\n",
    "                reference_points_input = reference_points[:, :, None] \\\n",
    "                                         * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]\n",
    "            else:\n",
    "                assert reference_points.shape[-1] == 2\n",
    "                reference_points_input = reference_points[:, :, None] * src_valid_ratios[:, None]\n",
    "            output = layer(output, query_pos, reference_points_input, src, src_spatial_shapes, src_level_start_index, src_padding_mask)\n",
    "\n",
    "            # hack implementation for iterative bounding box refinement\n",
    "            self.bbox_embed = None\n",
    "            if self.bbox_embed is not None:\n",
    "                tmp = self.bbox_embed[lid](output)\n",
    "                if reference_points.shape[-1] == 4:\n",
    "                    new_reference_points = tmp + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                else:\n",
    "                    assert reference_points.shape[-1] == 2\n",
    "                    new_reference_points = tmp\n",
    "                    new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                reference_points = new_reference_points.detach()\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "                intermediate_reference_points.append(reference_points)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n",
    "\n",
    "        return output, reference_points  \n",
    "\n",
    "class DeformableTransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.return_intermediate = return_intermediate\n",
    "        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n",
    "        self.bbox_embed = None\n",
    "        self.class_embed = None\n",
    "\n",
    "    def forward(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,\n",
    "                query_pos=None, src_padding_mask=None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        intermediate_reference_points = []\n",
    "        for lid, layer in enumerate(self.layers):\n",
    "            # print(\"Decoder refer\", reference_points.shape)\n",
    "            # print(reference_points)\n",
    "            # print(\"src_valid_ratios\", src_valid_ratios)\n",
    "            if reference_points.shape[-1] == 4:\n",
    "                reference_points_input = reference_points[:, :, None] \\\n",
    "                                         * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]\n",
    "            else:\n",
    "                assert reference_points.shape[-1] == 2\n",
    "                reference_points_input = reference_points[:, :, None] * src_valid_ratios[:, None]\n",
    "            # print(\"reference_points_input\", reference_points_input.shape)\n",
    "            output = layer(output, query_pos, reference_points_input, src, src_spatial_shapes, src_level_start_index, src_padding_mask)\n",
    "\n",
    "            # hack implementation for iterative bounding box refinement\n",
    "            if self.bbox_embed is not None:\n",
    "                tmp = self.bbox_embed[lid](output)\n",
    "                if reference_points.shape[-1] == 4:\n",
    "                    new_reference_points = tmp + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                else:\n",
    "                    assert reference_points.shape[-1] == 2\n",
    "                    new_reference_points = tmp\n",
    "                    new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                reference_points = new_reference_points.detach()\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "                intermediate_reference_points.append(reference_points)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n",
    "\n",
    "        return output, reference_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deforamble_transformer(args):\n",
    "    return DeformableTransformer(\n",
    "        d_model=args[\"hidden_dim\"],\n",
    "        nhead=args[\"nheads\"],\n",
    "        num_encoder_layers=args[\"enc_layers\"],\n",
    "        num_decoder_layers=args[\"dec_layers\"],\n",
    "        dim_feedforward=args[\"dim_feedforward\"],\n",
    "        dropout=args[\"dropout\"],\n",
    "        activation=\"relu\",\n",
    "        return_intermediate_dec=True,\n",
    "        num_feature_levels=args[\"num_feature_levels\"],\n",
    "        dec_n_points=args[\"dec_n_points\"],\n",
    "        enc_n_points=args[\"enc_n_points\"],\n",
    "        two_stage=args[\"two_stage\"],\n",
    "        two_stage_num_proposals=args[\"num_queries\"],\n",
    "        n_temporal_decoder_layers = args[\"n_temporal_decoder_layers\"], \n",
    "        num_frames = args[\"num_frames\"],\n",
    "        fixed_pretrained_model = args[\"fixed_pretrained_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Position and Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone and Position encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.misc\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device, non_blocking=False):\n",
    "      \n",
    "        cast_tensor = self.tensors.to(device, non_blocking=non_blocking)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device, non_blocking=non_blocking)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def record_stream(self, *args, **kwargs):\n",
    "        self.tensors.record_stream(*args, **kwargs)\n",
    "        if self.mask is not None:\n",
    "            self.mask.record_stream(*args, **kwargs)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        mask = tensor_list.mask\n",
    "        assert mask is not None\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = (y_embed - 0.5) / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = (x_embed - 0.5) / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos\n",
    "\n",
    "\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=256):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "def build_position_encoding(args):\n",
    "    N_steps = args[\"hidden_dim\"] // 2\n",
    "    if args[\"position_embedding\"] in ('v2', 'sine'):\n",
    "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
    "    elif args[\"position_embedding\"] in ('v3', 'learned'):\n",
    "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
    "    else:\n",
    "        error=args[\"position_embedding\"]\n",
    "        raise ValueError(f\"not supported {error}\")\n",
    "\n",
    "    return position_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, eps=1e-5):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "        self.eps = eps\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = self.eps\n",
    "        scale = w * (rv + eps).rsqrt()\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "\n",
    "class BackboneBase(nn.Module):\n",
    "    # backbone, 是否训练backbone, 是否返回中间值\n",
    "    def __init__(self, backbone: nn.Module, train_backbone: bool, return_interm_layers: bool):\n",
    "        super().__init__()\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "                parameter.requires_grad_(False)\n",
    "        if return_interm_layers:\n",
    "            # return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
    "            return_layers = {\"layer2\": \"0\", \"layer3\": \"1\", \"layer4\": \"2\"}\n",
    "            self.strides = [8, 16, 32]\n",
    "            self.num_channels = [512, 1024, 2048]\n",
    "        else:\n",
    "            return_layers = {'layer4': \"0\"}\n",
    "            self.strides = [32]\n",
    "            self.num_channels = [2048]\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        # tensor list \n",
    "        xs = self.body(tensor_list.tensors)\n",
    "        out: Dict[str, NestedTensor] = {}\n",
    "        for name, x in xs.items():\n",
    "            m = tensor_list.mask\n",
    "            assert m is not None\n",
    "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "            out[name] = NestedTensor(x, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Backbone(BackboneBase):\n",
    "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_backbone: bool,\n",
    "                 return_interm_layers: bool,\n",
    "                 dilation: bool):\n",
    "        norm_layer = FrozenBatchNorm2d\n",
    "        backbone = getattr(torchvision.models, name)(\n",
    "            replace_stride_with_dilation=[False, False, dilation],\n",
    "            pretrained=True, norm_layer=norm_layer)\n",
    "        assert name not in ('resnet18', 'resnet34'), \"number of channels are hard coded\"\n",
    "        super().__init__(backbone, train_backbone, return_interm_layers)\n",
    "        if dilation:\n",
    "            self.strides[-1] = self.strides[-1] // 2\n",
    "\n",
    "\n",
    "class Joiner(nn.Sequential):\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "        self.strides = backbone.strides\n",
    "        self.num_channels = backbone.num_channels\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self[0](tensor_list)\n",
    "        out: List[NestedTensor] = []\n",
    "        pos = []\n",
    "        for name, x in sorted(xs.items()):\n",
    "            out.append(x)\n",
    "\n",
    "        # position encoding\n",
    "        for x in out:\n",
    "            pos.append(self[1](x).to(x.tensors.dtype))\n",
    "        \n",
    "        return out, pos\n",
    "\n",
    "def build_backbone(args):\n",
    "    position_embedding = build_position_encoding(args)\n",
    "    train_backbone = args[\"lr_backbone\"] > 0\n",
    "    return_interm_layers = args[\"masks\"] or (args[\"num_feature_levels\"] > 1 )\n",
    "    backbone = Backbone(args[\"backbone\"], train_backbone, return_interm_layers, args[\"dilation\"])\n",
    "    model = Joiner(backbone, position_embedding)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backbone of swin, ONLY RUN IF SWIN IS USED AS BACKBONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if 'swin' in args[\"backbone\"]:\n",
    "    \n",
    "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "    from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "    import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "    class Mlp(nn.Module):\n",
    "        \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "        def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "            super().__init__()\n",
    "            out_features = out_features or in_features\n",
    "            hidden_features = hidden_features or in_features\n",
    "            self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "            self.act = act_layer()\n",
    "            self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "            self.drop = nn.Dropout(drop)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.act(x)\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.drop(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    def window_partition(x, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H, W, C)\n",
    "            window_size (int): window size\n",
    "        Returns:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "        \"\"\"\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "        return windows\n",
    "\n",
    "\n",
    "    def window_reverse(windows, window_size, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "            window_size (int): Window size\n",
    "            H (int): Height of image\n",
    "            W (int): Width of image\n",
    "        Returns:\n",
    "            x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "        x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    class WindowAttention(nn.Module):\n",
    "        \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "        It supports both of shifted and non-shifted window.\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            window_size (tuple[int]): The height and width of the window.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "            attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "            proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.window_size = window_size  # Wh, Ww\n",
    "            self.num_heads = num_heads\n",
    "            head_dim = dim // num_heads\n",
    "            self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "            # define a parameter table of relative position bias\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(self.window_size[0])\n",
    "            coords_w = torch.arange(self.window_size[1])\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "            self.proj = nn.Linear(dim, dim)\n",
    "            self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "            self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        def forward(self, x, mask=None):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: input features with shape of (num_windows*B, N, C)\n",
    "                mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "            \"\"\"\n",
    "            B_, N, C = x.shape\n",
    "            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "            q = q * self.scale\n",
    "            attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "            if mask is not None:\n",
    "                nW = mask.shape[0]\n",
    "                attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "                attn = attn.view(-1, self.num_heads, N, N)\n",
    "                attn = self.softmax(attn)\n",
    "            else:\n",
    "                attn = self.softmax(attn)\n",
    "\n",
    "            attn = self.attn_drop(attn)\n",
    "\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "            x = self.proj(x)\n",
    "            x = self.proj_drop(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class SwinTransformerBlock(nn.Module):\n",
    "        \"\"\" Swin Transformer Block.\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            window_size (int): Window size.\n",
    "            shift_size (int): Shift size for SW-MSA.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop (float, optional): Dropout rate. Default: 0.0\n",
    "            attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "            drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "            act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                    mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                    act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.num_heads = num_heads\n",
    "            self.window_size = window_size\n",
    "            self.shift_size = shift_size\n",
    "            self.mlp_ratio = mlp_ratio\n",
    "            assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "            self.norm1 = norm_layer(dim)\n",
    "            self.attn = WindowAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "            self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "            self.H = None\n",
    "            self.W = None\n",
    "\n",
    "        def forward(self, x, mask_matrix):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: Input feature, tensor size (B, H*W, C).\n",
    "                H, W: Spatial resolution of the input feature.\n",
    "                mask_matrix: Attention mask for cyclic shift.\n",
    "            \"\"\"\n",
    "            B, L, C = x.shape\n",
    "            H, W = self.H, self.W\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "            shortcut = x\n",
    "            x = self.norm1(x)\n",
    "            x = x.view(B, H, W, C)\n",
    "\n",
    "            # pad feature maps to multiples of window size\n",
    "            pad_l = pad_t = 0\n",
    "            pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "            pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, Hp, Wp, _ = x.shape\n",
    "\n",
    "            # cyclic shift\n",
    "            if self.shift_size > 0:\n",
    "                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "                attn_mask = mask_matrix\n",
    "            else:\n",
    "                shifted_x = x\n",
    "                attn_mask = None\n",
    "\n",
    "            # partition windows\n",
    "            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "            x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "            # W-MSA/SW-MSA\n",
    "            attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "            # merge windows\n",
    "            attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "            shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "            # reverse cyclic shift\n",
    "            if self.shift_size > 0:\n",
    "                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "            else:\n",
    "                x = shifted_x\n",
    "\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "            x = x.view(B, H * W, C)\n",
    "\n",
    "            # FFN\n",
    "            x = shortcut + self.drop_path(x)\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class PatchMerging(nn.Module):\n",
    "        \"\"\" Patch Merging Layer\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        \"\"\"\n",
    "        def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(4 * dim)\n",
    "\n",
    "        def forward(self, x, H, W):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: Input feature, tensor size (B, H*W, C).\n",
    "                H, W: Spatial resolution of the input feature.\n",
    "            \"\"\"\n",
    "            B, L, C = x.shape\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "            x = x.view(B, H, W, C)\n",
    "\n",
    "            # padding\n",
    "            pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "            x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "            x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "            x = self.norm(x)\n",
    "            x = self.reduction(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class BasicLayer(nn.Module):\n",
    "        \"\"\" A basic Swin Transformer layer for one stage.\n",
    "        Args:\n",
    "            dim (int): Number of feature channels\n",
    "            depth (int): Depths of this stage.\n",
    "            num_heads (int): Number of attention head.\n",
    "            window_size (int): Local window size. Default: 7.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop (float, optional): Dropout rate. Default: 0.0\n",
    "            attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "            drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "            norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    dim,\n",
    "                    depth,\n",
    "                    num_heads,\n",
    "                    window_size=7,\n",
    "                    mlp_ratio=4.,\n",
    "                    qkv_bias=True,\n",
    "                    qk_scale=None,\n",
    "                    drop=0.,\n",
    "                    attn_drop=0.,\n",
    "                    drop_path=0.,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    downsample=None,\n",
    "                    use_checkpoint=False):\n",
    "            super().__init__()\n",
    "            self.window_size = window_size\n",
    "            self.shift_size = window_size // 2\n",
    "            self.depth = depth\n",
    "            self.use_checkpoint = use_checkpoint\n",
    "\n",
    "            # build blocks\n",
    "            self.blocks = nn.ModuleList([\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer)\n",
    "                for i in range(depth)])\n",
    "\n",
    "            # patch merging layer\n",
    "            if downsample is not None:\n",
    "                self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "            else:\n",
    "                self.downsample = None\n",
    "\n",
    "        def forward(self, x, H, W):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: Input feature, tensor size (B, H*W, C).\n",
    "                H, W: Spatial resolution of the input feature.\n",
    "            \"\"\"\n",
    "\n",
    "            # calculate attention mask for SW-MSA\n",
    "            Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "            Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "            img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "            for blk in self.blocks:\n",
    "                blk.H, blk.W = H, W\n",
    "                if self.use_checkpoint:\n",
    "                    print('use_checkpoint')\n",
    "                    x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "                else:\n",
    "                    x = blk(x, attn_mask)\n",
    "            if self.downsample is not None:\n",
    "                x_down = self.downsample(x, H, W)\n",
    "                Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "                return x, H, W, x_down, Wh, Ww\n",
    "            else:\n",
    "                return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "    class PatchEmbed(nn.Module):\n",
    "        \"\"\" Image to Patch Embedding\n",
    "        Args:\n",
    "            patch_size (int): Patch token size. Default: 4.\n",
    "            in_chans (int): Number of input image channels. Default: 3.\n",
    "            embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "            norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "            super().__init__()\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            self.patch_size = patch_size\n",
    "\n",
    "            self.in_chans = in_chans\n",
    "            self.embed_dim = embed_dim\n",
    "\n",
    "            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "            if norm_layer is not None:\n",
    "                self.norm = norm_layer(embed_dim)\n",
    "            else:\n",
    "                self.norm = None\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward function.\"\"\"\n",
    "            # padding\n",
    "            _, _, H, W = x.size()\n",
    "            if W % self.patch_size[1] != 0:\n",
    "                x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "            if H % self.patch_size[0] != 0:\n",
    "                x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "            x = self.proj(x)  # B C Wh Ww\n",
    "            if self.norm is not None:\n",
    "                Wh, Ww = x.size(2), x.size(3)\n",
    "                x = x.flatten(2).transpose(1, 2)\n",
    "                x = self.norm(x)\n",
    "                x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class SwinTransformer(nn.Module):\n",
    "        \"\"\" Swin Transformer backbone.\n",
    "            A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "            https://arxiv.org/pdf/2103.14030\n",
    "        Args:\n",
    "            pretrain_img_size (int): Input image size for training the pretrained model,\n",
    "                used in absolute postion embedding. Default 224.\n",
    "            patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "            in_chans (int): Number of input image channels. Default: 3.\n",
    "            embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "            depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "            num_heads (tuple[int]): Number of attention head of each stage.\n",
    "            window_size (int): Window size. Default: 7.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop_rate (float): Dropout rate.\n",
    "            attn_drop_rate (float): Attention dropout rate. Default: 0.\n",
    "            drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "            norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "            ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.\n",
    "            patch_norm (bool): If True, add normalization after patch embedding. Default: True.\n",
    "            out_indices (Sequence[int]): Output from which stages.\n",
    "            frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "                -1 means not freezing any parameters.\n",
    "            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    pretrain_img_size=224,\n",
    "                    patch_size=4,\n",
    "                    in_chans=3,\n",
    "                    embed_dim=96,\n",
    "                    depths=[2, 2, 6, 2],\n",
    "                    num_heads=[3, 6, 12, 24],\n",
    "                    window_size=7,\n",
    "                    mlp_ratio=4.,\n",
    "                    qkv_bias=True,\n",
    "                    qk_scale=None,\n",
    "                    drop_rate=0.,\n",
    "                    attn_drop_rate=0.,\n",
    "                    drop_path_rate=0.2,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    ape=False,\n",
    "                    patch_norm=True,\n",
    "                    out_indices=(0, 1, 2, 3),\n",
    "                    frozen_stages=-1,\n",
    "                    use_checkpoint=False):\n",
    "            super().__init__()\n",
    "\n",
    "            self.pretrain_img_size = pretrain_img_size\n",
    "            self.num_layers = len(depths)\n",
    "            print('self.num_layers', self.num_layers)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.ape = ape\n",
    "            self.patch_norm = patch_norm\n",
    "            self.out_indices = out_indices\n",
    "            self.frozen_stages = frozen_stages\n",
    "            self.fpn = FeaturePyramidNetwork(in_channels_list=[256, 512, 1024],  out_channels=256)\n",
    "\n",
    "            # split image into non-overlapping patches\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "                norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "            # absolute position embedding\n",
    "            if self.ape:\n",
    "                pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "                patch_size = to_2tuple(patch_size)\n",
    "                patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]]\n",
    "\n",
    "                self.absolute_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1]))\n",
    "                trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "            self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "            # stochastic depth\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "            # build layers\n",
    "            self.layers = nn.ModuleList()\n",
    "            for i_layer in range(self.num_layers):\n",
    "                layer = BasicLayer(\n",
    "                    dim=int(embed_dim * 2 ** i_layer),\n",
    "                    depth=depths[i_layer],\n",
    "                    num_heads=num_heads[i_layer],\n",
    "                    window_size=window_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                    norm_layer=norm_layer,\n",
    "                    downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                    use_checkpoint=use_checkpoint)\n",
    "                self.layers.append(layer)\n",
    "\n",
    "            num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "            self.num_features = num_features\n",
    "\n",
    "            # add a norm layer for each output\n",
    "            for i_layer in out_indices:\n",
    "                layer = norm_layer(num_features[i_layer])\n",
    "                layer_name = f'norm{i_layer}'\n",
    "                self.add_module(layer_name, layer)\n",
    "\n",
    "            self._freeze_stages()\n",
    "\n",
    "        def _freeze_stages(self):\n",
    "            if self.frozen_stages >= 0:\n",
    "                self.patch_embed.eval()\n",
    "                for param in self.patch_embed.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            if self.frozen_stages >= 1 and self.ape:\n",
    "                self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "            if self.frozen_stages >= 2:\n",
    "                self.pos_drop.eval()\n",
    "                for i in range(0, self.frozen_stages - 1):\n",
    "                    m = self.layers[i]\n",
    "                    m.eval()\n",
    "                    for param in m.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        def init_weights(self, pretrained=None):\n",
    "            \"\"\"Initialize the weights in backbone.\n",
    "            Args:\n",
    "                pretrained (str, optional): Path to pre-trained weights.\n",
    "                    Defaults to None.\n",
    "            \"\"\"\n",
    "            def _init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    trunc_normal_(m.weight, std=.02)\n",
    "                    if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "            if isinstance(pretrained, str):\n",
    "                self.apply(_init_weights)\n",
    "                checkpoint = torch.load(pretrained, map_location='cpu')\n",
    "                print(f'load from {pretrained}.') \n",
    "                self.load_state_dict(checkpoint['model'], strict=False)\n",
    "            elif pretrained is None:\n",
    "                self.apply(_init_weights)\n",
    "            else:\n",
    "                raise TypeError('pretrained must be a str or None')\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward function.\"\"\"\n",
    "            x = self.patch_embed(x)\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            if self.ape:\n",
    "                # interpolate the position embedding to the corresponding size\n",
    "                absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')\n",
    "                x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "            else:\n",
    "                x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.pos_drop(x)\n",
    "\n",
    "            outs = []\n",
    "            for i in range(self.num_layers):\n",
    "                # print('i', i)\n",
    "                layer = self.layers[i]\n",
    "                x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "                if i in self.out_indices:\n",
    "                    norm_layer = getattr(self, f'norm{i}')\n",
    "                    x_out = norm_layer(x_out)\n",
    "                    out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "                    outs.append(out)\n",
    "            \n",
    "            # Modified swin-based backbone via feature aggregation\n",
    "            rets = {str(u): v for (u,v) in enumerate(outs)}\n",
    "            feat_fpn = self.fpn(rets)        \n",
    "            bs, dim, size_h, size_w = feat_fpn['0'].shape\n",
    "            feat_aggregate = feat_fpn['0'] # torch.Size([1, 256, 25, 34])\n",
    "            outs_agg = []\n",
    "            for k, v in feat_fpn.items():\n",
    "                if k!='0':\n",
    "                    feat = F.interpolate(feat_fpn[k], size=(size_h, size_w), scale_factor=None, mode='bilinear', align_corners=None)\n",
    "                    feat_aggregate = feat_aggregate + feat\n",
    "            outs_agg.append(feat_aggregate) # torch.Size([1, 1024, 7, 9]\n",
    "\n",
    "            rets_agg = {str(u): v for (u,v) in enumerate(outs_agg)}\n",
    "\n",
    "            return rets_agg\n",
    "\n",
    "        def train(self, mode=True):\n",
    "            \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "            super(SwinTransformer, self).train(mode)\n",
    "            self._freeze_stages()\n",
    "\n",
    "\n",
    "    class BackboneBase(nn.Module):\n",
    "        def __init__(self, backbone: nn.Module, strides=[4, 8, 16, 32], num_channels=[96, 192, 384, 768]):\n",
    "            super().__init__()\n",
    "            self.strides = strides\n",
    "            self.num_channels = num_channels\n",
    "            self.body = backbone\n",
    "\n",
    "        def forward(self, tensor_list: NestedTensor):\n",
    "            xs = self.body(tensor_list.tensors)\n",
    "            out: Dict[str, NestedTensor] = {}\n",
    "            for name, x in xs.items():\n",
    "                m = tensor_list.mask\n",
    "                assert m is not None\n",
    "                mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "                out[name] = NestedTensor(x, mask)\n",
    "            return out\n",
    "\n",
    "\n",
    "    class Backbone(BackboneBase):\n",
    "        \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "        def __init__(self, name: str,\n",
    "                    checkpoint: bool = False,\n",
    "                    pretrained: str = None):\n",
    "            assert name in ['swin_t_p4w7', 'swin_s_p4w7', 'swin_b_p4w7', 'swin_l_p4w7']\n",
    "            cfgs = configs[name]\n",
    "            cfgs.update({'use_checkpoint': checkpoint})\n",
    "            out_indices = cfgs['out_indices']\n",
    "            strides = [int(2**(i+2)) for i in out_indices]\n",
    "            num_channels = [int(cfgs['embed_dim'] * 2**i) for i in out_indices]\n",
    "            backbone = SwinTransformer(**cfgs)\n",
    "            backbone.init_weights(pretrained)\n",
    "            super().__init__(backbone, strides, num_channels)\n",
    "\n",
    "\n",
    "    class Joiner(nn.Sequential):\n",
    "        def __init__(self, backbone, position_embedding):\n",
    "            super().__init__(backbone, position_embedding)\n",
    "            self.strides = backbone.strides\n",
    "            self.num_channels = backbone.num_channels\n",
    "\n",
    "        def forward(self, tensor_list: NestedTensor):\n",
    "            xs = self[0](tensor_list)\n",
    "            out: List[NestedTensor] = []\n",
    "            pos = []\n",
    "            for name, x in sorted(xs.items()):\n",
    "                out.append(x)\n",
    "            # position encoding\n",
    "            for x in out:\n",
    "                pos.append(self[1](x).to(x.tensors.dtype))\n",
    "            return out, pos\n",
    "\n",
    "        \n",
    "    def build_swin_backbone(args):\n",
    "        position_embedding = build_position_encoding(args)\n",
    "        backbone = Backbone(args[\"backbone\"], args[\"checkpoint\"], args[\"pretrained\"])\n",
    "        model = Joiner(backbone, position_embedding)\n",
    "        return model\n",
    "\n",
    "\n",
    "    configs = {\n",
    "        'swin_t_p4w7': dict(embed_dim=96,\n",
    "                        depths=[2, 2, 6, 2],\n",
    "                        num_heads=[3, 6, 12, 24],\n",
    "                        window_size=7,\n",
    "                        mlp_ratio=4.,\n",
    "                        qkv_bias=True,\n",
    "                        qk_scale=None,\n",
    "                        drop_rate=0.,\n",
    "                        attn_drop_rate=0.,\n",
    "                        drop_path_rate=0.2,\n",
    "                        ape=False,\n",
    "                        patch_norm=True,\n",
    "                        out_indices=(1, 2, 3),\n",
    "                        use_checkpoint=False),\n",
    "        'swin_s_p4w7': dict(embed_dim=96,\n",
    "                            depths=[2, 2, 18, 2],\n",
    "                            num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4.,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.,\n",
    "                            attn_drop_rate=0.,\n",
    "                            drop_path_rate=0.2,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            out_indices=(1, 2, 3),\n",
    "                            use_checkpoint=False),\n",
    "        'swin_b_p4w7': dict(embed_dim=128,\n",
    "                            depths=[2, 2, 18, 2],\n",
    "                            num_heads=[4, 8, 16, 32],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4.,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.,\n",
    "                            attn_drop_rate=0.,\n",
    "                            drop_path_rate=0.3,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            out_indices=(1, 2, 3),\n",
    "                            use_checkpoint=True),\n",
    "        'swin_l_p4w7': dict(embed_dim=192,\n",
    "                            depths=[2, 2, 18, 2],\n",
    "                            num_heads=[6, 12, 24, 48],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4.,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.,\n",
    "                            attn_drop_rate=0.,\n",
    "                            drop_path_rate=0.3,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            out_indices=(1, 2, 3),\n",
    "                            use_checkpoint=False),\n",
    "            \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher, loss of bounding box, class and generalized intersection over union (giou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.boxes import box_area\n",
    "\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / union\n",
    "    return iou, union\n",
    "\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Generalized IoU from https://giou.stanford.edu/\n",
    "\n",
    "    The boxes should be in [x0, y0, x1, y1] format\n",
    "\n",
    "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "    and M = len(boxes2)\n",
    "    \"\"\"\n",
    "    # degenerate boxes gives inf / nan results\n",
    "    # so do an early check\n",
    "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cost_class: float = 1,\n",
    "                 cost_bbox: float = 1,\n",
    "                 cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "            # We flatten to compute the cost matrices in a batch\n",
    "            out_prob = outputs[\"pred_logits\"].flatten(0, 1).sigmoid() # [batch_size * num_queries, num_classes]\n",
    "            out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "            # Also concat the target labels and boxes\n",
    "            tgt_ids = torch.cat([v[\"labels\"] for v in targets]) \n",
    "            # print(\"tgt_ids_shape\", tgt_ids.shape)\n",
    "            tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "            # Compute the classification cost.\n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "            pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "            #print(\"pos_cost_class_shape\", pos_cost_class.shape)\n",
    "            cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "            #print(\"cost_class_shape\", cost_class.shape)\n",
    "\n",
    "            # Compute the L1 cost between boxes\n",
    "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "            # Compute the giou cost betwen boxes\n",
    "            cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox),\n",
    "                                             box_cxcywh_to_xyxy(tgt_bbox))\n",
    "\n",
    "            # Final cost matrix\n",
    "            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
    "            C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "            sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "            #print(\"size\", sizes)\n",
    "            indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "            return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "\n",
    "def build_matcher(args):\n",
    "    return HungarianMatcher(cost_class=args[\"set_cost_class\"],\n",
    "                            cost_bbox=args[\"set_cost_bbox\"],\n",
    "                            cost_giou=args[\"set_cost_giou\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
    "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
    "    return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n",
    "\n",
    "def _max_by_axis(the_list):\n",
    "    # type: (List[List[int]]) -> List[int]\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # TODO make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        # TODO make it support different-sized images\n",
    "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from panopticapi.utils import id2rgb, rgb2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class DETRsegm(nn.Module):\n",
    "    def __init__(self, detr, freeze_detr=False):\n",
    "        super().__init__()\n",
    "        self.detr = detr\n",
    "\n",
    "        if freeze_detr:\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n",
    "        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0)\n",
    "        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        if not isinstance(samples, NestedTensor):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.detr.backbone(samples)\n",
    "\n",
    "        bs = features[-1].tensors.shape[0]\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        src_proj = self.detr.input_proj(src)\n",
    "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
    "\n",
    "        outputs_class = self.detr.class_embed(hs)\n",
    "        outputs_coord = self.detr.bbox_embed(hs).sigmoid()\n",
    "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
    "        if self.detr.aux_loss:\n",
    "            out[\"aux_outputs\"] = [\n",
    "                {\"pred_logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])\n",
    "            ]\n",
    "\n",
    "        # FIXME h_boxes takes the last one computed, keep this in mind\n",
    "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
    "\n",
    "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
    "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "\n",
    "        out[\"pred_masks\"] = outputs_seg_masks\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm.\n",
    "    Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
    "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, bbox_mask, fpns):\n",
    "        def expand(tensor, length):\n",
    "            return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
    "\n",
    "        x = torch.cat([expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.out_lay(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHAttentionMap(nn.Module):\n",
    "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "\n",
    "        nn.init.zeros_(self.k_linear.bias)\n",
    "        nn.init.zeros_(self.q_linear.bias)\n",
    "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
    "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask=None):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view_as(weights)\n",
    "        weights = self.dropout(weights)\n",
    "        return weights\n",
    "\n",
    "\n",
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    \"\"\"\n",
    "    Compute the DICE loss, similar to generalized IOU for masks\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "    \"\"\"\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes\n",
    "\n",
    "\n",
    "class PostProcessSegm(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            results[i][\"masks\"] = F.interpolate(\n",
    "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
    "            ).byte()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class PostProcessPanoptic(nn.Module):\n",
    "    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n",
    "    coco panoptic API \"\"\"\n",
    "\n",
    "    def __init__(self, is_thing_map, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n",
    "                          the class is  a thing (True) or a stuff (False) class\n",
    "           threshold: confidence threshold: segments with confidence lower than this will be deleted\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.is_thing_map = is_thing_map\n",
    "\n",
    "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
    "        \"\"\" This function computes the panoptic prediction from the model's predictions.\n",
    "        Parameters:\n",
    "            outputs: This is a dict coming directly from the model. See the model doc for the content.\n",
    "            processed_sizes: This is a list of tuples (or torch tensors) of sizes of the images that were passed to the\n",
    "                             model, ie the size after data augmentation but before batching.\n",
    "            target_sizes: This is a list of tuples (or torch tensors) corresponding to the requested final size\n",
    "                          of each prediction. If left to None, it will default to the processed_sizes\n",
    "            \"\"\"\n",
    "        if target_sizes is None:\n",
    "            target_sizes = processed_sizes\n",
    "        assert len(processed_sizes) == len(target_sizes)\n",
    "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
    "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
    "        preds = []\n",
    "\n",
    "        def to_tuple(tup):\n",
    "            if isinstance(tup, tuple):\n",
    "                return tup\n",
    "            return tuple(tup.cpu().tolist())\n",
    "\n",
    "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n",
    "            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n",
    "        ):\n",
    "            # we filter empty queries and detection below threshold\n",
    "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
    "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
    "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
    "            cur_scores = cur_scores[keep]\n",
    "            cur_classes = cur_classes[keep]\n",
    "            cur_masks = cur_masks[keep]\n",
    "            cur_masks = interpolate(cur_masks[None], to_tuple(size), mode=\"bilinear\").squeeze(0)\n",
    "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
    "\n",
    "            h, w = cur_masks.shape[-2:]\n",
    "            assert len(cur_boxes) == len(cur_classes)\n",
    "\n",
    "            # It may be that we have several predicted masks for the same stuff class.\n",
    "            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n",
    "            cur_masks = cur_masks.flatten(1)\n",
    "            stuff_equiv_classes = defaultdict(lambda: [])\n",
    "            for k, label in enumerate(cur_classes):\n",
    "                if not self.is_thing_map[label.item()]:\n",
    "                    stuff_equiv_classes[label.item()].append(k)\n",
    "\n",
    "            def get_ids_area(masks, scores, dedup=False):\n",
    "                # This helper function creates the final panoptic segmentation image\n",
    "                # It also returns the area of the masks that appears on the image\n",
    "\n",
    "                m_id = masks.transpose(0, 1).softmax(-1)\n",
    "\n",
    "                if m_id.shape[-1] == 0:\n",
    "                    # We didn't detect any mask :(\n",
    "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
    "                else:\n",
    "                    m_id = m_id.argmax(-1).view(h, w)\n",
    "\n",
    "                if dedup:\n",
    "                    # Merge the masks corresponding to the same stuff class\n",
    "                    for equiv in stuff_equiv_classes.values():\n",
    "                        if len(equiv) > 1:\n",
    "                            for eq_id in equiv:\n",
    "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
    "\n",
    "                final_h, final_w = to_tuple(target_size)\n",
    "\n",
    "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
    "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
    "\n",
    "                np_seg_img = (\n",
    "                    torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
    "                )\n",
    "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
    "\n",
    "                area = []\n",
    "                for i in range(len(scores)):\n",
    "                    area.append(m_id.eq(i).sum().item())\n",
    "                return area, seg_img\n",
    "\n",
    "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
    "            if cur_classes.numel() > 0:\n",
    "                # We know filter empty masks as long as we find some\n",
    "                while True:\n",
    "                    filtered_small = torch.as_tensor(\n",
    "                        [area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device\n",
    "                    )\n",
    "                    if filtered_small.any().item():\n",
    "                        cur_scores = cur_scores[~filtered_small]\n",
    "                        cur_classes = cur_classes[~filtered_small]\n",
    "                        cur_masks = cur_masks[~filtered_small]\n",
    "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
    "\n",
    "            segments_info = []\n",
    "            for i, a in enumerate(area):\n",
    "                cat = cur_classes[i].item()\n",
    "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
    "            del cur_classes\n",
    "\n",
    "            with io.BytesIO() as out:\n",
    "                seg_img.save(out, format=\"PNG\")\n",
    "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
    "            preds.append(predictions)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Part 2 DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableDETR(nn.Module):\n",
    "    \"\"\" This is the Deformable DETR module that performs object detection \"\"\"\n",
    "    def __init__(self, backbone, transformer, num_classes, num_queries, num_feature_levels, \n",
    "                 num_frames = 3, aux_loss=True, with_box_refine=False, two_stage=False):\n",
    "        \"\"\" Initializes the model.\n",
    "        Parameters:\n",
    "            backbone: torch module of the backbone to be used. See backbone.py\n",
    "            transformer: torch module of the transformer architecture. See transformer.py\n",
    "            num_classes: number of object classes\n",
    "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
    "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
    "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
    "            with_box_refine: iterative bounding box refinement\n",
    "            two_stage: two-stage Deformable DETR\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        #self.num_ref_frames = num_ref_frames\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "        self.num_feature_levels = num_feature_levels\n",
    "\n",
    "        self.temp_class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.temp_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "    \n",
    "        if not two_stage:\n",
    "            self.query_embed = nn.Embedding(num_queries, hidden_dim*2)\n",
    "        if num_feature_levels > 1:\n",
    "            num_backbone_outs = len(backbone.strides)\n",
    "            input_proj_list = []\n",
    "            for _ in range(num_backbone_outs):\n",
    "                in_channels = backbone.num_channels[_]\n",
    "                input_proj_list.append(nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, hidden_dim, kernel_size=1),\n",
    "                    nn.GroupNorm(32, hidden_dim),\n",
    "                ))\n",
    "            for _ in range(num_feature_levels - num_backbone_outs):\n",
    "                input_proj_list.append(nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.GroupNorm(32, hidden_dim),\n",
    "                ))\n",
    "                in_channels = hidden_dim\n",
    "            self.input_proj = nn.ModuleList(input_proj_list)\n",
    "        else:\n",
    "            # self.input_proj = nn.ModuleList([\n",
    "            #     nn.Sequential(\n",
    "            #         nn.Conv2d(backbone.num_channels[0], hidden_dim, kernel_size=1),\n",
    "            #         nn.GroupNorm(32, hidden_dim),\n",
    "            #     )])\n",
    "            self.input_proj = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1),\n",
    "                    nn.GroupNorm(32, hidden_dim),\n",
    "                )])\n",
    "        self.backbone = backbone\n",
    "        self.aux_loss = aux_loss\n",
    "        self.with_box_refine = with_box_refine\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        prior_prob = 0.01\n",
    "        bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        self.class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "        ###############\n",
    "        self.temp_class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "        nn.init.constant_(self.temp_bbox_embed.layers[-1].weight.data, 0)\n",
    "        nn.init.constant_(self.temp_bbox_embed.layers[-1].bias.data, 0)\n",
    "        nn.init.constant_(self.temp_bbox_embed.layers[-1].bias.data[2:], -2.0)\n",
    "        ##############\n",
    "\n",
    "        nn.init.constant_(self.bbox_embed.layers[-1].weight.data, 0)\n",
    "        nn.init.constant_(self.bbox_embed.layers[-1].bias.data, 0)\n",
    "        for proj in self.input_proj:\n",
    "            nn.init.xavier_uniform_(proj[0].weight, gain=1)\n",
    "            nn.init.constant_(proj[0].bias, 0)\n",
    "        self.temp_class_embed_list = _get_clones(self.temp_class_embed, 3)\n",
    "        self.temp_bbox_embed_list = _get_clones(self.temp_bbox_embed, 3)\n",
    "        # if two-stage, the last class_embed and bbox_embed is for region proposal generation\n",
    "        num_pred = (transformer.decoder.num_layers + 1) if two_stage else transformer.decoder.num_layers\n",
    "        if with_box_refine:\n",
    "            self.class_embed = _get_clones(self.class_embed, num_pred)\n",
    "            self.bbox_embed = _get_clones(self.bbox_embed, num_pred)\n",
    "            nn.init.constant_(self.bbox_embed[0].layers[-1].bias.data[2:], -2.0)\n",
    "            # hack implementation for iterative bounding box refinement\n",
    "            self.transformer.decoder.bbox_embed = self.bbox_embed\n",
    "        else:\n",
    "            nn.init.constant_(self.bbox_embed.layers[-1].bias.data[2:], -2.0)\n",
    "            self.class_embed = nn.ModuleList([self.class_embed for _ in range(num_pred)])\n",
    "            self.bbox_embed = nn.ModuleList([self.bbox_embed for _ in range(num_pred)])\n",
    "            self.transformer.decoder.bbox_embed = None\n",
    "        if two_stage:\n",
    "            # hack implementation for two-stage\n",
    "            self.transformer.decoder.class_embed = self.class_embed\n",
    "            for box_embed in self.bbox_embed:\n",
    "                nn.init.constant_(box_embed.layers[-1].bias.data[2:], 0.0)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
    "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
    "\n",
    "            It returns a dict with the following elements:\n",
    "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
    "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
    "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
    "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
    "                               relative to the size of each individual image (disregarding possible padding).\n",
    "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
    "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
    "                                dictionnaries containing the two above keys for each decoder layer.\n",
    "        \"\"\"\n",
    "        if not isinstance(samples, NestedTensor):\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "        # print('features[-1].tensors.shape', features[-1].tensors.shape)\n",
    "\n",
    "        srcs = []\n",
    "        masks = []\n",
    "        for l, feat in enumerate(features):\n",
    "            src, mask = feat.decompose()\n",
    "            srcs.append(self.input_proj[l](src))\n",
    "            masks.append(mask)\n",
    "            assert mask is not None\n",
    "\n",
    "        if self.num_feature_levels > len(srcs):\n",
    "            _len_srcs = len(srcs)\n",
    "            for l in range(_len_srcs, self.num_feature_levels):\n",
    "                if l == _len_srcs:\n",
    "                    src = self.input_proj[l](features[-1].tensors)\n",
    "                else:\n",
    "                    src = self.input_proj[l](srcs[-1])\n",
    "                m = samples.mask\n",
    "                mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]\n",
    "                pos_l = self.backbone[1](NestedTensor(src, mask)).to(src.dtype)\n",
    "                srcs.append(src)\n",
    "                masks.append(mask)\n",
    "                pos.append(pos_l)\n",
    "\n",
    "        query_embeds = None\n",
    "        if not self.two_stage:\n",
    "            query_embeds = self.query_embed.weight\n",
    "        hs, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact, final_hs, final_references_out, out = self.transformer(srcs, masks, pos, query_embeds, self.class_embed[-1], self.temp_class_embed_list, self.temp_bbox_embed_list)\n",
    "        \n",
    "\n",
    "        outputs_classes = []\n",
    "        outputs_coords = []\n",
    "        \n",
    "       # out = {}\n",
    "        if self.two_stage:\n",
    "            enc_outputs_coord = enc_outputs_coord_unact.sigmoid()\n",
    "            out['enc_outputs'] = {'pred_logits': enc_outputs_class, 'pred_boxes': enc_outputs_coord}\n",
    "     \n",
    "        if final_hs is not None:\n",
    "            reference = inverse_sigmoid(final_references_out)\n",
    "            output_class = self.temp_class_embed_list[2](final_hs)\n",
    "            tmp = self.temp_bbox_embed_list[2](final_hs)\n",
    "            if reference.shape[-1] == 4:\n",
    "                tmp += reference\n",
    "            else:\n",
    "                assert reference.shape[-1] == 2\n",
    "                tmp[..., :2] += reference\n",
    "            output_coord = tmp.sigmoid()\n",
    "            out[\"pred_logits\"] = output_class # [4, 300, 30]\n",
    "            out[\"pred_boxes\"] = output_coord  # [4, 300, 4]\n",
    "        #print(out.keys())\n",
    "        return out\n",
    "\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        # this is a workaround to make torchscript happy, as torchscript\n",
    "        # doesn't support dictionary with non-homogeneous values, such\n",
    "        # as a dict having both a Tensor and a list.\n",
    "        return [{'pred_logits': a, 'pred_boxes': b}\n",
    "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
    "\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, matcher, weight_dict, losses, focal_alpha=0.25):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "            focal_alpha: alpha in Focal Loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.focal_alpha = focal_alpha\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n",
    "                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)\n",
    "        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "\n",
    "        target_classes_onehot = target_classes_onehot[:,:,:-1]\n",
    "        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2) * src_logits.shape[1]\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, h, w), normalized by the image size.\n",
    "        \"\"\"\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_cxcywh_to_xyxy(src_boxes),\n",
    "            box_cxcywh_to_xyxy(target_boxes)))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "\n",
    "        # TODO use valid to mask invalid areas due to padding in loss\n",
    "        target_masks, valid = nested_tensor_from_tensor_list([t[\"masks\"] for t in targets]).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "\n",
    "        src_masks = src_masks[src_idx]\n",
    "        # upsample predictions to the target size\n",
    "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks[tgt_idx].flatten(1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'boxes': self.loss_boxes,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs' and k != 'enc_outputs'}\n",
    "        #print(outputs_without_aux)\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs[\"pred_boxes\"])).device)\n",
    "        \n",
    "        num_boxes = torch.clamp(num_boxes / 1, min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            kwargs = {}\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes, **kwargs))\n",
    "\n",
    "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    if loss == 'masks':\n",
    "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
    "                        continue\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        # Logging is enabled only for the last layer\n",
    "                        kwargs['log'] = False\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        if 'enc_outputs' in outputs:\n",
    "            enc_outputs = outputs['enc_outputs']\n",
    "            bin_targets = copy.deepcopy(targets)\n",
    "            for bt in bin_targets:\n",
    "                bt['labels'] = torch.zeros_like(bt['labels'])\n",
    "            indices = self.matcher(enc_outputs, bin_targets)\n",
    "            for loss in self.losses:\n",
    "                if loss == 'masks':\n",
    "                    # Intermediate masks losses are too costly to compute, we ignore them.\n",
    "                    continue\n",
    "                kwargs = {}\n",
    "                if loss == 'labels':\n",
    "                    # Logging is enabled only for the last layer\n",
    "                    kwargs['log'] = False\n",
    "                l_dict = self.get_loss(loss, enc_outputs, bin_targets, indices, num_boxes, **kwargs)\n",
    "                l_dict = {k + f'_enc': v for k, v in l_dict.items()}\n",
    "                losses.update(l_dict)\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "class PostProcess(nn.Module):\n",
    "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        \"\"\" Perform the computation\n",
    "        Parameters:\n",
    "            outputs: raw outputs of the model\n",
    "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
    "                          For evaluation, this must be the original image size (before any data augmentation)\n",
    "                          For visualization, this should be the image size after data augment, but before padding\n",
    "        \"\"\"\n",
    "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
    "\n",
    "        assert len(out_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        prob = out_logits.sigmoid()\n",
    "        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n",
    "        scores = topk_values\n",
    "        topk_boxes = topk_indexes // out_logits.shape[2]\n",
    "        labels = topk_indexes % out_logits.shape[2]\n",
    "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1,1,4))\n",
    "\n",
    "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "        boxes = boxes * scale_fct[:, None, :]\n",
    "\n",
    "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(args):\n",
    "    num_classes = 31 #2 Maybe let original number of classes and fine tune one !!!!\n",
    "    device = torch.device(args[\"device\"])\n",
    "\n",
    "    if 'swin' in args[\"backbone\"]:\n",
    "        backbone = build_swin_backbone(args) \n",
    "    else:\n",
    "        backbone = build_backbone(args)\n",
    "    # backbone = build_backbone(args)\n",
    "\n",
    "    transformer = build_deforamble_transformer(args)\n",
    "    model = DeformableDETR(\n",
    "        backbone,\n",
    "        transformer,\n",
    "        num_classes=num_classes,\n",
    "        num_queries=args[\"num_queries\"],\n",
    "        num_feature_levels=args[\"num_feature_levels\"],\n",
    "        num_frames=args[\"num_frames\"],\n",
    "        aux_loss=args[\"aux_loss\"],\n",
    "        with_box_refine=args[\"with_box_refine\"],\n",
    "        two_stage=args[\"two_stage\"],\n",
    "    )\n",
    "    if args[\"masks\"]:\n",
    "        model = DETRsegm(model, freeze_detr=(args[\"frozen_weights\"] is not None))\n",
    "    matcher = build_matcher(args)\n",
    "    weight_dict = {'loss_ce': args[\"cls_loss_coef\"], 'loss_bbox': args[\"bbox_loss_coef\"]}\n",
    "    weight_dict['loss_giou'] = args[\"giou_loss_coef\"]\n",
    "    if args[\"masks\"]:\n",
    "        weight_dict[\"loss_mask\"] = args[\"mask_loss_coef\"]\n",
    "        weight_dict[\"loss_dice\"] = args[\"dice_loss_coef\"]\n",
    "    # TODO this is a hack\n",
    "    if args[\"aux_loss\"]:\n",
    "        aux_weight_dict = {}\n",
    "        for i in range(args[\"dec_layers\"] - 1):\n",
    "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "        aux_weight_dict.update({k + f'_enc': v for k, v in weight_dict.items()})\n",
    "        weight_dict.update(aux_weight_dict)\n",
    "\n",
    "    losses = ['labels', 'boxes', 'cardinality']\n",
    "    if args[\"masks\"]:\n",
    "        losses += [\"masks\"]\n",
    "    # num_classes, matcher, weight_dict, losses, focal_alpha=0.25\n",
    "    criterion = SetCriterion(num_classes, matcher, weight_dict, losses, focal_alpha=args[\"focal_alpha\"])\n",
    "    criterion.to(device)\n",
    "    postprocessors = {'bbox': PostProcess()}\n",
    "    if args[\"masks\"]:\n",
    "        postprocessors['segm'] = PostProcessSegm()\n",
    "        if args[\"dataset_file\"] == \"coco_panoptic\":\n",
    "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
    "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
    "\n",
    "    return model, criterion, postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_layers 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model, criterion, postprocessors=build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeformableDETR(\n",
      "  (transformer): DeformableTransformer(\n",
      "    (encoder): DeformableTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_embed): ModuleList(\n",
      "        (0-5): 6 x MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (temporal_query_layer1): TemporalQueryEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout4): Dropout(p=0.1, inplace=False)\n",
      "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (temporal_query_layer2): TemporalQueryEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout4): Dropout(p=0.1, inplace=False)\n",
      "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (temporal_query_layer3): TemporalQueryEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout4): Dropout(p=0.1, inplace=False)\n",
      "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (temporal_decoder1): TemporalDeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (temporal_decoder2): TemporalDeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (temporal_decoder3): TemporalDeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reference_points): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (class_embed): ModuleList(\n",
      "    (0-5): 6 x Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      "  (bbox_embed): ModuleList(\n",
      "    (0-5): 6 x MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (temp_class_embed): Linear(in_features=256, out_features=31, bias=True)\n",
      "  (temp_bbox_embed): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (query_embed): Embedding(100, 512)\n",
      "  (input_proj): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (backbone): Joiner(\n",
      "    (0): Backbone(\n",
      "      (body): SwinTransformer(\n",
      "        (fpn): FeaturePyramidNetwork(\n",
      "          (inner_blocks): ModuleList(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (layer_blocks): ModuleList(\n",
      "            (0-2): 3 x Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.013)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.026)\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.039)\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (2): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.052)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.065)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.078)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.091)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.104)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.117)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.130)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.143)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.157)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.170)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.183)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.196)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (12): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.209)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (13): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.222)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (14): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.235)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (15): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.248)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (16): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.261)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (17): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.274)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (3): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.287)\n",
      "                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.300)\n",
      "                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (temp_class_embed_list): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      "  (temp_bbox_embed_list): ModuleList(\n",
      "    (0-2): 3 x MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vid multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.vision import VisionDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import tqdm\n",
    "from io import BytesIO\n",
    "from pycocotools.coco import COCO, _isArrayLike\n",
    "\n",
    "class TvCocoDetection(VisionDataset):\n",
    "    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        annFile (string): Path to json annotation file.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, annFile, transform=None, target_transform=None, transforms=None,\n",
    "                 cache_mode=False, local_rank=0, local_size=1):\n",
    "        super(TvCocoDetection, self).__init__(root, transforms, transform, target_transform)\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.cache_mode = cache_mode\n",
    "        self.local_rank = local_rank\n",
    "        self.local_size = local_size\n",
    "        if cache_mode:\n",
    "            self.cache = {}\n",
    "            self.cache_images()\n",
    "\n",
    "    def cache_images(self):\n",
    "        self.cache = {}\n",
    "        for index, img_id in zip(tqdm.trange(len(self.ids)), self.ids):\n",
    "            if index % self.local_size != self.local_rank:\n",
    "                continue\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "            with open(os.path.join(self.root, path), 'rb') as f:\n",
    "                self.cache[path] = f.read()\n",
    "\n",
    "    def get_image(self, path):\n",
    "        if self.cache_mode:\n",
    "            if path not in self.cache.keys():\n",
    "                with open(os.path.join(self.root, path), 'rb') as f:\n",
    "                    self.cache[path] = f.read()\n",
    "            return Image.open(BytesIO(self.cache[path])).convert('RGB')\n",
    "        return Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = self.get_image(path)\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "class CocoVID(COCO):\n",
    "    \"\"\"Inherit official COCO class in order to parse the annotations of bbox-\n",
    "    related video tasks.\n",
    "    Args:\n",
    "        annotation_file (str): location of annotation file. Defaults to None.\n",
    "        load_img_as_vid (bool): If True, convert image data to video data,\n",
    "            which means each image is converted to a video. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotation_file=None, load_img_as_vid=False):\n",
    "        assert annotation_file, 'Annotation file must be provided.'\n",
    "        self.load_img_as_vid = load_img_as_vid\n",
    "        super(CocoVID, self).__init__(annotation_file=annotation_file)\n",
    "\n",
    "    def convert_img_to_vid(self, dataset):\n",
    "        \"\"\"Convert image data to video data.\"\"\"\n",
    "        if 'images' in self.dataset:\n",
    "            videos = []\n",
    "            for i, img in enumerate(self.dataset['images']):\n",
    "                videos.append(dict(id=img['id'], name=img['file_name']))\n",
    "                img['video_id'] = img['id']\n",
    "                img['frame_id'] = 0\n",
    "            dataset['videos'] = videos\n",
    "\n",
    "        if 'annotations' in self.dataset:\n",
    "            for i, ann in enumerate(self.dataset['annotations']):\n",
    "                ann['video_id'] = ann['image_id']\n",
    "                ann['instance_id'] = ann['id']\n",
    "        return dataset\n",
    "\n",
    "    def createIndex(self):\n",
    "        \"\"\"Create index.\"\"\"\n",
    "        print('creating index...')\n",
    "        anns, cats, imgs, vids = {}, {}, {}, {}\n",
    "        (imgToAnns, catToImgs, vidToImgs, vidToInstances,\n",
    "         instancesToImgs) = defaultdict(list), defaultdict(list), defaultdict(\n",
    "             list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "        if 'videos' not in self.dataset and self.load_img_as_vid:\n",
    "            self.dataset = self.convert_img_to_vid(self.dataset)\n",
    "\n",
    "        if 'videos' in self.dataset:\n",
    "            for video in self.dataset['videos']:\n",
    "                vids[video['id']] = video\n",
    "\n",
    "        if 'annotations' in self.dataset:\n",
    "            for ann in self.dataset['annotations']:\n",
    "                imgToAnns[ann['image_id']].append(ann)\n",
    "                anns[ann['id']] = ann\n",
    "                if 'instance_id' in ann:\n",
    "                    instancesToImgs[ann['instance_id']].append(ann['image_id'])\n",
    "                    if 'video_id' in ann and \\\n",
    "                        ann['instance_id'] not in \\\n",
    "                            vidToInstances[ann['video_id']]:\n",
    "                        vidToInstances[ann['video_id']].append(\n",
    "                            ann['instance_id'])\n",
    "\n",
    "        if 'images' in self.dataset:\n",
    "            for img in self.dataset['images']:\n",
    "                vidToImgs[img['video_id']].append(img)\n",
    "                imgs[img['id']] = img\n",
    "\n",
    "        if 'categories' in self.dataset:\n",
    "            for cat in self.dataset['categories']:\n",
    "                cats[cat['id']] = cat\n",
    "\n",
    "        if 'annotations' in self.dataset and 'categories' in self.dataset:\n",
    "            for ann in self.dataset['annotations']:\n",
    "                catToImgs[ann['category_id']].append(ann['image_id'])\n",
    "\n",
    "        print('index created!')\n",
    "\n",
    "        self.anns = anns\n",
    "        self.imgToAnns = imgToAnns\n",
    "        self.catToImgs = catToImgs\n",
    "        self.imgs = imgs\n",
    "        self.cats = cats\n",
    "        self.videos = vids\n",
    "        self.vidToImgs = vidToImgs\n",
    "        self.vidToInstances = vidToInstances\n",
    "        self.instancesToImgs = instancesToImgs\n",
    "\n",
    "    def get_vid_ids(self, vidIds=[]):\n",
    "        \"\"\"Get video ids that satisfy given filter conditions.\n",
    "        Default return all video ids.\n",
    "        Args:\n",
    "            vidIds (list[int]): The given video ids. Defaults to [].\n",
    "        Returns:\n",
    "            list[int]: Video ids.\n",
    "        \"\"\"\n",
    "        vidIds = vidIds if _isArrayLike(vidIds) else [vidIds]\n",
    "\n",
    "        if len(vidIds) == 0:\n",
    "            ids = self.videos.keys()\n",
    "        else:\n",
    "            ids = set(vidIds)\n",
    "\n",
    "        return list(ids)\n",
    "\n",
    "    def get_img_ids_from_vid(self, vidId):\n",
    "        \"\"\"Get image ids from given video id.\n",
    "        Args:\n",
    "            vidId (int): The given video id.\n",
    "        Returns:\n",
    "            list[int]: Image ids of given video id.\n",
    "        \"\"\"\n",
    "        img_infos = self.vidToImgs[vidId]\n",
    "        ids = list(np.zeros([len(img_infos)], dtype=np.int64))\n",
    "\n",
    "        for i, img_info in enumerate(img_infos):\n",
    "            ids[i] = img_info[\"id\"]\n",
    "        # for img_info in img_infos:\n",
    "        #     ids[img_info['frame_id']] = img_info['id']\n",
    "            \n",
    "        return ids\n",
    "\n",
    "    def get_ins_ids_from_vid(self, vidId):\n",
    "        \"\"\"Get instance ids from given video id.\n",
    "        Args:\n",
    "            vidId (int): The given video id.\n",
    "        Returns:\n",
    "            list[int]: Instance ids of given video id.\n",
    "        \"\"\"\n",
    "        return self.vidToInstances[vidId]\n",
    "\n",
    "    def get_img_ids_from_ins_id(self, insId):\n",
    "        \"\"\"Get image ids from given instance id.\n",
    "        Args:\n",
    "            insId (int): The given instance id.\n",
    "        Returns:\n",
    "            list[int]: Image ids of given instance id.\n",
    "        \"\"\"\n",
    "        return self.instancesToImgs[insId]\n",
    "\n",
    "    def load_vids(self, ids=[]):\n",
    "        \"\"\"Get video information of given video ids.\n",
    "        Default return all videos information.\n",
    "        Args:\n",
    "            ids (list[int]): The given video ids. Defaults to [].\n",
    "        Returns:\n",
    "            list[dict]: List of video information.\n",
    "        \"\"\"\n",
    "        if _isArrayLike(ids):\n",
    "            return [self.videos[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.videos[ids]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "from numpy import random as rand\n",
    "import PIL\n",
    "\n",
    "\n",
    "def bbox_overlaps(bboxes1, bboxes2, mode='iou', eps=1e-6):\n",
    "    assert mode in ['iou', 'iof']\n",
    "    bboxes1 = bboxes1.astype(np.float32)\n",
    "    bboxes2 = bboxes2.astype(np.float32)\n",
    "    rows = bboxes1.shape[0]\n",
    "    cols = bboxes2.shape[0]\n",
    "    ious = np.zeros((rows, cols), dtype=np.float32)\n",
    "    if rows * cols == 0:\n",
    "        return ious\n",
    "    exchange = False\n",
    "    if bboxes1.shape[0] > bboxes2.shape[0]:\n",
    "        bboxes1, bboxes2 = bboxes2, bboxes1\n",
    "        ious = np.zeros((cols, rows), dtype=np.float32)\n",
    "        exchange = True\n",
    "    area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
    "    area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
    "    for i in range(bboxes1.shape[0]):\n",
    "        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n",
    "        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n",
    "        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n",
    "        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n",
    "        overlap = np.maximum(x_end - x_start, 0) * np.maximum(y_end - y_start, 0)\n",
    "        if mode == 'iou':\n",
    "            union = area1[i] + area2 - overlap\n",
    "        else:\n",
    "            union = area1[i] if not exchange else area2\n",
    "        union = np.maximum(union, eps)\n",
    "        ious[i, :] = overlap / union\n",
    "    if exchange:\n",
    "        ious = ious.T\n",
    "    return ious\n",
    "\n",
    "\n",
    "def crop(clip, target, region):\n",
    "    cropped_image = []\n",
    "    for image in clip:\n",
    "        cropped_image.append(F.crop(image, *region))\n",
    "\n",
    "    target = target.copy()\n",
    "    i, j, h, w = region\n",
    "\n",
    "    # should we do something wrt the original size?\n",
    "    target[\"size\"] = torch.tensor([h, w])\n",
    "\n",
    "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
    "\n",
    "    if \"boxes\" in target:\n",
    "        boxes = target[\"boxes\"]\n",
    "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
    "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
    "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
    "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
    "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
    "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
    "        target[\"area\"] = area\n",
    "        fields.append(\"boxes\")\n",
    "\n",
    "    if \"masks\" in target:\n",
    "        # FIXME should we update the area here if there are no boxes?\n",
    "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
    "        fields.append(\"masks\")\n",
    "\n",
    "    return cropped_image, target\n",
    "\n",
    "\n",
    "def hflip(clip, target):\n",
    "    flipped_image = []\n",
    "    for image in clip:\n",
    "        flipped_image.append(F.hflip(image))\n",
    "\n",
    "    w, h = clip[0].size\n",
    "\n",
    "    targets = target.copy()\n",
    "    \n",
    "    for target in targets:\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
    "            target[\"boxes\"] = boxes\n",
    "\n",
    "        if \"masks\" in target:\n",
    "            target['masks'] = target['masks'].flip(-1)\n",
    "    \n",
    "    return flipped_image, targets\n",
    "\n",
    "def vflip(clip,target):\n",
    "    flipped_image = []\n",
    "    for image in clip:\n",
    "        flipped_image.append(F.vflip(image))\n",
    "    w, h = clip[0].size\n",
    "\n",
    "    targets = target.copy()\n",
    "    for target in targets:\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes = boxes[:, [0, 3, 2, 1]] * torch.as_tensor([1, -1, 1, -1]) + torch.as_tensor([0, h, 0, h])\n",
    "            target[\"boxes\"] = boxes\n",
    "        if \"masks\" in target:\n",
    "            target['masks'] = target['masks'].flip(1)\n",
    "\n",
    "    return flipped_image, targets\n",
    "\n",
    "def resize(clip, target, size, max_size=None):\n",
    "    # size can be min_size (scalar) or (w, h) tuple\n",
    "\n",
    "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
    "        w, h = image_size\n",
    "        if max_size is not None:\n",
    "            min_original_size = float(min((w, h)))\n",
    "            max_original_size = float(max((w, h)))\n",
    "            if max_original_size / min_original_size * size > max_size:\n",
    "                size = int(round(max_size * min_original_size / max_original_size))\n",
    "\n",
    "        if (w <= h and w == size) or (h <= w and h == size):\n",
    "            return (h, w)\n",
    "\n",
    "        if w < h:\n",
    "            ow = size\n",
    "            oh = int(size * h / w)\n",
    "        else:\n",
    "            oh = size\n",
    "            ow = int(size * w / h)\n",
    "\n",
    "        return (oh, ow)\n",
    "\n",
    "    def get_size(image_size, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            return size[::-1]\n",
    "        else:\n",
    "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
    "\n",
    "    size = get_size(clip[0].size, size, max_size)\n",
    "    rescaled_image = []\n",
    "    for image in clip:\n",
    "        rescaled_image.append(F.resize(image, size))\n",
    "\n",
    "    if target is None:\n",
    "        return rescaled_image, None\n",
    "\n",
    "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image[0].size, clip[0].size))\n",
    "    ratio_width, ratio_height = ratios\n",
    "\n",
    "\n",
    "    \n",
    "    targets = target.copy()\n",
    "    for target in targets:\n",
    "        # print(\"transforms_nips_164\", target)\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
    "            target[\"boxes\"] = scaled_boxes\n",
    "\n",
    "        if \"area\" in target:\n",
    "            area = target[\"area\"]\n",
    "            scaled_area = area * (ratio_width * ratio_height)\n",
    "            target[\"area\"] = scaled_area\n",
    "\n",
    "        h, w = size\n",
    "        target[\"size\"] = torch.tensor([h, w])\n",
    "\n",
    "        if \"masks\" in target:\n",
    "            if target['masks'].shape[0]>0:\n",
    "                target['masks'] = interpolate(\n",
    "                    target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
    "            else:\n",
    "                target['masks'] = torch.zeros((target['masks'].shape[0],h,w))\n",
    "    return rescaled_image, targets\n",
    "\n",
    "\n",
    "def pad(clip, target, padding):\n",
    "    # assumes that we only pad on the bottom right corners\n",
    "    padded_image = []\n",
    "    for image in clip:\n",
    "        padded_image.append(F.pad(image, (0, 0, padding[0], padding[1])))\n",
    "    if target is None:\n",
    "        return padded_image, None\n",
    "    target = target.copy()\n",
    "    # should we do something wrt the original size?\n",
    "    target[\"size\"] = torch.tensor(padded_image[0].size[::-1])\n",
    "    if \"masks\" in target:\n",
    "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
    "    return padded_image, target\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        region = T.RandomCrop.get_params(img, self.size)\n",
    "        return crop(img, target, region)\n",
    "\n",
    "\n",
    "class RandomSizeCrop(object):\n",
    "    def __init__(self, min_size: int, max_size: int):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
    "        w = random.randint(self.min_size, min(img[0].width, self.max_size))\n",
    "        h = random.randint(self.min_size, min(img[0].height, self.max_size))\n",
    "        region = T.RandomCrop.get_params(img[0], [h, w])\n",
    "        return crop(img, target, region)\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        image_width, image_height = img.size\n",
    "        crop_height, crop_width = self.size\n",
    "        crop_top = int(round((image_height - crop_height) / 2.))\n",
    "        crop_left = int(round((image_width - crop_width) / 2.))\n",
    "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
    "\n",
    "\n",
    "class MinIoURandomCrop(object):\n",
    "    def __init__(self, min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3):\n",
    "        self.min_ious = min_ious\n",
    "        self.sample_mode = (1, *min_ious, 0)\n",
    "        self.min_crop_size = min_crop_size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        w,h = img.size\n",
    "        while True:\n",
    "            mode = random.choice(self.sample_mode)\n",
    "            self.mode = mode\n",
    "            if mode == 1:\n",
    "                return img,target\n",
    "            min_iou = mode\n",
    "            boxes = target['boxes'].numpy()\n",
    "            labels = target['labels']\n",
    "\n",
    "            for i in range(50):\n",
    "                new_w = rand.uniform(self.min_crop_size * w, w)\n",
    "                new_h = rand.uniform(self.min_crop_size * h, h)\n",
    "                if new_h / new_w < 0.5 or new_h / new_w > 2:\n",
    "                    continue\n",
    "                left = rand.uniform(w - new_w)\n",
    "                top = rand.uniform(h - new_h)\n",
    "                patch = np.array((int(left), int(top), int(left + new_w), int(top + new_h)))\n",
    "                if patch[2] == patch[0] or patch[3] == patch[1]:\n",
    "                    continue\n",
    "                overlaps = bbox_overlaps(patch.reshape(-1, 4), boxes.reshape(-1, 4)).reshape(-1)\n",
    "                if len(overlaps) > 0 and overlaps.min() < min_iou:\n",
    "                    continue\n",
    "                \n",
    "                if len(overlaps) > 0:\n",
    "                    def is_center_of_bboxes_in_patch(boxes, patch):\n",
    "                        center = (boxes[:, :2] + boxes[:, 2:]) / 2\n",
    "                        mask = ((center[:, 0] > patch[0]) * (center[:, 1] > patch[1]) * (center[:, 0] < patch[2]) * (center[:, 1] < patch[3]))\n",
    "                        return mask\n",
    "                    mask = is_center_of_bboxes_in_patch(boxes, patch)\n",
    "                    if False in mask:\n",
    "                        continue\n",
    "                    #TODO: use no center boxes\n",
    "                    #if not mask.any():\n",
    "                    #    continue\n",
    "\n",
    "                    boxes[:, 2:] = boxes[:, 2:].clip(max=patch[2:])\n",
    "                    boxes[:, :2] = boxes[:, :2].clip(min=patch[:2])\n",
    "                    boxes -= np.tile(patch[:2], 2)\n",
    "                    target['boxes'] = torch.tensor(boxes)\n",
    "                \n",
    "                img = np.asarray(img)[patch[1]:patch[3], patch[0]:patch[2]]\n",
    "                img = Image.fromarray(img)\n",
    "                width, height = img.size\n",
    "                target['orig_size'] = torch.tensor([height,width])\n",
    "                target['size'] = torch.tensor([height,width])\n",
    "                return img,target \n",
    "\n",
    "\n",
    "class RandomContrast(object):\n",
    "    def __init__(self, lower=0.5, upper=1.5):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n",
    "        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n",
    "    def __call__(self, image, target):\n",
    "        \n",
    "        if rand.randint(2):\n",
    "            alpha = rand.uniform(self.lower, self.upper)\n",
    "            image *= alpha\n",
    "        return image, target\n",
    "\n",
    "class RandomBrightness(object):\n",
    "    def __init__(self, delta=32):\n",
    "        assert delta >= 0.0\n",
    "        assert delta <= 255.0\n",
    "        self.delta = delta\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            delta = rand.uniform(-self.delta, self.delta)\n",
    "            image += delta\n",
    "        return image, target\n",
    "\n",
    "class RandomSaturation(object):\n",
    "    def __init__(self, lower=0.5, upper=1.5):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n",
    "        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            image[:, :, 1] *= rand.uniform(self.lower, self.upper)\n",
    "        return image, target\n",
    "\n",
    "class RandomHue(object): #\n",
    "    def __init__(self, delta=18.0):\n",
    "        assert delta >= 0.0 and delta <= 360.0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            image[:, :, 0] += rand.uniform(-self.delta, self.delta)\n",
    "            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n",
    "            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n",
    "        return image, target\n",
    "\n",
    "class RandomLightingNoise(object):\n",
    "    def __init__(self):\n",
    "        self.perms = ((0, 1, 2), (0, 2, 1),\n",
    "                      (1, 0, 2), (1, 2, 0),\n",
    "                      (2, 0, 1), (2, 1, 0))\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            swap = self.perms[rand.randint(len(self.perms))]\n",
    "            shuffle = SwapChannels(swap)  # shuffle channels\n",
    "            image = shuffle(image)\n",
    "        return image, target\n",
    "\n",
    "class ConvertColor(object):\n",
    "    def __init__(self, current='BGR', transform='HSV'):\n",
    "        self.transform = transform\n",
    "        self.current = current\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if self.current == 'BGR' and self.transform == 'HSV':\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        elif self.current == 'HSV' and self.transform == 'BGR':\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return image, target\n",
    "\n",
    "class SwapChannels(object):\n",
    "    def __init__(self, swaps):\n",
    "        self.swaps = swaps\n",
    "    def __call__(self, image):\n",
    "        image = image[:, :, self.swaps]\n",
    "        return image\n",
    "\n",
    "class PhotometricDistort(object):\n",
    "    def __init__(self):\n",
    "        self.pd = [\n",
    "            RandomContrast(),\n",
    "            ConvertColor(transform='HSV'),\n",
    "            RandomSaturation(),\n",
    "            RandomHue(),\n",
    "            ConvertColor(current='HSV', transform='BGR'),\n",
    "            RandomContrast()\n",
    "        ]\n",
    "        self.rand_brightness = RandomBrightness()\n",
    "        self.rand_light_noise = RandomLightingNoise()\n",
    "    \n",
    "    def __call__(self,clip,target):\n",
    "        imgs = []\n",
    "        for img in clip:\n",
    "            img = np.asarray(img).astype('float32')\n",
    "            img, target = self.rand_brightness(img, target)\n",
    "            if rand.randint(2):\n",
    "                distort = Compose(self.pd[:-1])\n",
    "            else:\n",
    "                distort = Compose(self.pd[1:])\n",
    "            img, target = distort(img, target)\n",
    "            img, target = self.rand_light_noise(img, target)\n",
    "            imgs.append(Image.fromarray(img.astype('uint8')))\n",
    "        return imgs, target\n",
    "\n",
    "#NOTICE: if used for mask, need to change\n",
    "class Expand(object):\n",
    "    def __init__(self, mean):\n",
    "        self.mean = mean\n",
    "    def __call__(self, clip, target):\n",
    "        if rand.randint(2):\n",
    "            return clip,target\n",
    "        imgs = []\n",
    "        masks = []\n",
    "        image = np.asarray(clip[0]).astype('float32')\n",
    "        height, width, depth = image.shape\n",
    "        ratio = rand.uniform(1, 4)\n",
    "        left = rand.uniform(0, width*ratio - width)\n",
    "        top = rand.uniform(0, height*ratio - height)\n",
    "        for i in range(len(clip)):\n",
    "            image = np.asarray(clip[i]).astype('float32')\n",
    "            expand_image = np.zeros((int(height*ratio), int(width*ratio), depth),dtype=image.dtype)\n",
    "            expand_image[:, :, :] = self.mean\n",
    "            expand_image[int(top):int(top + height),int(left):int(left + width)] = image\n",
    "            imgs.append(Image.fromarray(expand_image.astype('uint8')))\n",
    "            expand_mask = torch.zeros((int(height*ratio), int(width*ratio)),dtype=torch.uint8)\n",
    "            expand_mask[int(top):int(top + height),int(left):int(left + width)] = target['masks'][i]\n",
    "            masks.append(expand_mask)\n",
    "        boxes = target['boxes'].numpy()\n",
    "        boxes[:, :2] += (int(left), int(top))\n",
    "        boxes[:, 2:] += (int(left), int(top))\n",
    "        target['boxes'] = torch.tensor(boxes)\n",
    "        target['masks']=torch.stack(masks)\n",
    "        return imgs, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return hflip(img, target)\n",
    "        return img, target\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return vflip(img, target)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomResize(object):\n",
    "    def __init__(self, sizes, max_size=None):\n",
    "        assert isinstance(sizes, (list, tuple))\n",
    "        self.sizes = sizes\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img, target=None):\n",
    "        size = random.choice(self.sizes)\n",
    "        return resize(img, target, size, self.max_size)\n",
    "\n",
    "\n",
    "class RandomPad(object):\n",
    "    def __init__(self, max_pad):\n",
    "        self.max_pad = max_pad\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        pad_x = random.randint(0, self.max_pad)\n",
    "        pad_y = random.randint(0, self.max_pad)\n",
    "        return pad(img, target, (pad_x, pad_y))\n",
    "\n",
    "\n",
    "class RandomSelect(object):\n",
    "    \"\"\"\n",
    "    Randomly selects between transforms1 and transforms2,\n",
    "    with probability p for transforms1 and (1 - p) for transforms2\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms1, transforms2, p=0.5):\n",
    "        self.transforms1 = transforms1\n",
    "        self.transforms2 = transforms2\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return self.transforms1(img, target)\n",
    "        return self.transforms2(img, target)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, clip, target):\n",
    "        img = []\n",
    "        for im in clip:\n",
    "            img.append(F.to_tensor(im))\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomErasing(object):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.eraser(img), target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, clip, target=None):\n",
    "        image = []\n",
    "        for im in clip:\n",
    "            image.append(F.normalize(im, mean=self.mean, std=self.std))\n",
    "        if target is None:\n",
    "            return image, None\n",
    "        targets = target.copy()\n",
    "        for i, target in enumerate(targets):\n",
    "            h, w = image[i].shape[-2:]\n",
    "            if \"boxes\" in target:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes = box_xyxy_to_cxcywh(boxes)\n",
    "                boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "                target[\"boxes\"] = boxes\n",
    "        return image, targets\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += \"    {0}\".format(t)\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "class CocoDetection(TvCocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks, num_frames = 4,\n",
    "        is_train = True,  filter_key_img=True,  cache_mode=False, local_rank=0, local_size=1):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file,\n",
    "                                            cache_mode=cache_mode, local_rank=local_rank, local_size=local_size)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "        # self.prepare_seq = ConvertCocoSeqPolysToMask(return_masks)\n",
    "        self.ann_file = ann_file\n",
    "        self.frame_range = [-2, 2]\n",
    "        self.num_ref_frames = num_frames - 1\n",
    "        self.cocovid = CocoVID(self.ann_file)\n",
    "        self.is_train = is_train\n",
    "        self.filter_key_img = filter_key_img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        imgs = []\n",
    "        tgts = []\n",
    "\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        path = img_info['file_name']\n",
    "        video_id = img_info['video_id']\n",
    "        img = self.get_image(path)\n",
    "        target = {'image_id': img_id,'video_id': video_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        imgs.append(img)\n",
    "        tgts.append(target)\n",
    "        if video_id == -1:\n",
    "            for i in range(self.num_ref_frames):\n",
    "                imgs.append(copy.deepcopy(img))\n",
    "                tgts.append(copy.deepcopy(target))\n",
    "        else:\n",
    "            img_ids = self.cocovid.get_img_ids_from_vid(video_id) \n",
    "            #print(\"length\", len(img_ids))\n",
    "            ref_img_ids = []\n",
    "            if self.is_train:\n",
    "                interval = 5 # *20\n",
    "                left = max(img_ids[0], img_id - interval)\n",
    "                right = min(img_ids[-1], img_id + interval)\n",
    "                sample_range = list(range(left, right))\n",
    "                if self.filter_key_img and img_id in sample_range:\n",
    "                    sample_range.remove(img_id)\n",
    "                if self.num_ref_frames >= 10:\n",
    "                    sample_range = img_ids\n",
    "                while self.num_ref_frames > len(sample_range):\n",
    "                    sample_range.extend(sample_range)\n",
    "                ref_img_ids = random.sample(sample_range, self.num_ref_frames)\n",
    "\n",
    "            else:\n",
    "                #print(\"------------------------------\")i\n",
    "                ref_img_ids = []\n",
    "                Len = len(img_ids)\n",
    "                interval  = max(int(Len // 15), 1)  #\n",
    "                left_indexs = int((img_id - img_ids[0]) // interval)\n",
    "                right_indexs = int((img_ids[-1] - img_id) // interval)\n",
    "                if left_indexs < self.num_ref_frames:\n",
    "                   for i in range(self.num_ref_frames):\n",
    "                       ref_img_ids.append(min(img_id + (i+1)*interval, img_ids[-1]))\n",
    "                else:\n",
    "                   for i in range(self.num_ref_frames):\n",
    "                       ref_img_ids.append(max(img_id - (i+1)* interval, img_ids[0]))\n",
    "\n",
    "                # print(\"ref_img_ids\", ref_img_ids)\n",
    "            for ref_img_id in ref_img_ids:\n",
    "                ref_ann_ids = coco.getAnnIds(imgIds=ref_img_id)\n",
    "                ref_img_info = coco.loadImgs(ref_img_id)[0]\n",
    "                ref_img_path = ref_img_info['file_name']\n",
    "                ref_img = self.get_image(ref_img_path)\n",
    "                ref_target = coco.loadAnns(ref_ann_ids)\n",
    "                ref_target = {'image_id': ref_img_id, 'video_id': video_id, 'annotations': ref_target}\n",
    "                ref_img, ref_target = self.prepare(ref_img, ref_target)\n",
    "                imgs.append(ref_img)\n",
    "                tgts.append(ref_target)\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            imgs, target = self._transforms(imgs, tgts) \n",
    "\n",
    "        return  torch.cat(imgs, dim=0),  target\n",
    "\n",
    "\n",
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "\n",
    "class ConvertCocoPolysToMask(object):\n",
    "    def __init__(self, return_masks=False):\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        if self.return_masks:\n",
    "            masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        if self.return_masks:\n",
    "            target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "def make_coco_transforms(image_set):\n",
    "\n",
    "    normalize = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "    if image_set == 'train_vid' or image_set == \"train_det\" or image_set == \"train_joint\":\n",
    "        return Compose([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomResize([600], max_size=1000),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return Compose([\n",
    "            RandomResize([600], max_size=1000),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "def build_vid_multi(image_set, args):\n",
    "    root = Path(args[\"vid_path\"])\n",
    "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "    mode = 'instances'\n",
    "    PATHS = {\n",
    "        \"train_det\": [(root / \"Data\" / \"DET\", root / \"annotations\" / 'imagenet_det_30plus1cls_vid_train.json')], #Thoses two files we don't have\n",
    "        \"train_vid\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_train.json')],\n",
    "        \"train_joint\": [(root / \"Data\" , root / \"annotations\" / 'imagenet_vid_train_joint_30.json')],\n",
    "        \"val\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_val.json')],\n",
    "    }\n",
    "    datasets = []\n",
    "    for (img_folder, ann_file) in PATHS[image_set]:\n",
    "        dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), is_train =(not args[\"eval\"]), return_masks=args[\"masks\"], cache_mode=args[\"cache_mode\"], local_rank=0, local_size=1, num_frames=args[\"num_frames\"])\n",
    "        datasets.append(dataset)\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vid multi eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools import mask as coco_mask\n",
    "def ChooseFrame(List, Gap, num_frames):\n",
    "    ret = []\n",
    "    start_id = 0\n",
    "    max_gap = Gap*num_frames\n",
    "    num = len(List) // max_gap\n",
    "    for i in range(num):\n",
    "        start_id = i * max_gap\n",
    "        for j in range(Gap):\n",
    "            tmp = []\n",
    "            for k in range(num_frames):\n",
    "                tmp.append(List[start_id + j + k * Gap])\n",
    "            ret.append(copy.deepcopy(tmp))\n",
    "\n",
    "    if num * max_gap == len(List):\n",
    "        return ret\n",
    "\n",
    "    new_list = List[num * max_gap:]\n",
    "    random.shuffle(new_list)\n",
    "    ret.extend(np.array_split(new_list, len(new_list) // num_frames))\n",
    "    return ret\n",
    "\n",
    "class CocoDetection(TvCocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks, num_frames= 4,\n",
    "        is_train = True,  filter_key_img=True,  cache_mode=False, local_rank=0, local_size=1, gap = 1, is_shuffle=True):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file,\n",
    "                                            cache_mode=cache_mode, local_rank=local_rank, local_size=local_size)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "        self.ann_file = ann_file\n",
    "        self.frame_range = [-2, 2]\n",
    "        self.num_frames = num_frames\n",
    "        self.cocovid = CocoVID(self.ann_file)\n",
    "        self.vid_ids = self.cocovid.get_vid_ids()\n",
    "        self.img_ids = []\n",
    "        import numpy as np\n",
    "        import math\n",
    "        import copy\n",
    "        \n",
    "        for vid_id in self.vid_ids:\n",
    "            single_video_img_ids = self.cocovid.get_img_ids_from_vid(vid_id)\n",
    "            while len(single_video_img_ids) < num_frames:\n",
    "                single_video_img_ids.extend(copy.deepcopy(single_video_img_ids))\n",
    "            nums = math.ceil(len(single_video_img_ids)* 1.0 / num_frames) # 4\n",
    "            offset = nums * num_frames - len(single_video_img_ids) # 1\n",
    "            if offset != 0 :\n",
    "                single_video_img_ids.extend(copy.deepcopy(single_video_img_ids[-offset:]))\n",
    "            if is_shuffle:\n",
    "                random.shuffle(single_video_img_ids) \n",
    "            self.img_ids.extend(ChooseFrame(single_video_img_ids, gap, num_frames))\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.filter_key_img = filter_key_img\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        imgs = [] \n",
    "        tgts = []\n",
    "\n",
    "        idxs = self.img_ids[idx]\n",
    "        for i in idxs:    \n",
    "            coco = self.coco\n",
    "            img_id = self.ids[i-1]\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "            target = coco.loadAnns(ann_ids)\n",
    "            img_info = coco.loadImgs(img_id)[0]\n",
    "            path = img_info['file_name']\n",
    "            video_id = img_info['video_id']\n",
    "            img = self.get_image(path)\n",
    "            target = {'image_id': img_id, 'annotations': target, 'path': path}\n",
    "            img, target = self.prepare(img, target)\n",
    "            imgs.append(img)\n",
    "            tgts.append(target)\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            imgs, tgts = self._transforms(imgs, tgts)\n",
    "\n",
    "        for target_item in tgts:\n",
    "            target_item['path'] = path\n",
    "        \n",
    "        return  torch.cat(imgs, dim=0),  tgts\n",
    "\n",
    "\n",
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "\n",
    "class ConvertCocoPolysToMask(object):\n",
    "    def __init__(self, return_masks=False):\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        if self.return_masks:\n",
    "            masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        if self.return_masks:\n",
    "            target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "def make_coco_transforms(image_set):\n",
    "\n",
    "    normalize = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "    if image_set == 'train_vid' or image_set == \"train_det\" or image_set == \"train_joint\":\n",
    "        return Compose([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomResize([600], max_size=1000),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return Compose([\n",
    "            RandomResize([600], max_size=1000),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "def build_vid_multi_eval(image_set, args):\n",
    "    root = Path(args[\"vid_path\"])\n",
    "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "    mode = 'instances'\n",
    "    PATHS = {\n",
    "        \"train_det\": [(root / \"Data\" / \"DET\", root / \"annotations\" / 'imagenet_det_30plus1cls_vid_train.json')],\n",
    "        \"train_vid\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_train.json')],\n",
    "        \"train_joint\": [(root / \"Data\" , root / \"annotations\" / 'imagenet_vid_train_joint_30.json')],\n",
    "        \"val\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_val.json')],\n",
    "    }\n",
    "    datasets = []\n",
    "    for (img_folder, ann_file) in PATHS[image_set]:\n",
    "        dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), is_train =(not args[\"eval\"]), \n",
    "                                num_frames = args[\"num_frames\"], return_masks=args[\"masks\"], cache_mode=args[\"cache_mode\"], \n",
    "                                local_rank=0, local_size=1, gap = args[\"gap\"], is_shuffle=args[\"is_shuffle\"])\n",
    "        datasets.append(dataset)\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_api_from_dataset(dataset):\n",
    "    for _ in range(10):\n",
    "        # if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "        #     break\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            dataset = dataset.dataset\n",
    "    if isinstance(dataset, TvCocoDetection):\n",
    "        return dataset.coco\n",
    "\n",
    "\n",
    "def build_dataset(image_set, args):\n",
    "    if args[\"dataset_file\"] == \"vid_multi\":\n",
    "        return build_vid_multi(image_set, args)\n",
    "    if args[\"dataset_file\"] == \"vid_multi_eval\":\n",
    "        return build_vid_multi_eval(image_set, args)\n",
    "    raise ValueError(f'dataset {args.dataset_file} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main, Train the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions used for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not False: #is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "    \n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = 1\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    \n",
    "def get_total_grad_norm(parameters, norm_type=2):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    device = parameters[0].grad.device\n",
    "    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]),\n",
    "                            norm_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import sys\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    metric_logger.add_meter('grad_norm', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "\n",
    "\n",
    "    # prefetcher = data_prefetcher(data_loader, device, prefetch=True)\n",
    "    # data_loader_iter = iter(data_loader)\n",
    "    # samples, targets = data_loader_iter.next()\n",
    "    # samples = samples.to(device)\n",
    "    # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "    # for _ in metric_logger.log_every(range(len(data_loader)), print_freq, header):\n",
    "\n",
    "        # assert samples is None, samples\n",
    "        # outputs = model(samples)\n",
    "        samples = samples.to(device)\n",
    "        #print(\"engine_target_shape\",targets)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets[0]]\n",
    "        # print(\"targets\", targets)\n",
    "        # print(\"input model\", type(samples))\n",
    "        outputs = model(samples)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    " \n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict) #won't change anything on single gpu\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if max_norm > 0:\n",
    "            grad_total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        else:\n",
    "            grad_total_norm = get_total_grad_norm(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(grad_norm=grad_total_norm)\n",
    "\n",
    "        # samples, ref_samples, targets = prefetcher.next()\n",
    "        # try: \n",
    "        #     samples, targets = data_loader_iter.next()\n",
    "        # except StopIteration:\n",
    "        #     data_loader_iter = iter(data_loader)\n",
    "        #     samples,targets = data_loader_iter.next()\n",
    "        # samples = samples.to(device)\n",
    "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "import contextlib\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = 1\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "\n",
    "def merge(img_ids, eval_imgs):\n",
    "    all_img_ids = all_gather(img_ids)\n",
    "    all_eval_imgs = all_gather(eval_imgs)\n",
    "\n",
    "    merged_img_ids = []\n",
    "    for p in all_img_ids:\n",
    "        merged_img_ids.extend(p)\n",
    "\n",
    "    merged_eval_imgs = []\n",
    "    for p in all_eval_imgs:\n",
    "        merged_eval_imgs.append(p)\n",
    "\n",
    "    merged_img_ids = np.array(merged_img_ids)\n",
    "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
    "\n",
    "    # keep only unique (and in sorted order) images\n",
    "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
    "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
    "\n",
    "    return merged_img_ids, merged_eval_imgs\n",
    "\n",
    "\n",
    "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
    "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
    "    img_ids = list(img_ids)\n",
    "    eval_imgs = list(eval_imgs.flatten())\n",
    "\n",
    "    coco_eval.evalImgs = eval_imgs\n",
    "    coco_eval.params.imgIds = img_ids\n",
    "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# From pycocotools, just removed the prints and fixed\n",
    "# a Python3 bug about unicode not defined\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def evaluate_coco(self):\n",
    "    '''\n",
    "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
    "    :return: None\n",
    "    '''\n",
    "    # tic = time.time()\n",
    "    # print('Running per image evaluation...')\n",
    "    p = self.params\n",
    "    # add backward compatibility if useSegm is specified in params\n",
    "    if p.useSegm is not None:\n",
    "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
    "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
    "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
    "    p.imgIds = list(np.unique(p.imgIds))\n",
    "    if p.useCats:\n",
    "        p.catIds = list(np.unique(p.catIds))\n",
    "    p.maxDets = sorted(p.maxDets)\n",
    "    self.params = p\n",
    "\n",
    "    self._prepare()\n",
    "    # loop through images, area range, max detection number\n",
    "    catIds = p.catIds if p.useCats else [-1]\n",
    "\n",
    "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
    "        computeIoU = self.computeIoU\n",
    "    elif p.iouType == 'keypoints':\n",
    "        computeIoU = self.computeOks\n",
    "    self.ious = {\n",
    "        (imgId, catId): computeIoU(imgId, catId)\n",
    "        for imgId in p.imgIds\n",
    "        for catId in catIds}\n",
    "\n",
    "    evaluateImg = self.evaluateImg\n",
    "    maxDet = p.maxDets[-1]\n",
    "    evalImgs = [\n",
    "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
    "        for catId in catIds\n",
    "        for areaRng in p.areaRng\n",
    "        for imgId in p.imgIds\n",
    "    ]\n",
    "    # this is NOT in the pycocotools code, but could be done outside\n",
    "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
    "    self._paramsEval = copy.deepcopy(self.params)\n",
    "    # toc = time.time()\n",
    "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
    "    return p.imgIds, evalImgs\n",
    "\n",
    "class CocoEvaluator(object):\n",
    "    def __init__(self, coco_gt, iou_types):\n",
    "        assert isinstance(iou_types, (list, tuple))\n",
    "        coco_gt = copy.deepcopy(coco_gt)\n",
    "        self.coco_gt = coco_gt\n",
    "\n",
    "        self.iou_types = iou_types\n",
    "        self.coco_eval = {}\n",
    "        for iou_type in iou_types:\n",
    "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
    "\n",
    "        self.img_ids = []\n",
    "        self.eval_imgs = {k: [] for k in iou_types}\n",
    "\n",
    "    def update(self, predictions):\n",
    "        img_ids = list(np.unique(list(predictions.keys())))\n",
    "        self.img_ids.extend(img_ids)\n",
    "\n",
    "        for iou_type in self.iou_types:\n",
    "            results = self.prepare(predictions, iou_type)\n",
    "\n",
    "            # suppress pycocotools prints\n",
    "            with open(os.devnull, 'w') as devnull:\n",
    "                with contextlib.redirect_stdout(devnull):\n",
    "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
    "            coco_eval = self.coco_eval[iou_type]\n",
    "\n",
    "            coco_eval.cocoDt = coco_dt\n",
    "            coco_eval.params.imgIds = list(img_ids)\n",
    "            img_ids, eval_imgs = evaluate_coco(coco_eval)\n",
    "\n",
    "            self.eval_imgs[iou_type].append(eval_imgs)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
    "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
    "\n",
    "    def accumulate(self):\n",
    "        for coco_eval in self.coco_eval.values():\n",
    "            coco_eval.accumulate()\n",
    "\n",
    "    def summarize(self):\n",
    "        for iou_type, coco_eval in self.coco_eval.items():\n",
    "            print(\"IoU metric: {}\".format(iou_type))\n",
    "            coco_eval.summarize()\n",
    "\n",
    "    def prepare(self, predictions, iou_type):\n",
    "        if iou_type == \"bbox\":\n",
    "            return self.prepare_for_coco_detection(predictions)\n",
    "        elif iou_type == \"segm\":\n",
    "            return self.prepare_for_coco_segmentation(predictions)\n",
    "        elif iou_type == \"keypoints\":\n",
    "            return self.prepare_for_coco_keypoint(predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
    "\n",
    "    def prepare_for_coco_detection(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"bbox\": box,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, box in enumerate(boxes)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_segmentation(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            scores = prediction[\"scores\"]\n",
    "            labels = prediction[\"labels\"]\n",
    "            masks = prediction[\"masks\"]\n",
    "\n",
    "            masks = masks > 0.5\n",
    "\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            rles = [\n",
    "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
    "                for mask in masks\n",
    "            ]\n",
    "            for rle in rles:\n",
    "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"segmentation\": rle,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, rle in enumerate(rles)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_keypoint(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "            keypoints = prediction[\"keypoints\"]\n",
    "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        'keypoints': keypoint,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, keypoint in enumerate(keypoints)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Test:'\n",
    "\n",
    "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
    "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
    "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
    "\n",
    "    panoptic_evaluator = None\n",
    "    if 'panoptic' in postprocessors.keys():\n",
    "        panoptic_evaluator = PanopticEvaluator(\n",
    "            data_loader.dataset.ann_file,\n",
    "            data_loader.dataset.ann_folder,\n",
    "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
    "        )\n",
    "\n",
    "    for samples, targets  in metric_logger.log_every(data_loader, 10, header):\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items() if k!='path'} for t in targets[0]]\n",
    "\n",
    "        outputs = model(samples)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                             **loss_dict_reduced_scaled,\n",
    "                             **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "\n",
    "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
    "        if 'segm' in postprocessors.keys():\n",
    "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
    "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
    "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
    "        if coco_evaluator is not None:\n",
    "            coco_evaluator.update(res)\n",
    "\n",
    "        if panoptic_evaluator is not None:\n",
    "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
    "            for i, target in enumerate(targets):\n",
    "                image_id = target[\"image_id\"].item()\n",
    "                file_name = f\"{image_id:012d}.png\"\n",
    "                res_pano[i][\"image_id\"] = image_id\n",
    "                res_pano[i][\"file_name\"] = file_name\n",
    "\n",
    "            panoptic_evaluator.update(res_pano)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "    if panoptic_evaluator is not None:\n",
    "        panoptic_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "    panoptic_res = None\n",
    "    if panoptic_evaluator is not None:\n",
    "        panoptic_res = panoptic_evaluator.summarize()\n",
    "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "    if coco_evaluator is not None:\n",
    "        if 'bbox' in postprocessors.keys():\n",
    "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
    "        if 'segm' in postprocessors.keys():\n",
    "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
    "    if panoptic_res is not None:\n",
    "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
    "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
    "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n",
    "    return stats, coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_master(*args, **kwargs):\n",
    "    torch.save(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0002, 'lr_backbone_names': ['backbone.0'], 'lr_backbone': 2e-05, 'lr_linear_proj_names': ['reference_points', 'sampling_offsets'], 'lr_linear_proj_mult': 0.1, 'batch_size': 1, 'weight_decay': 0.0001, 'epochs': 7, 'lr_drop': 5, 'lr_drop_epochs': [5, 6], 'clip_max_norm': 0.1, 'num_ref_frames': 3, 'num_frames': 12, 'sgd': False, 'gap': 2, 'with_box_refine': True, 'two_stage': False, 'frozen_weights': None, 'pretrained': None, 'backbone': 'swin_b_p4w7', 'dilation': True, 'position_embedding': 'sine', 'position_embedding_scale': 6.283185307179586, 'num_feature_levels': 1, 'checkpoint': False, 'enc_layers': 6, 'dec_layers': 6, 'dim_feedforward': 1024, 'hidden_dim': 256, 'dropout': 0.1, 'nheads': 8, 'num_queries': 100, 'dec_n_points': 4, 'enc_n_points': 4, 'n_temporal_decoder_layers': 1, 'interval1': 20, 'interval2': 60, 'fixed_pretrained_model': False, 'is_shuffle': False, 'masks': False, 'aux_loss': False, 'set_cost_class': 2, 'set_cost_bbox': 5, 'set_cost_giou': 2, 'mask_loss_coef': 1, 'dice_loss_coef': 1, 'cls_loss_coef': 2, 'bbox_loss_coef': 5, 'giou_loss_coef': 2, 'focal_alpha': 0.25, 'dataset_file': 'vid_multi', 'coco_path': './data/coco', 'vid_path': './data/vid', 'coco_pretrain': False, 'coco_panoptic_path': '', 'remove_difficult': False, 'output_dir': 'Final_output', 'device': 'cuda', 'seed': 42, 'resume': './exps/exps_single/swinb_88.3/checkpoint0006.pth', 'start_epoch': 0, 'eval': False, 'num_workers': 0, 'cache_mode': False}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vid_multi 11111111\n",
      "vid_multi\n",
      "self.num_layers 4\n",
      "number of params: 106274744\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.12s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.19s)\n",
      "creating index...\n",
      "index created!\n",
      "transformer.level_embed\n",
      "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.0.norm1.weight\n",
      "transformer.encoder.layers.0.norm1.bias\n",
      "transformer.encoder.layers.0.linear1.weight\n",
      "transformer.encoder.layers.0.linear1.bias\n",
      "transformer.encoder.layers.0.linear2.weight\n",
      "transformer.encoder.layers.0.linear2.bias\n",
      "transformer.encoder.layers.0.norm2.weight\n",
      "transformer.encoder.layers.0.norm2.bias\n",
      "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.1.norm1.weight\n",
      "transformer.encoder.layers.1.norm1.bias\n",
      "transformer.encoder.layers.1.linear1.weight\n",
      "transformer.encoder.layers.1.linear1.bias\n",
      "transformer.encoder.layers.1.linear2.weight\n",
      "transformer.encoder.layers.1.linear2.bias\n",
      "transformer.encoder.layers.1.norm2.weight\n",
      "transformer.encoder.layers.1.norm2.bias\n",
      "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.2.norm1.weight\n",
      "transformer.encoder.layers.2.norm1.bias\n",
      "transformer.encoder.layers.2.linear1.weight\n",
      "transformer.encoder.layers.2.linear1.bias\n",
      "transformer.encoder.layers.2.linear2.weight\n",
      "transformer.encoder.layers.2.linear2.bias\n",
      "transformer.encoder.layers.2.norm2.weight\n",
      "transformer.encoder.layers.2.norm2.bias\n",
      "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.3.norm1.weight\n",
      "transformer.encoder.layers.3.norm1.bias\n",
      "transformer.encoder.layers.3.linear1.weight\n",
      "transformer.encoder.layers.3.linear1.bias\n",
      "transformer.encoder.layers.3.linear2.weight\n",
      "transformer.encoder.layers.3.linear2.bias\n",
      "transformer.encoder.layers.3.norm2.weight\n",
      "transformer.encoder.layers.3.norm2.bias\n",
      "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.4.norm1.weight\n",
      "transformer.encoder.layers.4.norm1.bias\n",
      "transformer.encoder.layers.4.linear1.weight\n",
      "transformer.encoder.layers.4.linear1.bias\n",
      "transformer.encoder.layers.4.linear2.weight\n",
      "transformer.encoder.layers.4.linear2.bias\n",
      "transformer.encoder.layers.4.norm2.weight\n",
      "transformer.encoder.layers.4.norm2.bias\n",
      "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.5.norm1.weight\n",
      "transformer.encoder.layers.5.norm1.bias\n",
      "transformer.encoder.layers.5.linear1.weight\n",
      "transformer.encoder.layers.5.linear1.bias\n",
      "transformer.encoder.layers.5.linear2.weight\n",
      "transformer.encoder.layers.5.linear2.bias\n",
      "transformer.encoder.layers.5.norm2.weight\n",
      "transformer.encoder.layers.5.norm2.bias\n",
      "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.0.norm1.weight\n",
      "transformer.decoder.layers.0.norm1.bias\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.0.norm2.weight\n",
      "transformer.decoder.layers.0.norm2.bias\n",
      "transformer.decoder.layers.0.linear1.weight\n",
      "transformer.decoder.layers.0.linear1.bias\n",
      "transformer.decoder.layers.0.linear2.weight\n",
      "transformer.decoder.layers.0.linear2.bias\n",
      "transformer.decoder.layers.0.norm3.weight\n",
      "transformer.decoder.layers.0.norm3.bias\n",
      "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.1.norm1.weight\n",
      "transformer.decoder.layers.1.norm1.bias\n",
      "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.1.norm2.weight\n",
      "transformer.decoder.layers.1.norm2.bias\n",
      "transformer.decoder.layers.1.linear1.weight\n",
      "transformer.decoder.layers.1.linear1.bias\n",
      "transformer.decoder.layers.1.linear2.weight\n",
      "transformer.decoder.layers.1.linear2.bias\n",
      "transformer.decoder.layers.1.norm3.weight\n",
      "transformer.decoder.layers.1.norm3.bias\n",
      "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.2.norm1.weight\n",
      "transformer.decoder.layers.2.norm1.bias\n",
      "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.2.norm2.weight\n",
      "transformer.decoder.layers.2.norm2.bias\n",
      "transformer.decoder.layers.2.linear1.weight\n",
      "transformer.decoder.layers.2.linear1.bias\n",
      "transformer.decoder.layers.2.linear2.weight\n",
      "transformer.decoder.layers.2.linear2.bias\n",
      "transformer.decoder.layers.2.norm3.weight\n",
      "transformer.decoder.layers.2.norm3.bias\n",
      "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.3.norm1.weight\n",
      "transformer.decoder.layers.3.norm1.bias\n",
      "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.3.norm2.weight\n",
      "transformer.decoder.layers.3.norm2.bias\n",
      "transformer.decoder.layers.3.linear1.weight\n",
      "transformer.decoder.layers.3.linear1.bias\n",
      "transformer.decoder.layers.3.linear2.weight\n",
      "transformer.decoder.layers.3.linear2.bias\n",
      "transformer.decoder.layers.3.norm3.weight\n",
      "transformer.decoder.layers.3.norm3.bias\n",
      "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.4.norm1.weight\n",
      "transformer.decoder.layers.4.norm1.bias\n",
      "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.4.norm2.weight\n",
      "transformer.decoder.layers.4.norm2.bias\n",
      "transformer.decoder.layers.4.linear1.weight\n",
      "transformer.decoder.layers.4.linear1.bias\n",
      "transformer.decoder.layers.4.linear2.weight\n",
      "transformer.decoder.layers.4.linear2.bias\n",
      "transformer.decoder.layers.4.norm3.weight\n",
      "transformer.decoder.layers.4.norm3.bias\n",
      "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.5.norm1.weight\n",
      "transformer.decoder.layers.5.norm1.bias\n",
      "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.5.norm2.weight\n",
      "transformer.decoder.layers.5.norm2.bias\n",
      "transformer.decoder.layers.5.linear1.weight\n",
      "transformer.decoder.layers.5.linear1.bias\n",
      "transformer.decoder.layers.5.linear2.weight\n",
      "transformer.decoder.layers.5.linear2.bias\n",
      "transformer.decoder.layers.5.norm3.weight\n",
      "transformer.decoder.layers.5.norm3.bias\n",
      "transformer.decoder.bbox_embed.0.layers.0.weight\n",
      "transformer.decoder.bbox_embed.0.layers.0.bias\n",
      "transformer.decoder.bbox_embed.0.layers.1.weight\n",
      "transformer.decoder.bbox_embed.0.layers.1.bias\n",
      "transformer.decoder.bbox_embed.0.layers.2.weight\n",
      "transformer.decoder.bbox_embed.0.layers.2.bias\n",
      "transformer.decoder.bbox_embed.1.layers.0.weight\n",
      "transformer.decoder.bbox_embed.1.layers.0.bias\n",
      "transformer.decoder.bbox_embed.1.layers.1.weight\n",
      "transformer.decoder.bbox_embed.1.layers.1.bias\n",
      "transformer.decoder.bbox_embed.1.layers.2.weight\n",
      "transformer.decoder.bbox_embed.1.layers.2.bias\n",
      "transformer.decoder.bbox_embed.2.layers.0.weight\n",
      "transformer.decoder.bbox_embed.2.layers.0.bias\n",
      "transformer.decoder.bbox_embed.2.layers.1.weight\n",
      "transformer.decoder.bbox_embed.2.layers.1.bias\n",
      "transformer.decoder.bbox_embed.2.layers.2.weight\n",
      "transformer.decoder.bbox_embed.2.layers.2.bias\n",
      "transformer.decoder.bbox_embed.3.layers.0.weight\n",
      "transformer.decoder.bbox_embed.3.layers.0.bias\n",
      "transformer.decoder.bbox_embed.3.layers.1.weight\n",
      "transformer.decoder.bbox_embed.3.layers.1.bias\n",
      "transformer.decoder.bbox_embed.3.layers.2.weight\n",
      "transformer.decoder.bbox_embed.3.layers.2.bias\n",
      "transformer.decoder.bbox_embed.4.layers.0.weight\n",
      "transformer.decoder.bbox_embed.4.layers.0.bias\n",
      "transformer.decoder.bbox_embed.4.layers.1.weight\n",
      "transformer.decoder.bbox_embed.4.layers.1.bias\n",
      "transformer.decoder.bbox_embed.4.layers.2.weight\n",
      "transformer.decoder.bbox_embed.4.layers.2.bias\n",
      "transformer.decoder.bbox_embed.5.layers.0.weight\n",
      "transformer.decoder.bbox_embed.5.layers.0.bias\n",
      "transformer.decoder.bbox_embed.5.layers.1.weight\n",
      "transformer.decoder.bbox_embed.5.layers.1.bias\n",
      "transformer.decoder.bbox_embed.5.layers.2.weight\n",
      "transformer.decoder.bbox_embed.5.layers.2.bias\n",
      "transformer.temporal_query_layer1.self_attn.in_proj_weight\n",
      "transformer.temporal_query_layer1.self_attn.in_proj_bias\n",
      "transformer.temporal_query_layer1.self_attn.out_proj.weight\n",
      "transformer.temporal_query_layer1.self_attn.out_proj.bias\n",
      "transformer.temporal_query_layer1.norm2.weight\n",
      "transformer.temporal_query_layer1.norm2.bias\n",
      "transformer.temporal_query_layer1.cross_attn.in_proj_weight\n",
      "transformer.temporal_query_layer1.cross_attn.in_proj_bias\n",
      "transformer.temporal_query_layer1.cross_attn.out_proj.weight\n",
      "transformer.temporal_query_layer1.cross_attn.out_proj.bias\n",
      "transformer.temporal_query_layer1.norm1.weight\n",
      "transformer.temporal_query_layer1.norm1.bias\n",
      "transformer.temporal_query_layer1.linear1.weight\n",
      "transformer.temporal_query_layer1.linear1.bias\n",
      "transformer.temporal_query_layer1.linear2.weight\n",
      "transformer.temporal_query_layer1.linear2.bias\n",
      "transformer.temporal_query_layer1.norm3.weight\n",
      "transformer.temporal_query_layer1.norm3.bias\n",
      "transformer.temporal_query_layer2.self_attn.in_proj_weight\n",
      "transformer.temporal_query_layer2.self_attn.in_proj_bias\n",
      "transformer.temporal_query_layer2.self_attn.out_proj.weight\n",
      "transformer.temporal_query_layer2.self_attn.out_proj.bias\n",
      "transformer.temporal_query_layer2.norm2.weight\n",
      "transformer.temporal_query_layer2.norm2.bias\n",
      "transformer.temporal_query_layer2.cross_attn.in_proj_weight\n",
      "transformer.temporal_query_layer2.cross_attn.in_proj_bias\n",
      "transformer.temporal_query_layer2.cross_attn.out_proj.weight\n",
      "transformer.temporal_query_layer2.cross_attn.out_proj.bias\n",
      "transformer.temporal_query_layer2.norm1.weight\n",
      "transformer.temporal_query_layer2.norm1.bias\n",
      "transformer.temporal_query_layer2.linear1.weight\n",
      "transformer.temporal_query_layer2.linear1.bias\n",
      "transformer.temporal_query_layer2.linear2.weight\n",
      "transformer.temporal_query_layer2.linear2.bias\n",
      "transformer.temporal_query_layer2.norm3.weight\n",
      "transformer.temporal_query_layer2.norm3.bias\n",
      "transformer.temporal_query_layer3.self_attn.in_proj_weight\n",
      "transformer.temporal_query_layer3.self_attn.in_proj_bias\n",
      "transformer.temporal_query_layer3.self_attn.out_proj.weight\n",
      "transformer.temporal_query_layer3.self_attn.out_proj.bias\n",
      "transformer.temporal_query_layer3.norm2.weight\n",
      "transformer.temporal_query_layer3.norm2.bias\n",
      "transformer.temporal_query_layer3.cross_attn.in_proj_weight\n",
      "transformer.temporal_query_layer3.cross_attn.in_proj_bias\n",
      "transformer.temporal_query_layer3.cross_attn.out_proj.weight\n",
      "transformer.temporal_query_layer3.cross_attn.out_proj.bias\n",
      "transformer.temporal_query_layer3.norm1.weight\n",
      "transformer.temporal_query_layer3.norm1.bias\n",
      "transformer.temporal_query_layer3.linear1.weight\n",
      "transformer.temporal_query_layer3.linear1.bias\n",
      "transformer.temporal_query_layer3.linear2.weight\n",
      "transformer.temporal_query_layer3.linear2.bias\n",
      "transformer.temporal_query_layer3.norm3.weight\n",
      "transformer.temporal_query_layer3.norm3.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.value_proj.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.value_proj.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.output_proj.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.output_proj.bias\n",
      "transformer.temporal_decoder1.layers.0.norm1.weight\n",
      "transformer.temporal_decoder1.layers.0.norm1.bias\n",
      "transformer.temporal_decoder1.layers.0.self_attn.in_proj_weight\n",
      "transformer.temporal_decoder1.layers.0.self_attn.in_proj_bias\n",
      "transformer.temporal_decoder1.layers.0.self_attn.out_proj.weight\n",
      "transformer.temporal_decoder1.layers.0.self_attn.out_proj.bias\n",
      "transformer.temporal_decoder1.layers.0.norm2.weight\n",
      "transformer.temporal_decoder1.layers.0.norm2.bias\n",
      "transformer.temporal_decoder1.layers.0.linear1.weight\n",
      "transformer.temporal_decoder1.layers.0.linear1.bias\n",
      "transformer.temporal_decoder1.layers.0.linear2.weight\n",
      "transformer.temporal_decoder1.layers.0.linear2.bias\n",
      "transformer.temporal_decoder1.layers.0.norm3.weight\n",
      "transformer.temporal_decoder1.layers.0.norm3.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.value_proj.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.value_proj.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.output_proj.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.output_proj.bias\n",
      "transformer.temporal_decoder2.layers.0.norm1.weight\n",
      "transformer.temporal_decoder2.layers.0.norm1.bias\n",
      "transformer.temporal_decoder2.layers.0.self_attn.in_proj_weight\n",
      "transformer.temporal_decoder2.layers.0.self_attn.in_proj_bias\n",
      "transformer.temporal_decoder2.layers.0.self_attn.out_proj.weight\n",
      "transformer.temporal_decoder2.layers.0.self_attn.out_proj.bias\n",
      "transformer.temporal_decoder2.layers.0.norm2.weight\n",
      "transformer.temporal_decoder2.layers.0.norm2.bias\n",
      "transformer.temporal_decoder2.layers.0.linear1.weight\n",
      "transformer.temporal_decoder2.layers.0.linear1.bias\n",
      "transformer.temporal_decoder2.layers.0.linear2.weight\n",
      "transformer.temporal_decoder2.layers.0.linear2.bias\n",
      "transformer.temporal_decoder2.layers.0.norm3.weight\n",
      "transformer.temporal_decoder2.layers.0.norm3.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.value_proj.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.value_proj.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.output_proj.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.output_proj.bias\n",
      "transformer.temporal_decoder3.layers.0.norm1.weight\n",
      "transformer.temporal_decoder3.layers.0.norm1.bias\n",
      "transformer.temporal_decoder3.layers.0.self_attn.in_proj_weight\n",
      "transformer.temporal_decoder3.layers.0.self_attn.in_proj_bias\n",
      "transformer.temporal_decoder3.layers.0.self_attn.out_proj.weight\n",
      "transformer.temporal_decoder3.layers.0.self_attn.out_proj.bias\n",
      "transformer.temporal_decoder3.layers.0.norm2.weight\n",
      "transformer.temporal_decoder3.layers.0.norm2.bias\n",
      "transformer.temporal_decoder3.layers.0.linear1.weight\n",
      "transformer.temporal_decoder3.layers.0.linear1.bias\n",
      "transformer.temporal_decoder3.layers.0.linear2.weight\n",
      "transformer.temporal_decoder3.layers.0.linear2.bias\n",
      "transformer.temporal_decoder3.layers.0.norm3.weight\n",
      "transformer.temporal_decoder3.layers.0.norm3.bias\n",
      "transformer.reference_points.weight\n",
      "transformer.reference_points.bias\n",
      "class_embed.0.weight\n",
      "class_embed.0.bias\n",
      "class_embed.1.weight\n",
      "class_embed.1.bias\n",
      "class_embed.2.weight\n",
      "class_embed.2.bias\n",
      "class_embed.3.weight\n",
      "class_embed.3.bias\n",
      "class_embed.4.weight\n",
      "class_embed.4.bias\n",
      "class_embed.5.weight\n",
      "class_embed.5.bias\n",
      "temp_class_embed.weight\n",
      "temp_class_embed.bias\n",
      "temp_bbox_embed.layers.0.weight\n",
      "temp_bbox_embed.layers.0.bias\n",
      "temp_bbox_embed.layers.1.weight\n",
      "temp_bbox_embed.layers.1.bias\n",
      "temp_bbox_embed.layers.2.weight\n",
      "temp_bbox_embed.layers.2.bias\n",
      "query_embed.weight\n",
      "input_proj.0.0.weight\n",
      "input_proj.0.0.bias\n",
      "input_proj.0.1.weight\n",
      "input_proj.0.1.bias\n",
      "backbone.0.body.fpn.inner_blocks.0.0.weight\n",
      "backbone.0.body.fpn.inner_blocks.0.0.bias\n",
      "backbone.0.body.fpn.inner_blocks.1.0.weight\n",
      "backbone.0.body.fpn.inner_blocks.1.0.bias\n",
      "backbone.0.body.fpn.inner_blocks.2.0.weight\n",
      "backbone.0.body.fpn.inner_blocks.2.0.bias\n",
      "backbone.0.body.fpn.layer_blocks.0.0.weight\n",
      "backbone.0.body.fpn.layer_blocks.0.0.bias\n",
      "backbone.0.body.fpn.layer_blocks.1.0.weight\n",
      "backbone.0.body.fpn.layer_blocks.1.0.bias\n",
      "backbone.0.body.fpn.layer_blocks.2.0.weight\n",
      "backbone.0.body.fpn.layer_blocks.2.0.bias\n",
      "backbone.0.body.patch_embed.proj.weight\n",
      "backbone.0.body.patch_embed.proj.bias\n",
      "backbone.0.body.patch_embed.norm.weight\n",
      "backbone.0.body.patch_embed.norm.bias\n",
      "backbone.0.body.layers.0.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.0.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.0.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.0.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.0.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.0.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.0.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.0.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.0.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.0.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.0.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.0.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.0.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.0.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.0.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.0.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.0.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.0.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.layers.0.downsample.reduction.weight\n",
      "backbone.0.body.layers.0.downsample.norm.weight\n",
      "backbone.0.body.layers.0.downsample.norm.bias\n",
      "backbone.0.body.layers.1.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.1.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.1.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.1.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.1.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.1.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.1.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.1.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.1.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.1.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.1.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.1.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.1.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.1.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.1.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.1.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.1.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.1.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.layers.1.downsample.reduction.weight\n",
      "backbone.0.body.layers.1.downsample.norm.weight\n",
      "backbone.0.body.layers.1.downsample.norm.bias\n",
      "backbone.0.body.layers.2.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.2.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.2.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.2.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.2.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.2.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.2.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.2.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.2.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.2.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.3.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.3.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.3.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.3.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.3.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.3.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.3.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.3.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.3.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.4.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.4.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.4.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.4.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.4.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.4.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.4.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.4.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.4.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.5.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.5.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.5.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.5.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.5.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.5.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.5.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.5.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.5.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.6.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.6.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.6.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.6.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.6.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.6.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.6.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.6.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.6.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.7.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.7.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.7.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.7.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.7.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.7.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.7.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.7.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.7.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.8.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.8.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.8.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.8.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.8.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.8.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.8.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.8.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.8.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.9.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.9.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.9.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.9.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.9.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.9.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.9.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.9.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.9.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.10.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.10.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.10.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.10.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.10.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.10.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.10.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.10.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.10.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.11.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.11.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.11.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.11.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.11.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.11.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.11.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.11.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.11.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.12.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.12.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.12.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.12.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.12.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.12.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.12.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.12.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.12.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.13.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.13.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.13.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.13.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.13.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.13.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.13.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.13.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.13.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.14.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.14.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.14.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.14.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.14.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.14.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.14.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.14.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.14.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.15.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.15.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.15.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.15.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.15.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.15.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.15.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.15.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.15.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.16.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.16.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.16.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.16.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.16.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.16.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.16.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.16.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.16.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.17.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.17.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.17.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.17.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.17.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.17.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.17.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.17.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.17.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.downsample.reduction.weight\n",
      "backbone.0.body.layers.2.downsample.norm.weight\n",
      "backbone.0.body.layers.2.downsample.norm.bias\n",
      "backbone.0.body.layers.3.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.3.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.3.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.3.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.3.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.3.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.3.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.3.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.3.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.3.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.3.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.3.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.3.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.3.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.3.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.3.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.3.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.3.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.norm1.weight\n",
      "backbone.0.body.norm1.bias\n",
      "backbone.0.body.norm2.weight\n",
      "backbone.0.body.norm2.bias\n",
      "backbone.0.body.norm3.weight\n",
      "backbone.0.body.norm3.bias\n",
      "temp_class_embed_list.0.weight\n",
      "temp_class_embed_list.0.bias\n",
      "temp_class_embed_list.1.weight\n",
      "temp_class_embed_list.1.bias\n",
      "temp_class_embed_list.2.weight\n",
      "temp_class_embed_list.2.bias\n",
      "temp_bbox_embed_list.0.layers.0.weight\n",
      "temp_bbox_embed_list.0.layers.0.bias\n",
      "temp_bbox_embed_list.0.layers.1.weight\n",
      "temp_bbox_embed_list.0.layers.1.bias\n",
      "temp_bbox_embed_list.0.layers.2.weight\n",
      "temp_bbox_embed_list.0.layers.2.bias\n",
      "temp_bbox_embed_list.1.layers.0.weight\n",
      "temp_bbox_embed_list.1.layers.0.bias\n",
      "temp_bbox_embed_list.1.layers.1.weight\n",
      "temp_bbox_embed_list.1.layers.1.bias\n",
      "temp_bbox_embed_list.1.layers.2.weight\n",
      "temp_bbox_embed_list.1.layers.2.bias\n",
      "temp_bbox_embed_list.2.layers.0.weight\n",
      "temp_bbox_embed_list.2.layers.0.bias\n",
      "temp_bbox_embed_list.2.layers.1.weight\n",
      "temp_bbox_embed_list.2.layers.1.bias\n",
      "temp_bbox_embed_list.2.layers.2.weight\n",
      "temp_bbox_embed_list.2.layers.2.bias\n",
      "[5, 6]\n",
      "Missing Keys: ['transformer.temporal_query_layer1.self_attn.in_proj_weight', 'transformer.temporal_query_layer1.self_attn.in_proj_bias', 'transformer.temporal_query_layer1.self_attn.out_proj.weight', 'transformer.temporal_query_layer1.self_attn.out_proj.bias', 'transformer.temporal_query_layer1.norm2.weight', 'transformer.temporal_query_layer1.norm2.bias', 'transformer.temporal_query_layer1.cross_attn.in_proj_weight', 'transformer.temporal_query_layer1.cross_attn.in_proj_bias', 'transformer.temporal_query_layer1.cross_attn.out_proj.weight', 'transformer.temporal_query_layer1.cross_attn.out_proj.bias', 'transformer.temporal_query_layer1.norm1.weight', 'transformer.temporal_query_layer1.norm1.bias', 'transformer.temporal_query_layer1.linear1.weight', 'transformer.temporal_query_layer1.linear1.bias', 'transformer.temporal_query_layer1.linear2.weight', 'transformer.temporal_query_layer1.linear2.bias', 'transformer.temporal_query_layer1.norm3.weight', 'transformer.temporal_query_layer1.norm3.bias', 'transformer.temporal_query_layer2.self_attn.in_proj_weight', 'transformer.temporal_query_layer2.self_attn.in_proj_bias', 'transformer.temporal_query_layer2.self_attn.out_proj.weight', 'transformer.temporal_query_layer2.self_attn.out_proj.bias', 'transformer.temporal_query_layer2.norm2.weight', 'transformer.temporal_query_layer2.norm2.bias', 'transformer.temporal_query_layer2.cross_attn.in_proj_weight', 'transformer.temporal_query_layer2.cross_attn.in_proj_bias', 'transformer.temporal_query_layer2.cross_attn.out_proj.weight', 'transformer.temporal_query_layer2.cross_attn.out_proj.bias', 'transformer.temporal_query_layer2.norm1.weight', 'transformer.temporal_query_layer2.norm1.bias', 'transformer.temporal_query_layer2.linear1.weight', 'transformer.temporal_query_layer2.linear1.bias', 'transformer.temporal_query_layer2.linear2.weight', 'transformer.temporal_query_layer2.linear2.bias', 'transformer.temporal_query_layer2.norm3.weight', 'transformer.temporal_query_layer2.norm3.bias', 'transformer.temporal_query_layer3.self_attn.in_proj_weight', 'transformer.temporal_query_layer3.self_attn.in_proj_bias', 'transformer.temporal_query_layer3.self_attn.out_proj.weight', 'transformer.temporal_query_layer3.self_attn.out_proj.bias', 'transformer.temporal_query_layer3.norm2.weight', 'transformer.temporal_query_layer3.norm2.bias', 'transformer.temporal_query_layer3.cross_attn.in_proj_weight', 'transformer.temporal_query_layer3.cross_attn.in_proj_bias', 'transformer.temporal_query_layer3.cross_attn.out_proj.weight', 'transformer.temporal_query_layer3.cross_attn.out_proj.bias', 'transformer.temporal_query_layer3.norm1.weight', 'transformer.temporal_query_layer3.norm1.bias', 'transformer.temporal_query_layer3.linear1.weight', 'transformer.temporal_query_layer3.linear1.bias', 'transformer.temporal_query_layer3.linear2.weight', 'transformer.temporal_query_layer3.linear2.bias', 'transformer.temporal_query_layer3.norm3.weight', 'transformer.temporal_query_layer3.norm3.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.value_proj.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.value_proj.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.output_proj.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.output_proj.bias', 'transformer.temporal_decoder1.layers.0.norm1.weight', 'transformer.temporal_decoder1.layers.0.norm1.bias', 'transformer.temporal_decoder1.layers.0.self_attn.in_proj_weight', 'transformer.temporal_decoder1.layers.0.self_attn.in_proj_bias', 'transformer.temporal_decoder1.layers.0.self_attn.out_proj.weight', 'transformer.temporal_decoder1.layers.0.self_attn.out_proj.bias', 'transformer.temporal_decoder1.layers.0.norm2.weight', 'transformer.temporal_decoder1.layers.0.norm2.bias', 'transformer.temporal_decoder1.layers.0.linear1.weight', 'transformer.temporal_decoder1.layers.0.linear1.bias', 'transformer.temporal_decoder1.layers.0.linear2.weight', 'transformer.temporal_decoder1.layers.0.linear2.bias', 'transformer.temporal_decoder1.layers.0.norm3.weight', 'transformer.temporal_decoder1.layers.0.norm3.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.value_proj.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.value_proj.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.output_proj.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.output_proj.bias', 'transformer.temporal_decoder2.layers.0.norm1.weight', 'transformer.temporal_decoder2.layers.0.norm1.bias', 'transformer.temporal_decoder2.layers.0.self_attn.in_proj_weight', 'transformer.temporal_decoder2.layers.0.self_attn.in_proj_bias', 'transformer.temporal_decoder2.layers.0.self_attn.out_proj.weight', 'transformer.temporal_decoder2.layers.0.self_attn.out_proj.bias', 'transformer.temporal_decoder2.layers.0.norm2.weight', 'transformer.temporal_decoder2.layers.0.norm2.bias', 'transformer.temporal_decoder2.layers.0.linear1.weight', 'transformer.temporal_decoder2.layers.0.linear1.bias', 'transformer.temporal_decoder2.layers.0.linear2.weight', 'transformer.temporal_decoder2.layers.0.linear2.bias', 'transformer.temporal_decoder2.layers.0.norm3.weight', 'transformer.temporal_decoder2.layers.0.norm3.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.value_proj.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.value_proj.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.output_proj.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.output_proj.bias', 'transformer.temporal_decoder3.layers.0.norm1.weight', 'transformer.temporal_decoder3.layers.0.norm1.bias', 'transformer.temporal_decoder3.layers.0.self_attn.in_proj_weight', 'transformer.temporal_decoder3.layers.0.self_attn.in_proj_bias', 'transformer.temporal_decoder3.layers.0.self_attn.out_proj.weight', 'transformer.temporal_decoder3.layers.0.self_attn.out_proj.bias', 'transformer.temporal_decoder3.layers.0.norm2.weight', 'transformer.temporal_decoder3.layers.0.norm2.bias', 'transformer.temporal_decoder3.layers.0.linear1.weight', 'transformer.temporal_decoder3.layers.0.linear1.bias', 'transformer.temporal_decoder3.layers.0.linear2.weight', 'transformer.temporal_decoder3.layers.0.linear2.bias', 'transformer.temporal_decoder3.layers.0.norm3.weight', 'transformer.temporal_decoder3.layers.0.norm3.bias', 'temp_class_embed.weight', 'temp_class_embed.bias', 'temp_bbox_embed.layers.0.weight', 'temp_bbox_embed.layers.0.bias', 'temp_bbox_embed.layers.1.weight', 'temp_bbox_embed.layers.1.bias', 'temp_bbox_embed.layers.2.weight', 'temp_bbox_embed.layers.2.bias', 'temp_class_embed_list.0.weight', 'temp_class_embed_list.0.bias', 'temp_class_embed_list.1.weight', 'temp_class_embed_list.1.bias', 'temp_class_embed_list.2.weight', 'temp_class_embed_list.2.bias', 'temp_bbox_embed_list.0.layers.0.weight', 'temp_bbox_embed_list.0.layers.0.bias', 'temp_bbox_embed_list.0.layers.1.weight', 'temp_bbox_embed_list.0.layers.1.bias', 'temp_bbox_embed_list.0.layers.2.weight', 'temp_bbox_embed_list.0.layers.2.bias', 'temp_bbox_embed_list.1.layers.0.weight', 'temp_bbox_embed_list.1.layers.0.bias', 'temp_bbox_embed_list.1.layers.1.weight', 'temp_bbox_embed_list.1.layers.1.bias', 'temp_bbox_embed_list.1.layers.2.weight', 'temp_bbox_embed_list.1.layers.2.bias', 'temp_bbox_embed_list.2.layers.0.weight', 'temp_bbox_embed_list.2.layers.0.bias', 'temp_bbox_embed_list.2.layers.1.weight', 'temp_bbox_embed_list.2.layers.1.bias', 'temp_bbox_embed_list.2.layers.2.weight', 'temp_bbox_embed_list.2.layers.2.bias']\n",
      "Model ready to start Training !\n",
      "Start training\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\vid\\\\Data\\\\VID/train/ILSVRC2015_VID_train_0002/ILSVRC2015_train_00710000/000573.JPEG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m], args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m--> 147\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_max_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m, args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[26], line 22\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[0;32m     14\u001b[0m print_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# prefetcher = data_prefetcher(data_loader, device, prefetch=True)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# data_loader_iter = iter(data_loader)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# samples, targets = data_loader_iter.next()\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# samples = samples.to(device)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetric_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# for _ in metric_logger.log_every(range(len(data_loader)), print_freq, header):\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# assert samples is None, samples\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# outputs = model(samples)\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#print(\"engine_target_shape\",targets)\u001b[39;49;00m\n",
      "Cell \u001b[1;32mIn[25], line 126\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    117\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter\u001b[38;5;241m.\u001b[39mjoin([\n\u001b[0;32m    118\u001b[0m         header,\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m space_fmt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata: \u001b[39m\u001b[38;5;132;01m{data}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    124\u001b[0m     ])\n\u001b[0;32m    125\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[23], line 77\u001b[0m, in \u001b[0;36mCocoDetection.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     75\u001b[0m path \u001b[38;5;241m=\u001b[39m img_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     76\u001b[0m video_id \u001b[38;5;241m=\u001b[39m img_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 77\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m target \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m: img_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m: target, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: path}\n\u001b[0;32m     79\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare(img, target)\n",
      "Cell \u001b[1;32mIn[20], line 49\u001b[0m, in \u001b[0;36mTvCocoDetection.get_image\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[path] \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(BytesIO(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[path]))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\vid\\\\Data\\\\VID/train/ILSVRC2015_VID_train_0002/ILSVRC2015_train_00710000/000573.JPEG'"
     ]
    }
   ],
   "source": [
    "if args[\"output_dir\"]:\n",
    "    Path(args[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(args[\"dataset_file\"], 11111111)\n",
    "print(args[\"dataset_file\"])\n",
    "device = torch.device(args[\"device\"])\n",
    "#utils.init_distributed_mode(args) #probably remove (need for multi gpus)\n",
    "#print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "\n",
    "if args[\"frozen_weights\"] is not None:\n",
    "    assert args[\"masks\"], \"Frozen training is meant for segmentation only\"\n",
    "\n",
    "\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args[\"seed\"] #+ utils.get_rank() #no need to get rank as it is a single gpu\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train_joint', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args[\"batch_size\"], drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                            collate_fn=collate_fn, num_workers=args[\"num_workers\"],\n",
    "                            pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, args[\"batch_size\"], sampler=sampler_val,\n",
    "                            drop_last=False, collate_fn=collate_fn, num_workers=args[\"num_workers\"],\n",
    "                            pin_memory=True)\n",
    "\n",
    "# lr_backbone_names = [\"backbone.0\", \"backbone.neck\", \"input_proj\", \"transformer.encoder\"]\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n",
    "\n",
    "for n, p in model_without_ddp.named_parameters():\n",
    "    print(n)\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "            if not match_name_keywords(n, args[\"lr_backbone_names\"]) and not match_name_keywords(n, args[\"lr_linear_proj_names\"]) and p.requires_grad],\n",
    "        \"lr\": args[\"lr\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args[\"lr_backbone_names\"]) and p.requires_grad],\n",
    "        \"lr\": args[\"lr_backbone\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args[\"lr_linear_proj_names\"]) and p.requires_grad],\n",
    "        \"lr\": args[\"lr\"] * args[\"lr_linear_proj_mult\"],\n",
    "    }\n",
    "]\n",
    "if args[\"sgd\"]:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args[\"lr\"], momentum=0.9,\n",
    "                                weight_decay=args[\"weight_decay\"])\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args[\"lr\"],\n",
    "                                weight_decay=args[\"weight_decay\"])\n",
    "print(args[\"lr_drop_epochs\"])\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, args[\"lr_drop_epochs\"])\n",
    "\n",
    "\n",
    "\n",
    "if args[\"dataset_file\"] == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args[\"frozen_weights\"] is not None:\n",
    "    checkpoint = torch.load(args[\"frozen_weights\"], map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args[\"output_dir\"])\n",
    "\n",
    "#IF WE WANT TO RESUME THE TRAINING\n",
    "if args[\"resume\"]:\n",
    "    if args[\"resume\"].startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args[\"resume\"], map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args[\"resume\"], map_location='cpu')\n",
    "\n",
    "    if args[\"eval\"]:\n",
    "        missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    else:\n",
    "        tmp_dict = model_without_ddp.state_dict().copy()\n",
    "        if args[\"coco_pretrain\"]: # singleBaseline\n",
    "            for k, v in checkpoint['model'].items():\n",
    "                if ('class_embed' not in k) :\n",
    "                    tmp_dict[k] = v \n",
    "                else:\n",
    "                    print('k', k)\n",
    "        \n",
    "        else:\n",
    "            tmp_dict = checkpoint['model']\n",
    "            for name, param in model_without_ddp.named_parameters():\n",
    "\n",
    "                if ('temp' in name):\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "        missing_keys, unexpected_keys = model_without_ddp.load_state_dict(tmp_dict, strict=False)\n",
    "        \n",
    "    unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "    if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "\n",
    "#If we want to evaluate a model\n",
    "if args[\"eval\"]:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                        data_loader_val, base_ds, device, args[\"output_dir\"])\n",
    "\n",
    "    if args[\"output_dir\"]:\n",
    "        save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    print(\"Job Finished, eval cocoevaluator\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"Model ready to start Training !\")\n",
    "#Train the model and save checkpoint and final\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args[\"start_epoch\"], args[\"epochs\"]):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch, args[\"clip_max_norm\"])\n",
    "    lr_scheduler.step()\n",
    "    print('output_dir', args[\"output_dir\"])\n",
    "    if args[\"output_dir\"]:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 5 epochs\n",
    "        # if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 1 == 0:\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    #test_stats, coco_evaluator = evaluate(\n",
    "    #   model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "    #)\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                'epoch': epoch,\n",
    "                'n_parameters': n_parameters}\n",
    "\n",
    "    if args[\"output_dir\"]:\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "model(torch.random(2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
