{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "#Model\n",
    "from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "\n",
    "import MultiScaleDeformableAttention as MSDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "2.2.1+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda_version)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "args=dict(\n",
    "lr= 2e-4, #learning rate\n",
    "lr_backbone_names=[\"backbone.0\"], #Backbone name such as resnet\n",
    "lr_backbone=2e-5, #learning rate for backbone\n",
    "lr_linear_proj_names=['reference_points', 'sampling_offsets'],\n",
    "lr_linear_proj_mult=0.1,\n",
    "batch_size=1, #video per batch\n",
    "weight_decay=1e-4, #\n",
    "epochs=7,\n",
    "lr_drop=5,\n",
    "lr_drop_epochs=[5,6],    \n",
    "clip_max_norm=0.1, #gradient clipping max norm\n",
    "\n",
    "#parameters of model\n",
    "num_ref_frames=3,\n",
    "num_frames=1,\n",
    "\n",
    "sgd=False,\n",
    "gap=2,\n",
    "\n",
    "#Variants of Deformable DETR\n",
    "with_box_refine=True,\n",
    "two_stage=False,\n",
    "\n",
    "#pretrained model\n",
    "frozen_weights=None, #use pretrained model to fine tune it, only mask head will be trained, give the path to the model\n",
    "pretrained=None, #if resume from a checkpoint\n",
    "\n",
    "#Backbone\n",
    "backbone='swin_b_p4w7',#Name of the convolutional backbone to use\n",
    "dilation=True,#If true, we replace stride with dilation in the last convolutional block (DC5)\n",
    "position_embedding='sine',#choices=('sine', 'learned'), Type of positional embedding to use on top of the image features\n",
    "position_embedding_scale=2 * math.pi, #position / size * scale\n",
    "num_feature_levels=1, #number of feature levels\n",
    "checkpoint=False, #store a checkpoint if true\n",
    "\n",
    "#Transformers \n",
    "enc_layers=6, #Number of encoding layers in the transformer\n",
    "dec_layers=6, #Number of decoding layers in the transformer\n",
    "dim_feedforward=1024, #Intermediate size of the feedforward layers in the transformer blocks\n",
    "hidden_dim=256, #Size of the embeddings (dimension of the transformer)\n",
    "dropout=0.1, #Dropout applied in the transformer\n",
    "nheads=8, #Number of attention heads inside the transformer's attentions\n",
    "num_queries=100, #Number of query slots\n",
    "dec_n_points=4,\n",
    "enc_n_points=4,\n",
    "n_temporal_decoder_layers=1,\n",
    "interval1=20,\n",
    "interval2=60,\n",
    "fixed_pretrained_model=False,\n",
    "is_shuffle=False,\n",
    "\n",
    "# * Segmentation\n",
    "masks=False, #Train segmentation head if the flag is provided\n",
    "\n",
    "# Loss\n",
    "aux_loss=False,\n",
    "\n",
    "# * Matcher\n",
    "set_cost_class=2, #Class coefficient in the matching cost\n",
    "set_cost_bbox=5, #L1 box coefficient in the matching cost\n",
    "set_cost_giou=2, #giou box coefficient in the matching cost\n",
    "\n",
    "# * Loss coefficients\n",
    "mask_loss_coef=1,\n",
    "dice_loss_coef=1,\n",
    "cls_loss_coef=2,\n",
    "bbox_loss_coef=5,\n",
    "giou_loss_coef=2,\n",
    "focal_alpha=0.25,\n",
    "\n",
    "# dataset parameters\n",
    "dataset_file='vid_multi',\n",
    "coco_path='./data/coco',\n",
    "vid_path='./data/vid',\n",
    "coco_pretrain=False,\n",
    "coco_panoptic_path=\"\",\n",
    "remove_difficult=False,\n",
    "\n",
    "output_dir='Final_output', #path where to save, empty for no saving\n",
    "device='cuda', #device to use for training / testing\n",
    "seed=42,\n",
    "resume='./exps/exps_single/swinb_88.3/checkpoint0006.pth', #resume from checkpoint\n",
    "start_epoch=0, #metavar='N', start epoch)\n",
    "eval=True,\n",
    "num_workers=0,\n",
    "cache_mode=False) #whether to cache images on memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Multiscale DeformableAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSDeformAttnFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step):\n",
    "        ctx.im2col_step = im2col_step\n",
    "        output = MSDA.ms_deform_attn_forward(\n",
    "            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, ctx.im2col_step)\n",
    "        ctx.save_for_backward(value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_output):\n",
    "        value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights = ctx.saved_tensors\n",
    "        grad_value, grad_sampling_loc, grad_attn_weight = \\\n",
    "            MSDA.ms_deform_attn_backward(\n",
    "                value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, grad_output, ctx.im2col_step)\n",
    "\n",
    "        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n",
    "\n",
    "\n",
    "def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n",
    "    # for debug and test only,\n",
    "    # need to use cuda version instead\n",
    "    N_, S_, M_, D_ = value.shape\n",
    "    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n",
    "    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n",
    "    sampling_grids = 2 * sampling_locations - 1\n",
    "    sampling_value_list = []\n",
    "    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n",
    "        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_\n",
    "        value_l_ = value_list[lid_].flatten(2).transpose(1, 2).reshape(N_*M_, D_, H_, W_)\n",
    "        # N_, Lq_, M_, P_, 2 -> N_, M_, Lq_, P_, 2 -> N_*M_, Lq_, P_, 2\n",
    "        sampling_grid_l_ = sampling_grids[:, :, :, lid_].transpose(1, 2).flatten(0, 1)\n",
    "        # N_*M_, D_, Lq_, P_\n",
    "        sampling_value_l_ = F.grid_sample(value_l_, sampling_grid_l_,\n",
    "                                          mode='bilinear', padding_mode='zeros', align_corners=False)\n",
    "        sampling_value_list.append(sampling_value_l_)\n",
    "    # (N_, Lq_, M_, L_, P_) -> (N_, M_, Lq_, L_, P_) -> (N_, M_, 1, Lq_, L_*P_)\n",
    "    attention_weights = attention_weights.transpose(1, 2).reshape(N_*M_, 1, Lq_, L_*P_)\n",
    "    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights).sum(-1).view(N_, M_*D_, Lq_)\n",
    "    return output.transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "def _is_power_of_2(n):\n",
    "    if (not isinstance(n, int)) or (n < 0):\n",
    "        raise ValueError(\"invalid input for _is_power_of_2: {} (type: {})\".format(n, type(n)))\n",
    "    return (n & (n-1) == 0) and n != 0\n",
    "\n",
    "class MSDeformAttn(nn.Module):\n",
    "    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n",
    "        \"\"\"\n",
    "        Multi-Scale Deformable Attention Module\n",
    "        :param d_model      hidden dimension\n",
    "        :param n_levels     number of feature levels\n",
    "        :param n_heads      number of attention heads\n",
    "        :param n_points     number of sampling points per attention head per feature level\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if d_model % n_heads != 0:\n",
    "            raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n",
    "        _d_per_head = d_model // n_heads\n",
    "        # you'd better set _d_per_head to a power of 2 which is more efficient in our CUDA implementation\n",
    "        if not _is_power_of_2(_d_per_head):\n",
    "            warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 \"\n",
    "                          \"which is more efficient in our CUDA implementation.\")\n",
    "\n",
    "        self.im2col_step = 64\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_levels = n_levels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_points = n_points\n",
    "\n",
    "        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n",
    "        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.output_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        constant_(self.sampling_offsets.weight.data, 0.)\n",
    "        thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n",
    "        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n",
    "        grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n",
    "        for i in range(self.n_points):\n",
    "            grid_init[:, :, i, :] *= i + 1\n",
    "        with torch.no_grad():\n",
    "            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n",
    "        constant_(self.attention_weights.weight.data, 0.)\n",
    "        constant_(self.attention_weights.bias.data, 0.)\n",
    "        xavier_uniform_(self.value_proj.weight.data)\n",
    "        constant_(self.value_proj.bias.data, 0.)\n",
    "        xavier_uniform_(self.output_proj.weight.data)\n",
    "        constant_(self.output_proj.bias.data, 0.)\n",
    "\n",
    "    def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n",
    "        \"\"\"\n",
    "        :param query                       (N, Length_{query}, C)\n",
    "        :param reference_points            (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0), bottom-right (1, 1), including padding area\n",
    "                                        or (N, Length_{query}, n_levels, 4), add additional (w, h) to form reference boxes\n",
    "        :param input_flatten               (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l, C)\n",
    "        :param input_spatial_shapes        (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n",
    "        :param input_level_start_index     (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1, H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\n",
    "        :param input_padding_mask          (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l), True for padding elements, False for non-padding elements\n",
    "\n",
    "        :return output                     (N, Length_{query}, C)\n",
    "        \"\"\"\n",
    "        N, Len_q, _ = query.shape\n",
    "        N, Len_in, _ = input_flatten.shape\n",
    "        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
    "\n",
    "        value = self.value_proj(input_flatten)\n",
    "        if input_padding_mask is not None:\n",
    "            value = value.masked_fill(input_padding_mask[..., None], float(0))\n",
    "        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n",
    "        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n",
    "        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n",
    "        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n",
    "        # N, Len_q, n_heads, n_levels, n_points, 2\n",
    "        if reference_points.shape[-1] == 2:\n",
    "            offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n",
    "            # print(\"shape122\", offset_normalizer.shape)\n",
    "            # print(sampling_offsets.shape)\n",
    "            sampling_locations = reference_points[:, :, None, :, None, :] \\\n",
    "                                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n",
    "        elif reference_points.shape[-1] == 4:\n",
    "            sampling_locations = reference_points[:, :, None, :, None, :2] \\\n",
    "                                 + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n",
    "        output = MSDeformAttnFunction.apply(\n",
    "            value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-5):\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1/x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part 1 : Deformable DETR (DEtection TRansformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for model part 1, deformable transformer multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableTransformer(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, dropout=0.1,\n",
    "                 activation=\"relu\", return_intermediate_dec=False,\n",
    "                 num_feature_levels=4, dec_n_points=4,  enc_n_points=4,\n",
    "                 two_stage=False, two_stage_num_proposals=300, n_temporal_decoder_layers = 1,\n",
    "                 num_frames= 3, fixed_pretrained_model = False, args=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.two_stage = two_stage\n",
    "        self.num_frames = num_frames\n",
    "        self.two_stage_num_proposals = two_stage_num_proposals\n",
    "        self.fixed_pretrained_model = fixed_pretrained_model\n",
    "        self.n_temporal_query_layers = 3\n",
    "\n",
    "        encoder_layer = DeformableTransformerEncoderLayer(d_model, dim_feedforward,\n",
    "                                                          dropout, activation,\n",
    "                                                          num_feature_levels, nhead, enc_n_points)\n",
    "        self.encoder = DeformableTransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "        decoder_layer = DeformableTransformerDecoderLayer(d_model, dim_feedforward,\n",
    "                                                          dropout, activation,\n",
    "                                                          num_feature_levels, nhead, dec_n_points)\n",
    "        self.decoder = DeformableTransformerDecoder(decoder_layer, num_decoder_layers, return_intermediate_dec)\n",
    "\n",
    "        self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))\n",
    "                                                          \n",
    "        self.temporal_query_layer1 = TemporalQueryEncoderLayer(d_model, dim_feedforward, dropout, activation, nhead)\n",
    "        self.temporal_query_layer2 = TemporalQueryEncoderLayer(d_model, dim_feedforward, dropout, activation, nhead)\n",
    "        self.temporal_query_layer3 = TemporalQueryEncoderLayer(d_model, dim_feedforward, dropout, activation, nhead)\n",
    "        # self.temporal_query_encoder = TemporalQueryEncoder(self.temporal_query_layer, self.n_temporal_query_layers) \n",
    "        self.temporal_decoder1 = TemporalDeformableTransformerDecoder(decoder_layer, n_temporal_decoder_layers, False)\n",
    "        self.temporal_decoder2 = TemporalDeformableTransformerDecoder(decoder_layer, n_temporal_decoder_layers, False)\n",
    "        self.temporal_decoder3 = TemporalDeformableTransformerDecoder(decoder_layer, n_temporal_decoder_layers, False)\n",
    "\n",
    "        if two_stage:\n",
    "            self.enc_output = nn.Linear(d_model, d_model)\n",
    "            self.enc_output_norm = nn.LayerNorm(d_model)\n",
    "            self.pos_trans = nn.Linear(d_model * 2, d_model * 2)\n",
    "            self.pos_trans_norm = nn.LayerNorm(d_model * 2)\n",
    "        else:\n",
    "            self.reference_points = nn.Linear(d_model, 2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, MSDeformAttn):\n",
    "                m._reset_parameters()\n",
    "        if not self.two_stage:\n",
    "            xavier_uniform_(self.reference_points.weight.data, gain=1.0)\n",
    "            constant_(self.reference_points.bias.data, 0.)\n",
    "        normal_(self.level_embed)\n",
    "\n",
    "    def get_proposal_pos_embed(self, proposals):\n",
    "        num_pos_feats = 128\n",
    "        temperature = 10000\n",
    "        scale = 2 * math.pi\n",
    "\n",
    "        dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)\n",
    "        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n",
    "        # N, L, 4\n",
    "        proposals = proposals.sigmoid() * scale\n",
    "        # N, L, 4, 128\n",
    "        pos = proposals[:, :, :, None] / dim_t\n",
    "        # N, L, 4, 64, 2\n",
    "        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()), dim=4).flatten(2)\n",
    "        return pos\n",
    "\n",
    "    def gen_encoder_output_proposals(self, memory, memory_padding_mask, spatial_shapes):\n",
    "        N_, S_, C_ = memory.shape\n",
    "        base_scale = 4.0\n",
    "        proposals = []\n",
    "        _cur = 0\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, 1)\n",
    "            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)\n",
    "            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)\n",
    "\n",
    "            grid_y, grid_x = torch.meshgrid(torch.linspace(0, H_ - 1, H_, dtype=torch.float32, device=memory.device),\n",
    "                                            torch.linspace(0, W_ - 1, W_, dtype=torch.float32, device=memory.device))\n",
    "            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)\n",
    "\n",
    "            scale = torch.cat([valid_W.unsqueeze(-1), valid_H.unsqueeze(-1)], 1).view(N_, 1, 1, 2)\n",
    "            grid = (grid.unsqueeze(0).expand(N_, -1, -1, -1) + 0.5) / scale\n",
    "            wh = torch.ones_like(grid) * 0.05 * (2.0 ** lvl)\n",
    "            proposal = torch.cat((grid, wh), -1).view(N_, -1, 4)\n",
    "            proposals.append(proposal)\n",
    "            _cur += (H_ * W_)\n",
    "        output_proposals = torch.cat(proposals, 1)\n",
    "        output_proposals_valid = ((output_proposals > 0.01) & (output_proposals < 0.99)).all(-1, keepdim=True)\n",
    "        output_proposals = torch.log(output_proposals / (1 - output_proposals))\n",
    "        output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(-1), float('inf'))\n",
    "        output_proposals = output_proposals.masked_fill(~output_proposals_valid, float('inf'))\n",
    "\n",
    "        output_memory = memory\n",
    "        output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(-1), float(0))\n",
    "        output_memory = output_memory.masked_fill(~output_proposals_valid, float(0))\n",
    "        output_memory = self.enc_output_norm(self.enc_output(output_memory))\n",
    "        return output_memory, output_proposals\n",
    "\n",
    "    def get_valid_ratio(self, mask):\n",
    "        _, H, W = mask.shape\n",
    "        valid_H = torch.sum(~mask[:, :, 0], 1)\n",
    "        valid_W = torch.sum(~mask[:, 0, :], 1)\n",
    "        valid_ratio_h = valid_H.float() / H\n",
    "        valid_ratio_w = valid_W.float() / W\n",
    "        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n",
    "        return valid_ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
    "        reference_points_list = []\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "\n",
    "            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
    "                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n",
    "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
    "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
    "            ref = torch.stack((ref_x, ref_y), -1)\n",
    "            reference_points_list.append(ref)\n",
    "        reference_points = torch.cat(reference_points_list, 1)\n",
    "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
    "        return reference_points\n",
    "\n",
    "    def forward(self, srcs, masks, pos_embeds, query_embed=None, class_embed = None, temp_class_embed_list = None, temp_bbox_embed_list = None ):\n",
    "        assert self.two_stage or query_embed is not None\n",
    "\n",
    "        # prepare input for encoder\n",
    "        src_flatten = []\n",
    "        mask_flatten = []\n",
    "        lvl_pos_embed_flatten = []\n",
    "        spatial_shapes = []\n",
    "        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):\n",
    "            bs, c, h, w = src.shape\n",
    "            spatial_shape = (h, w)\n",
    "            spatial_shapes.append(spatial_shape)\n",
    "            src = src.flatten(2).transpose(1, 2)\n",
    "            mask = mask.flatten(1)\n",
    "\n",
    "            pos_embed = pos_embed.flatten(2).transpose(1, 2) \n",
    "            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)\n",
    "            lvl_pos_embed_flatten.append(lvl_pos_embed)\n",
    "            src_flatten.append(src)\n",
    "            mask_flatten.append(mask)\n",
    "        src_flatten = torch.cat(src_flatten, 1) \n",
    "        mask_flatten = torch.cat(mask_flatten, 1)\n",
    "        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n",
    "        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
    "        level_start_index = torch.cat((spatial_shapes.new_zeros((1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))\n",
    "        valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n",
    "\n",
    "        # encoder\n",
    "        memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)\n",
    "\n",
    "        # prepare input for decoder:\n",
    "        bs, _, c = memory.shape\n",
    "        if self.two_stage:\n",
    "            output_memory, output_proposals = self.gen_encoder_output_proposals(memory, mask_flatten, spatial_shapes)\n",
    "\n",
    "            # hack implementation for two-stage Deformable DETR\n",
    "            enc_outputs_class = self.decoder.class_embed[self.decoder.num_layers](output_memory)\n",
    "            enc_outputs_coord_unact = self.decoder.bbox_embed[self.decoder.num_layers](output_memory) + output_proposals\n",
    "\n",
    "            topk = self.two_stage_num_proposals\n",
    "            topk_proposals = torch.topk(enc_outputs_class[..., 0], topk, dim=1)[1]\n",
    "            topk_coords_unact = torch.gather(enc_outputs_coord_unact, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4))\n",
    "            topk_coords_unact = topk_coords_unact.detach()\n",
    "            reference_points = topk_coords_unact.sigmoid()\n",
    "            init_reference_out = reference_points\n",
    "            pos_trans_out = self.pos_trans_norm(self.pos_trans(self.get_proposal_pos_embed(topk_coords_unact)))\n",
    "            query_embed, tgt = torch.split(pos_trans_out, c, dim=2)\n",
    "        else:\n",
    "            query_embed, tgt = torch.split(query_embed, c, dim=1)\n",
    "            query_embed = query_embed.unsqueeze(0).expand(bs, -1, -1)\n",
    "            tgt = tgt.unsqueeze(0).expand(bs, -1, -1)\n",
    "            reference_points = self.reference_points(query_embed).sigmoid()\n",
    "            init_reference_out = reference_points\n",
    "\n",
    "        # decoder\n",
    "        hs, inter_references = self.decoder(tgt, reference_points, memory,\n",
    "                                            spatial_shapes, level_start_index, valid_ratios, query_embed, mask_flatten)\n",
    "\n",
    "        inter_references_out = inter_references\n",
    "        if self.two_stage:\n",
    "            return hs, init_reference_out, inter_references_out, enc_outputs_class, enc_outputs_coord_unact\n",
    "        \n",
    "        if self.fixed_pretrained_model:\n",
    "            print(\"fixed\")\n",
    "            memory = memory.detach()\n",
    "            hs = hs.detach()\n",
    "            inter_references = inter_references.detach()\n",
    "\n",
    "        self.SeqHQM = True\n",
    "        # Implementation of Sequential Hard Query Mining (SeqHQM)\n",
    "        if self.SeqHQM:\n",
    "            out = {}\n",
    "            last_reference_out  = inter_references_out[-1]\n",
    "            #print(\"11\", last_reference_out.shape)\n",
    "            last_hs = hs[-1]\n",
    "            new_hs, last_reference_out = update_QFH(class_embed, last_hs, last_reference_out, 80)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames, dim = 0)\n",
    "            new_hs = torch.cat(new_hs_list, 1) # 1, 300 * 4 , 128\n",
    "            new_hs = self.temporal_query_layer1(new_hs, new_hs)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames , dim = 1)\n",
    "            new_hs = torch.cat(new_hs_list , 0) # 4, 300, 128\n",
    "            new_hs, last_references_out = self.temporal_decoder1(new_hs, last_reference_out, memory,\n",
    "                                                                spatial_shapes, level_start_index, valid_ratios, None, None)\n",
    "            \n",
    "            reference1 = inverse_sigmoid(last_references_out)\n",
    "            output_class1 = temp_class_embed_list[0](new_hs)\n",
    "            tmp1 = temp_bbox_embed_list[0](new_hs)\n",
    "            if reference1.shape[-1] == 4:\n",
    "                tmp1 += reference1\n",
    "            else:\n",
    "                assert reference1.shape[-1] == 2\n",
    "                tmp1[..., :2] += reference1\n",
    "            output_coord1 = tmp1.sigmoid()\n",
    "            out['aux_outputs'] = [{\"pred_logits\":output_class1, \"pred_boxes\":output_coord1}]\n",
    "\n",
    "            # loss: new_hs [4, 50, 128] \n",
    "            # self.temp_class_embed\n",
    "            new_hs, last_reference_out = update_QFH(temp_class_embed_list[0], new_hs, last_reference_out, 50)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames, dim = 0)\n",
    "            new_hs = torch.cat(new_hs_list, 1) #1, 30 * 4 ,128\n",
    "            new_hs = self.temporal_query_layer2(new_hs, new_hs)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames , dim = 1)\n",
    "            new_hs = torch.cat(new_hs_list , 0)\n",
    "            new_hs, last_references_out = self.temporal_decoder2(new_hs, last_reference_out, memory,\n",
    "                                                                spatial_shapes, level_start_index, valid_ratios, None, None)\n",
    "            \n",
    "            \n",
    "            reference2 = inverse_sigmoid(last_references_out)\n",
    "            output_class2 = temp_class_embed_list[1](new_hs)\n",
    "            tmp2 = temp_bbox_embed_list[1](new_hs)\n",
    "            if reference2.shape[-1] == 4:\n",
    "                tmp2 += reference2\n",
    "            else:\n",
    "                assert reference2.shape[-1] == 2\n",
    "                tmp2[..., :2] += reference2\n",
    "            output_coord2 = tmp2.sigmoid()\n",
    "            out['aux_outputs'].append({\"pred_logits\":output_class2, \"pred_boxes\":output_coord2})\n",
    "            # loss: [4, 30, 128]\n",
    "\n",
    "            new_hs, last_reference_out = update_QFH(temp_class_embed_list[1], new_hs, last_reference_out, 30)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames, dim = 0)\n",
    "            new_hs = torch.cat(new_hs_list, 1)\n",
    "            new_hs = self.temporal_query_layer3(new_hs, new_hs)\n",
    "            new_hs_list = torch.chunk(new_hs, self.num_frames , dim = 1)\n",
    "            new_hs = torch.cat(new_hs_list , 0)\n",
    "            final_hs, final_references_out = self.temporal_decoder3(new_hs, last_reference_out, memory,\n",
    "                                            spatial_shapes, level_start_index, valid_ratios, None, None)\n",
    "\n",
    "            return hs, init_reference_out, inter_references_out, None, None, final_hs, final_references_out, out\n",
    "\n",
    "            \n",
    "        return hs[:,0:1,:,:], init_reference_out[0:1], inter_references_out[:,0:1,:,:], None, None, final_hs, final_references_out\n",
    "\n",
    "def update_QFH(class_embed, hs, last_reference_out, topk):\n",
    "    num_frames = hs.shape[0]\n",
    "    hs_logits = class_embed(hs)\n",
    "    prob = hs_logits.sigmoid()\n",
    "    prob = torch.max(prob, dim = -1)\n",
    "    topk_values, topk_indexes = torch.topk(prob[0], topk, dim = 1)\n",
    "    hs = torch.gather(hs, 1, topk_indexes.unsqueeze(-1).repeat(1,1,hs.shape[-1]))\n",
    "    last_reference_out = torch.gather(last_reference_out, 1, topk_indexes.unsqueeze(-1).repeat(1,1,last_reference_out.shape[-1]))\n",
    "    return hs, last_reference_out\n",
    "\n",
    "class TemporalQueryEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 256, d_ffn = 1024, dropout=0.1, activation=\"relu\", n_heads = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        # self attention \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # cross attention \n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # ffn \n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model) \n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "    \n",
    "    def forward(self, query , ref_query, query_pos = None, ref_query_pos = None):\n",
    "        # self.attention\n",
    "        q = k = self.with_pos_embed(query, query_pos)\n",
    "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), query.transpose(0, 1))[0].transpose(0, 1)\n",
    "        tgt = query + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # cross attention \n",
    "        tgt2 = self.cross_attn(\n",
    "            self.with_pos_embed(tgt, query_pos).transpose(0, 1), \n",
    "            self.with_pos_embed(ref_query, ref_query_pos).transpose(0, 1),\n",
    "            ref_query.transpose(0,1)\n",
    "        )[0].transpose(0,1)\n",
    "\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "class TemporalQueryEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, query , ref_query, query_pos = None, ref_query_pos = None):\n",
    "        output = query\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            output = layer(output, ref_query, query_pos, ref_query_pos)\n",
    "        return output\n",
    "\n",
    "class TemporalDeformableTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = 256, d_ffn=1024, dropout=0.1, \n",
    "                 activation='relu', num_ref_frames = 3, n_heads = 8, n_points=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # cross attention \n",
    "        self.cross_attn = MSDeformAttn(d_model, num_ref_frames, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, query_pos, reference_points, src, src_spatial_shapes, frame_start_index, src_padding_mask=None):\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1))[0].transpose(0, 1)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "    \n",
    "        # cross attention\n",
    "        tgt2 = self.cross_attn(self.with_pos_embed(tgt, query_pos),\n",
    "                               reference_points,\n",
    "                               src, src_spatial_shapes, frame_start_index, src_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class DeformableTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model=256, d_ffn=1024,\n",
    "                 dropout=0.1, activation=\"relu\",\n",
    "                 n_levels=4, n_heads=8, n_points=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, src):\n",
    "        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout3(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask=None):\n",
    "        # self attention\n",
    "        src2 = self.self_attn(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # ffn\n",
    "        src = self.forward_ffn(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class DeformableTransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
    "        reference_points_list = []\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "\n",
    "            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
    "                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n",
    "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
    "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
    "            ref = torch.stack((ref_x, ref_y), -1)\n",
    "            reference_points_list.append(ref)\n",
    "        reference_points = torch.cat(reference_points_list, 1)\n",
    "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
    "        return reference_points\n",
    "\n",
    "    def forward(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None):\n",
    "        output = src\n",
    "        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            # print(str(_) + \"deformable_transformer_\", [reference_points.shape, level_start_index, spatial_shapes] )\n",
    "            output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class DeformableTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=256, d_ffn=1024,\n",
    "                 dropout=0.1, activation=\"relu\",\n",
    "                 n_levels=4, n_heads=8, n_points=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # cross attention\n",
    "        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, query_pos, reference_points, src, src_spatial_shapes, level_start_index, src_padding_mask=None):\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        # \n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"q tran shape\", q.transpose(0,1).shape)\n",
    "        tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1))[0].transpose(0, 1)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # cross attention\n",
    "        # print(\"tgt\", tgt.shape)\n",
    "        # print(\"ref\", reference_points.shape)\n",
    "        # print(\"src_spatial_shapes\", src_spatial_shapes)\n",
    "        # print(\"mask\", src_padding_mask)\n",
    "        tgt2 = self.cross_attn(self.with_pos_embed(tgt, query_pos),\n",
    "                               reference_points,\n",
    "                               src, src_spatial_shapes, level_start_index, src_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class TemporalDeformableTransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.return_intermediate = return_intermediate\n",
    "        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n",
    "        self.bbox_embed = None\n",
    "        self.class_embed = None\n",
    "\n",
    "    def forward(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,\n",
    "                query_pos=None, src_padding_mask=None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        intermediate_reference_points = []\n",
    "        for lid, layer in enumerate(self.layers):\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            if reference_points.shape[-1] == 4:\n",
    "                reference_points_input = reference_points[:, :, None] \\\n",
    "                                         * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]\n",
    "            else:\n",
    "                assert reference_points.shape[-1] == 2\n",
    "                reference_points_input = reference_points[:, :, None] * src_valid_ratios[:, None]\n",
    "            output = layer(output, query_pos, reference_points_input, src, src_spatial_shapes, src_level_start_index, src_padding_mask)\n",
    "\n",
    "            # hack implementation for iterative bounding box refinement\n",
    "            self.bbox_embed = None\n",
    "            if self.bbox_embed is not None:\n",
    "                tmp = self.bbox_embed[lid](output)\n",
    "                if reference_points.shape[-1] == 4:\n",
    "                    new_reference_points = tmp + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                else:\n",
    "                    assert reference_points.shape[-1] == 2\n",
    "                    new_reference_points = tmp\n",
    "                    new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                reference_points = new_reference_points.detach()\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "                intermediate_reference_points.append(reference_points)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n",
    "\n",
    "        return output, reference_points  \n",
    "\n",
    "class DeformableTransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.return_intermediate = return_intermediate\n",
    "        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n",
    "        self.bbox_embed = None\n",
    "        self.class_embed = None\n",
    "\n",
    "    def forward(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,\n",
    "                query_pos=None, src_padding_mask=None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        intermediate_reference_points = []\n",
    "        for lid, layer in enumerate(self.layers):\n",
    "            # print(\"Decoder refer\", reference_points.shape)\n",
    "            # print(reference_points)\n",
    "            # print(\"src_valid_ratios\", src_valid_ratios)\n",
    "            if reference_points.shape[-1] == 4:\n",
    "                reference_points_input = reference_points[:, :, None] \\\n",
    "                                         * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]\n",
    "            else:\n",
    "                assert reference_points.shape[-1] == 2\n",
    "                reference_points_input = reference_points[:, :, None] * src_valid_ratios[:, None]\n",
    "            # print(\"reference_points_input\", reference_points_input.shape)\n",
    "            output = layer(output, query_pos, reference_points_input, src, src_spatial_shapes, src_level_start_index, src_padding_mask)\n",
    "\n",
    "            # hack implementation for iterative bounding box refinement\n",
    "            if self.bbox_embed is not None:\n",
    "                tmp = self.bbox_embed[lid](output)\n",
    "                if reference_points.shape[-1] == 4:\n",
    "                    new_reference_points = tmp + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                else:\n",
    "                    assert reference_points.shape[-1] == 2\n",
    "                    new_reference_points = tmp\n",
    "                    new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)\n",
    "                    new_reference_points = new_reference_points.sigmoid()\n",
    "                reference_points = new_reference_points.detach()\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "                intermediate_reference_points.append(reference_points)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n",
    "\n",
    "        return output, reference_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deforamble_transformer(args):\n",
    "    return DeformableTransformer(\n",
    "        d_model=args[\"hidden_dim\"],\n",
    "        nhead=args[\"nheads\"],\n",
    "        num_encoder_layers=args[\"enc_layers\"],\n",
    "        num_decoder_layers=args[\"dec_layers\"],\n",
    "        dim_feedforward=args[\"dim_feedforward\"],\n",
    "        dropout=args[\"dropout\"],\n",
    "        activation=\"relu\",\n",
    "        return_intermediate_dec=True,\n",
    "        num_feature_levels=args[\"num_feature_levels\"],\n",
    "        dec_n_points=args[\"dec_n_points\"],\n",
    "        enc_n_points=args[\"enc_n_points\"],\n",
    "        two_stage=args[\"two_stage\"],\n",
    "        two_stage_num_proposals=args[\"num_queries\"],\n",
    "        n_temporal_decoder_layers = args[\"n_temporal_decoder_layers\"], \n",
    "        num_frames = args[\"num_frames\"],\n",
    "        fixed_pretrained_model = args[\"fixed_pretrained_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Position and Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone and Position encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.misc\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device, non_blocking=False):\n",
    "      \n",
    "        cast_tensor = self.tensors.to(device, non_blocking=non_blocking)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device, non_blocking=non_blocking)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def record_stream(self, *args, **kwargs):\n",
    "        self.tensors.record_stream(*args, **kwargs)\n",
    "        if self.mask is not None:\n",
    "            self.mask.record_stream(*args, **kwargs)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        mask = tensor_list.mask\n",
    "        assert mask is not None\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = (y_embed - 0.5) / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = (x_embed - 0.5) / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos\n",
    "\n",
    "\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=256):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "def build_position_encoding(args):\n",
    "    N_steps = args[\"hidden_dim\"] // 2\n",
    "    if args[\"position_embedding\"] in ('v2', 'sine'):\n",
    "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
    "    elif args[\"position_embedding\"] in ('v3', 'learned'):\n",
    "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
    "    else:\n",
    "        error=args[\"position_embedding\"]\n",
    "        raise ValueError(f\"not supported {error}\")\n",
    "\n",
    "    return position_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, eps=1e-5):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "        self.eps = eps\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = self.eps\n",
    "        scale = w * (rv + eps).rsqrt()\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "\n",
    "class BackboneBase(nn.Module):\n",
    "    # backbone, backbone, \n",
    "    def __init__(self, backbone: nn.Module, train_backbone: bool, return_interm_layers: bool):\n",
    "        super().__init__()\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "                parameter.requires_grad_(False)\n",
    "        if return_interm_layers:\n",
    "            # return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
    "            return_layers = {\"layer2\": \"0\", \"layer3\": \"1\", \"layer4\": \"2\"}\n",
    "            self.strides = [8, 16, 32]\n",
    "            self.num_channels = [512, 1024, 2048]\n",
    "        else:\n",
    "            return_layers = {'layer4': \"0\"}\n",
    "            self.strides = [32]\n",
    "            self.num_channels = [2048]\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        # tensor list \n",
    "        xs = self.body(tensor_list.tensors)\n",
    "        out: Dict[str, NestedTensor] = {}\n",
    "        for name, x in xs.items():\n",
    "            m = tensor_list.mask\n",
    "            assert m is not None\n",
    "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "            out[name] = NestedTensor(x, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Backbone(BackboneBase):\n",
    "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_backbone: bool,\n",
    "                 return_interm_layers: bool,\n",
    "                 dilation: bool):\n",
    "        norm_layer = FrozenBatchNorm2d\n",
    "        backbone = getattr(torchvision.models, name)(\n",
    "            replace_stride_with_dilation=[False, False, dilation],\n",
    "            pretrained=True, norm_layer=norm_layer)\n",
    "        assert name not in ('resnet18', 'resnet34'), \"number of channels are hard coded\"\n",
    "        super().__init__(backbone, train_backbone, return_interm_layers)\n",
    "        if dilation:\n",
    "            self.strides[-1] = self.strides[-1] // 2\n",
    "\n",
    "\n",
    "class Joiner(nn.Sequential):\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "        self.strides = backbone.strides\n",
    "        self.num_channels = backbone.num_channels\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self[0](tensor_list)\n",
    "        out: List[NestedTensor] = []\n",
    "        pos = []\n",
    "        for name, x in sorted(xs.items()):\n",
    "            out.append(x)\n",
    "\n",
    "        # position encoding\n",
    "        for x in out:\n",
    "            pos.append(self[1](x).to(x.tensors.dtype))\n",
    "        \n",
    "        return out, pos\n",
    "\n",
    "def build_backbone(args):\n",
    "    position_embedding = build_position_encoding(args)\n",
    "    train_backbone = args[\"lr_backbone\"] > 0\n",
    "    return_interm_layers = args[\"masks\"] or (args[\"num_feature_levels\"] > 1 )\n",
    "    backbone = Backbone(args[\"backbone\"], train_backbone, return_interm_layers, args[\"dilation\"])\n",
    "    model = Joiner(backbone, position_embedding)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backbone of swin, ONLY RUN IF SWIN IS USED AS BACKBONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if 'swin' in args[\"backbone\"]:\n",
    "    \n",
    "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "    from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "    import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "    class Mlp(nn.Module):\n",
    "        \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "        def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "            super().__init__()\n",
    "            out_features = out_features or in_features\n",
    "            hidden_features = hidden_features or in_features\n",
    "            self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "            self.act = act_layer()\n",
    "            self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "            self.drop = nn.Dropout(drop)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.act(x)\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.drop(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    def window_partition(x, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H, W, C)\n",
    "            window_size (int): window size\n",
    "        Returns:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "        \"\"\"\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "        return windows\n",
    "\n",
    "\n",
    "    def window_reverse(windows, window_size, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "            window_size (int): Window size\n",
    "            H (int): Height of image\n",
    "            W (int): Width of image\n",
    "        Returns:\n",
    "            x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "        x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    class WindowAttention(nn.Module):\n",
    "        \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "        It supports both of shifted and non-shifted window.\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            window_size (tuple[int]): The height and width of the window.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "            attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "            proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.window_size = window_size  # Wh, Ww\n",
    "            self.num_heads = num_heads\n",
    "            head_dim = dim // num_heads\n",
    "            self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "            # define a parameter table of relative position bias\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(self.window_size[0])\n",
    "            coords_w = torch.arange(self.window_size[1])\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "            self.proj = nn.Linear(dim, dim)\n",
    "            self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "            self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        def forward(self, x, mask=None):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: input features with shape of (num_windows*B, N, C)\n",
    "                mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "            \"\"\"\n",
    "            B_, N, C = x.shape\n",
    "            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "            q = q * self.scale\n",
    "            attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "            if mask is not None:\n",
    "                nW = mask.shape[0]\n",
    "                attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "                attn = attn.view(-1, self.num_heads, N, N)\n",
    "                attn = self.softmax(attn)\n",
    "            else:\n",
    "                attn = self.softmax(attn)\n",
    "\n",
    "            attn = self.attn_drop(attn)\n",
    "\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "            x = self.proj(x)\n",
    "            x = self.proj_drop(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class SwinTransformerBlock(nn.Module):\n",
    "        \"\"\" Swin Transformer Block.\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            window_size (int): Window size.\n",
    "            shift_size (int): Shift size for SW-MSA.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop (float, optional): Dropout rate. Default: 0.0\n",
    "            attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "            drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "            act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                    mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                    act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.num_heads = num_heads\n",
    "            self.window_size = window_size\n",
    "            self.shift_size = shift_size\n",
    "            self.mlp_ratio = mlp_ratio\n",
    "            assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "            self.norm1 = norm_layer(dim)\n",
    "            self.attn = WindowAttention(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "            self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "            self.H = None\n",
    "            self.W = None\n",
    "\n",
    "        def forward(self, x, mask_matrix):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: Input feature, tensor size (B, H*W, C).\n",
    "                H, W: Spatial resolution of the input feature.\n",
    "                mask_matrix: Attention mask for cyclic shift.\n",
    "            \"\"\"\n",
    "            B, L, C = x.shape\n",
    "            H, W = self.H, self.W\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "            shortcut = x\n",
    "            x = self.norm1(x)\n",
    "            x = x.view(B, H, W, C)\n",
    "\n",
    "            # pad feature maps to multiples of window size\n",
    "            pad_l = pad_t = 0\n",
    "            pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "            pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, Hp, Wp, _ = x.shape\n",
    "\n",
    "            # cyclic shift\n",
    "            if self.shift_size > 0:\n",
    "                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "                attn_mask = mask_matrix\n",
    "            else:\n",
    "                shifted_x = x\n",
    "                attn_mask = None\n",
    "\n",
    "            # partition windows\n",
    "            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "            x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "            # W-MSA/SW-MSA\n",
    "            attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "            # merge windows\n",
    "            attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "            shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "            # reverse cyclic shift\n",
    "            if self.shift_size > 0:\n",
    "                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "            else:\n",
    "                x = shifted_x\n",
    "\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "            x = x.view(B, H * W, C)\n",
    "\n",
    "            # FFN\n",
    "            x = shortcut + self.drop_path(x)\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class PatchMerging(nn.Module):\n",
    "        \"\"\" Patch Merging Layer\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        \"\"\"\n",
    "        def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(4 * dim)\n",
    "\n",
    "        def forward(self, x, H, W):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: Input feature, tensor size (B, H*W, C).\n",
    "                H, W: Spatial resolution of the input feature.\n",
    "            \"\"\"\n",
    "            B, L, C = x.shape\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "            x = x.view(B, H, W, C)\n",
    "\n",
    "            # padding\n",
    "            pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "            x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "            x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "            x = self.norm(x)\n",
    "            x = self.reduction(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class BasicLayer(nn.Module):\n",
    "        \"\"\" A basic Swin Transformer layer for one stage.\n",
    "        Args:\n",
    "            dim (int): Number of feature channels\n",
    "            depth (int): Depths of this stage.\n",
    "            num_heads (int): Number of attention head.\n",
    "            window_size (int): Local window size. Default: 7.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop (float, optional): Dropout rate. Default: 0.0\n",
    "            attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "            drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "            norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    dim,\n",
    "                    depth,\n",
    "                    num_heads,\n",
    "                    window_size=7,\n",
    "                    mlp_ratio=4.,\n",
    "                    qkv_bias=True,\n",
    "                    qk_scale=None,\n",
    "                    drop=0.,\n",
    "                    attn_drop=0.,\n",
    "                    drop_path=0.,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    downsample=None,\n",
    "                    use_checkpoint=False):\n",
    "            super().__init__()\n",
    "            self.window_size = window_size\n",
    "            self.shift_size = window_size // 2\n",
    "            self.depth = depth\n",
    "            self.use_checkpoint = use_checkpoint\n",
    "\n",
    "            # build blocks\n",
    "            self.blocks = nn.ModuleList([\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer)\n",
    "                for i in range(depth)])\n",
    "\n",
    "            # patch merging layer\n",
    "            if downsample is not None:\n",
    "                self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "            else:\n",
    "                self.downsample = None\n",
    "\n",
    "        def forward(self, x, H, W):\n",
    "            \"\"\" Forward function.\n",
    "            Args:\n",
    "                x: Input feature, tensor size (B, H*W, C).\n",
    "                H, W: Spatial resolution of the input feature.\n",
    "            \"\"\"\n",
    "\n",
    "            # calculate attention mask for SW-MSA\n",
    "            Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "            Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "            img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "            for blk in self.blocks:\n",
    "                blk.H, blk.W = H, W\n",
    "                if self.use_checkpoint:\n",
    "                    print('use_checkpoint')\n",
    "                    x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "                else:\n",
    "                    x = blk(x, attn_mask)\n",
    "            if self.downsample is not None:\n",
    "                x_down = self.downsample(x, H, W)\n",
    "                Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "                return x, H, W, x_down, Wh, Ww\n",
    "            else:\n",
    "                return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "    class PatchEmbed(nn.Module):\n",
    "        \"\"\" Image to Patch Embedding\n",
    "        Args:\n",
    "            patch_size (int): Patch token size. Default: 4.\n",
    "            in_chans (int): Number of input image channels. Default: 3.\n",
    "            embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "            norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "            super().__init__()\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            self.patch_size = patch_size\n",
    "\n",
    "            self.in_chans = in_chans\n",
    "            self.embed_dim = embed_dim\n",
    "\n",
    "            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "            if norm_layer is not None:\n",
    "                self.norm = norm_layer(embed_dim)\n",
    "            else:\n",
    "                self.norm = None\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward function.\"\"\"\n",
    "            # padding\n",
    "            _, _, H, W = x.size()\n",
    "            if W % self.patch_size[1] != 0:\n",
    "                x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "            if H % self.patch_size[0] != 0:\n",
    "                x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "            x = self.proj(x)  # B C Wh Ww\n",
    "            if self.norm is not None:\n",
    "                Wh, Ww = x.size(2), x.size(3)\n",
    "                x = x.flatten(2).transpose(1, 2)\n",
    "                x = self.norm(x)\n",
    "                x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    class SwinTransformer(nn.Module):\n",
    "        \"\"\" Swin Transformer backbone.\n",
    "            A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "            https://arxiv.org/pdf/2103.14030\n",
    "        Args:\n",
    "            pretrain_img_size (int): Input image size for training the pretrained model,\n",
    "                used in absolute postion embedding. Default 224.\n",
    "            patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "            in_chans (int): Number of input image channels. Default: 3.\n",
    "            embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "            depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "            num_heads (tuple[int]): Number of attention head of each stage.\n",
    "            window_size (int): Window size. Default: 7.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
    "            drop_rate (float): Dropout rate.\n",
    "            attn_drop_rate (float): Attention dropout rate. Default: 0.\n",
    "            drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "            norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "            ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.\n",
    "            patch_norm (bool): If True, add normalization after patch embedding. Default: True.\n",
    "            out_indices (Sequence[int]): Output from which stages.\n",
    "            frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "                -1 means not freezing any parameters.\n",
    "            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    pretrain_img_size=224,\n",
    "                    patch_size=4,\n",
    "                    in_chans=3,\n",
    "                    embed_dim=96,\n",
    "                    depths=[2, 2, 6, 2],\n",
    "                    num_heads=[3, 6, 12, 24],\n",
    "                    window_size=7,\n",
    "                    mlp_ratio=4.,\n",
    "                    qkv_bias=True,\n",
    "                    qk_scale=None,\n",
    "                    drop_rate=0.,\n",
    "                    attn_drop_rate=0.,\n",
    "                    drop_path_rate=0.2,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    ape=False,\n",
    "                    patch_norm=True,\n",
    "                    out_indices=(0, 1, 2, 3),\n",
    "                    frozen_stages=-1,\n",
    "                    use_checkpoint=False):\n",
    "            super().__init__()\n",
    "\n",
    "            self.pretrain_img_size = pretrain_img_size\n",
    "            self.num_layers = len(depths)\n",
    "            print('self.num_layers', self.num_layers)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.ape = ape\n",
    "            self.patch_norm = patch_norm\n",
    "            self.out_indices = out_indices\n",
    "            self.frozen_stages = frozen_stages\n",
    "            self.fpn = FeaturePyramidNetwork(in_channels_list=[256, 512, 1024],  out_channels=256)\n",
    "\n",
    "            # split image into non-overlapping patches\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "                norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "            # absolute position embedding\n",
    "            if self.ape:\n",
    "                pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "                patch_size = to_2tuple(patch_size)\n",
    "                patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]]\n",
    "\n",
    "                self.absolute_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1]))\n",
    "                trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "            self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "            # stochastic depth\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "            # build layers\n",
    "            self.layers = nn.ModuleList()\n",
    "            for i_layer in range(self.num_layers):\n",
    "                layer = BasicLayer(\n",
    "                    dim=int(embed_dim * 2 ** i_layer),\n",
    "                    depth=depths[i_layer],\n",
    "                    num_heads=num_heads[i_layer],\n",
    "                    window_size=window_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                    norm_layer=norm_layer,\n",
    "                    downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                    use_checkpoint=use_checkpoint)\n",
    "                self.layers.append(layer)\n",
    "\n",
    "            num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "            self.num_features = num_features\n",
    "\n",
    "            # add a norm layer for each output\n",
    "            for i_layer in out_indices:\n",
    "                layer = norm_layer(num_features[i_layer])\n",
    "                layer_name = f'norm{i_layer}'\n",
    "                self.add_module(layer_name, layer)\n",
    "\n",
    "            self._freeze_stages()\n",
    "\n",
    "        def _freeze_stages(self):\n",
    "            if self.frozen_stages >= 0:\n",
    "                self.patch_embed.eval()\n",
    "                for param in self.patch_embed.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            if self.frozen_stages >= 1 and self.ape:\n",
    "                self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "            if self.frozen_stages >= 2:\n",
    "                self.pos_drop.eval()\n",
    "                for i in range(0, self.frozen_stages - 1):\n",
    "                    m = self.layers[i]\n",
    "                    m.eval()\n",
    "                    for param in m.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        def init_weights(self, pretrained=None):\n",
    "            \"\"\"Initialize the weights in backbone.\n",
    "            Args:\n",
    "                pretrained (str, optional): Path to pre-trained weights.\n",
    "                    Defaults to None.\n",
    "            \"\"\"\n",
    "            def _init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    trunc_normal_(m.weight, std=.02)\n",
    "                    if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "            if isinstance(pretrained, str):\n",
    "                self.apply(_init_weights)\n",
    "                checkpoint = torch.load(pretrained, map_location='cpu')\n",
    "                print(f'load from {pretrained}.') \n",
    "                self.load_state_dict(checkpoint['model'], strict=False)\n",
    "            elif pretrained is None:\n",
    "                self.apply(_init_weights)\n",
    "            else:\n",
    "                raise TypeError('pretrained must be a str or None')\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward function.\"\"\"\n",
    "            x = self.patch_embed(x)\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            if self.ape:\n",
    "                # interpolate the position embedding to the corresponding size\n",
    "                absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')\n",
    "                x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "            else:\n",
    "                x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.pos_drop(x)\n",
    "\n",
    "            outs = []\n",
    "            for i in range(self.num_layers):\n",
    "                # print('i', i)\n",
    "                layer = self.layers[i]\n",
    "                x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "                if i in self.out_indices:\n",
    "                    norm_layer = getattr(self, f'norm{i}')\n",
    "                    x_out = norm_layer(x_out)\n",
    "                    out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "                    outs.append(out)\n",
    "            \n",
    "            # Modified swin-based backbone via feature aggregation\n",
    "            rets = {str(u): v for (u,v) in enumerate(outs)}\n",
    "            feat_fpn = self.fpn(rets)        \n",
    "            bs, dim, size_h, size_w = feat_fpn['0'].shape\n",
    "            feat_aggregate = feat_fpn['0'] # torch.Size([1, 256, 25, 34])\n",
    "            outs_agg = []\n",
    "            for k, v in feat_fpn.items():\n",
    "                if k!='0':\n",
    "                    feat = F.interpolate(feat_fpn[k], size=(size_h, size_w), scale_factor=None, mode='bilinear', align_corners=None)\n",
    "                    feat_aggregate = feat_aggregate + feat\n",
    "            outs_agg.append(feat_aggregate) # torch.Size([1, 1024, 7, 9]\n",
    "\n",
    "            rets_agg = {str(u): v for (u,v) in enumerate(outs_agg)}\n",
    "\n",
    "            return rets_agg\n",
    "\n",
    "        def train(self, mode=True):\n",
    "            \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "            super(SwinTransformer, self).train(mode)\n",
    "            self._freeze_stages()\n",
    "\n",
    "\n",
    "    class BackboneBase(nn.Module):\n",
    "        def __init__(self, backbone: nn.Module, strides=[4, 8, 16, 32], num_channels=[96, 192, 384, 768]):\n",
    "            super().__init__()\n",
    "            self.strides = strides\n",
    "            self.num_channels = num_channels\n",
    "            self.body = backbone\n",
    "\n",
    "        def forward(self, tensor_list: NestedTensor):\n",
    "            xs = self.body(tensor_list.tensors)\n",
    "            out: Dict[str, NestedTensor] = {}\n",
    "            for name, x in xs.items():\n",
    "                m = tensor_list.mask\n",
    "                assert m is not None\n",
    "                mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "                out[name] = NestedTensor(x, mask)\n",
    "            return out\n",
    "\n",
    "\n",
    "    class Backbone(BackboneBase):\n",
    "        \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "        def __init__(self, name: str,\n",
    "                    checkpoint: bool = False,\n",
    "                    pretrained: str = None):\n",
    "            assert name in ['swin_t_p4w7', 'swin_s_p4w7', 'swin_b_p4w7', 'swin_l_p4w7']\n",
    "            cfgs = configs[name]\n",
    "            cfgs.update({'use_checkpoint': checkpoint})\n",
    "            out_indices = cfgs['out_indices']\n",
    "            strides = [int(2**(i+2)) for i in out_indices]\n",
    "            num_channels = [int(cfgs['embed_dim'] * 2**i) for i in out_indices]\n",
    "            backbone = SwinTransformer(**cfgs)\n",
    "            backbone.init_weights(pretrained)\n",
    "            super().__init__(backbone, strides, num_channels)\n",
    "\n",
    "\n",
    "    class Joiner(nn.Sequential):\n",
    "        def __init__(self, backbone, position_embedding):\n",
    "            super().__init__(backbone, position_embedding)\n",
    "            self.strides = backbone.strides\n",
    "            self.num_channels = backbone.num_channels\n",
    "\n",
    "        def forward(self, tensor_list: NestedTensor):\n",
    "            xs = self[0](tensor_list)\n",
    "            out: List[NestedTensor] = []\n",
    "            pos = []\n",
    "            for name, x in sorted(xs.items()):\n",
    "                out.append(x)\n",
    "            # position encoding\n",
    "            for x in out:\n",
    "                pos.append(self[1](x).to(x.tensors.dtype))\n",
    "            return out, pos\n",
    "\n",
    "        \n",
    "    def build_swin_backbone(args):\n",
    "        position_embedding = build_position_encoding(args)\n",
    "        backbone = Backbone(args[\"backbone\"], args[\"checkpoint\"], args[\"pretrained\"])\n",
    "        model = Joiner(backbone, position_embedding)\n",
    "        return model\n",
    "\n",
    "\n",
    "    configs = {\n",
    "        'swin_t_p4w7': dict(embed_dim=96,\n",
    "                        depths=[2, 2, 6, 2],\n",
    "                        num_heads=[3, 6, 12, 24],\n",
    "                        window_size=7,\n",
    "                        mlp_ratio=4.,\n",
    "                        qkv_bias=True,\n",
    "                        qk_scale=None,\n",
    "                        drop_rate=0.,\n",
    "                        attn_drop_rate=0.,\n",
    "                        drop_path_rate=0.2,\n",
    "                        ape=False,\n",
    "                        patch_norm=True,\n",
    "                        out_indices=(1, 2, 3),\n",
    "                        use_checkpoint=False),\n",
    "        'swin_s_p4w7': dict(embed_dim=96,\n",
    "                            depths=[2, 2, 18, 2],\n",
    "                            num_heads=[3, 6, 12, 24],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4.,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.,\n",
    "                            attn_drop_rate=0.,\n",
    "                            drop_path_rate=0.2,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            out_indices=(1, 2, 3),\n",
    "                            use_checkpoint=False),\n",
    "        'swin_b_p4w7': dict(embed_dim=128,\n",
    "                            depths=[2, 2, 18, 2],\n",
    "                            num_heads=[4, 8, 16, 32],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4.,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.,\n",
    "                            attn_drop_rate=0.,\n",
    "                            drop_path_rate=0.3,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            out_indices=(1, 2, 3),\n",
    "                            use_checkpoint=True),\n",
    "        'swin_l_p4w7': dict(embed_dim=192,\n",
    "                            depths=[2, 2, 18, 2],\n",
    "                            num_heads=[6, 12, 24, 48],\n",
    "                            window_size=7,\n",
    "                            mlp_ratio=4.,\n",
    "                            qkv_bias=True,\n",
    "                            qk_scale=None,\n",
    "                            drop_rate=0.,\n",
    "                            attn_drop_rate=0.,\n",
    "                            drop_path_rate=0.3,\n",
    "                            ape=False,\n",
    "                            patch_norm=True,\n",
    "                            out_indices=(1, 2, 3),\n",
    "                            use_checkpoint=False),\n",
    "            \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher, loss of bounding box, class and generalized intersection over union (giou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.boxes import box_area\n",
    "\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / union\n",
    "    return iou, union\n",
    "\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Generalized IoU from https://giou.stanford.edu/\n",
    "\n",
    "    The boxes should be in [x0, y0, x1, y1] format\n",
    "\n",
    "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "    and M = len(boxes2)\n",
    "    \"\"\"\n",
    "    # degenerate boxes gives inf / nan results\n",
    "    # so do an early check\n",
    "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cost_class: float = 1,\n",
    "                 cost_bbox: float = 1,\n",
    "                 cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "            # We flatten to compute the cost matrices in a batch\n",
    "            out_prob = outputs[\"pred_logits\"].flatten(0, 1).sigmoid() # [batch_size * num_queries, num_classes]\n",
    "            out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "            # Also concat the target labels and boxes\n",
    "            tgt_ids = torch.cat([v[\"labels\"] for v in targets]) \n",
    "            # print(\"tgt_ids_shape\", tgt_ids.shape)\n",
    "            tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "            # Compute the classification cost.\n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "            pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "            #print(\"pos_cost_class_shape\", pos_cost_class.shape)\n",
    "            cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "            #print(\"cost_class_shape\", cost_class.shape)\n",
    "\n",
    "            # Compute the L1 cost between boxes\n",
    "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "            # Compute the giou cost betwen boxes\n",
    "            cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox),\n",
    "                                             box_cxcywh_to_xyxy(tgt_bbox))\n",
    "\n",
    "            # Final cost matrix\n",
    "            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
    "            C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "            sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "            #print(\"size\", sizes)\n",
    "            indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "            return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "\n",
    "def build_matcher(args):\n",
    "    return HungarianMatcher(cost_class=args[\"set_cost_class\"],\n",
    "                            cost_bbox=args[\"set_cost_bbox\"],\n",
    "                            cost_giou=args[\"set_cost_giou\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
    "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
    "    return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n",
    "\n",
    "def _max_by_axis(the_list):\n",
    "    # type: (List[List[int]]) -> List[int]\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # TODO make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        # TODO make it support different-sized images\n",
    "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from panopticapi.utils import id2rgb, rgb2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class DETRsegm(nn.Module):\n",
    "    def __init__(self, detr, freeze_detr=False):\n",
    "        super().__init__()\n",
    "        self.detr = detr\n",
    "\n",
    "        if freeze_detr:\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n",
    "        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0)\n",
    "        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        if not isinstance(samples, NestedTensor):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.detr.backbone(samples)\n",
    "\n",
    "        bs = features[-1].tensors.shape[0]\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        src_proj = self.detr.input_proj(src)\n",
    "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
    "\n",
    "        outputs_class = self.detr.class_embed(hs)\n",
    "        outputs_coord = self.detr.bbox_embed(hs).sigmoid()\n",
    "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
    "        if self.detr.aux_loss:\n",
    "            out[\"aux_outputs\"] = [\n",
    "                {\"pred_logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])\n",
    "            ]\n",
    "\n",
    "        # FIXME h_boxes takes the last one computed, keep this in mind\n",
    "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
    "\n",
    "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
    "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "\n",
    "        out[\"pred_masks\"] = outputs_seg_masks\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm.\n",
    "    Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
    "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, bbox_mask, fpns):\n",
    "        def expand(tensor, length):\n",
    "            return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
    "\n",
    "        x = torch.cat([expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = expand(cur_fpn, x.size(0) / cur_fpn.size(0))\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.out_lay(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHAttentionMap(nn.Module):\n",
    "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "\n",
    "        nn.init.zeros_(self.k_linear.bias)\n",
    "        nn.init.zeros_(self.q_linear.bias)\n",
    "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
    "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask=None):\n",
    "        q = self.q_linear(q)\n",
    "        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        weights = F.softmax(weights.flatten(2), dim=-1).view_as(weights)\n",
    "        weights = self.dropout(weights)\n",
    "        return weights\n",
    "\n",
    "\n",
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    \"\"\"\n",
    "    Compute the DICE loss, similar to generalized IOU for masks\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "    \"\"\"\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes\n",
    "\n",
    "\n",
    "class PostProcessSegm(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
    "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
    "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
    "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
    "        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
    "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
    "\n",
    "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
    "            img_h, img_w = t[0], t[1]\n",
    "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
    "            results[i][\"masks\"] = F.interpolate(\n",
    "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
    "            ).byte()\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class PostProcessPanoptic(nn.Module):\n",
    "    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n",
    "    coco panoptic API \"\"\"\n",
    "\n",
    "    def __init__(self, is_thing_map, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n",
    "                          the class is  a thing (True) or a stuff (False) class\n",
    "           threshold: confidence threshold: segments with confidence lower than this will be deleted\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.is_thing_map = is_thing_map\n",
    "\n",
    "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
    "        \"\"\" This function computes the panoptic prediction from the model's predictions.\n",
    "        Parameters:\n",
    "            outputs: This is a dict coming directly from the model. See the model doc for the content.\n",
    "            processed_sizes: This is a list of tuples (or torch tensors) of sizes of the images that were passed to the\n",
    "                             model, ie the size after data augmentation but before batching.\n",
    "            target_sizes: This is a list of tuples (or torch tensors) corresponding to the requested final size\n",
    "                          of each prediction. If left to None, it will default to the processed_sizes\n",
    "            \"\"\"\n",
    "        if target_sizes is None:\n",
    "            target_sizes = processed_sizes\n",
    "        assert len(processed_sizes) == len(target_sizes)\n",
    "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
    "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
    "        preds = []\n",
    "\n",
    "        def to_tuple(tup):\n",
    "            if isinstance(tup, tuple):\n",
    "                return tup\n",
    "            return tuple(tup.cpu().tolist())\n",
    "\n",
    "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n",
    "            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n",
    "        ):\n",
    "            # we filter empty queries and detection below threshold\n",
    "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
    "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
    "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
    "            cur_scores = cur_scores[keep]\n",
    "            cur_classes = cur_classes[keep]\n",
    "            cur_masks = cur_masks[keep]\n",
    "            cur_masks = interpolate(cur_masks[None], to_tuple(size), mode=\"bilinear\").squeeze(0)\n",
    "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
    "\n",
    "            h, w = cur_masks.shape[-2:]\n",
    "            assert len(cur_boxes) == len(cur_classes)\n",
    "\n",
    "            # It may be that we have several predicted masks for the same stuff class.\n",
    "            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n",
    "            cur_masks = cur_masks.flatten(1)\n",
    "            stuff_equiv_classes = defaultdict(lambda: [])\n",
    "            for k, label in enumerate(cur_classes):\n",
    "                if not self.is_thing_map[label.item()]:\n",
    "                    stuff_equiv_classes[label.item()].append(k)\n",
    "\n",
    "            def get_ids_area(masks, scores, dedup=False):\n",
    "                # This helper function creates the final panoptic segmentation image\n",
    "                # It also returns the area of the masks that appears on the image\n",
    "\n",
    "                m_id = masks.transpose(0, 1).softmax(-1)\n",
    "\n",
    "                if m_id.shape[-1] == 0:\n",
    "                    # We didn't detect any mask :(\n",
    "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
    "                else:\n",
    "                    m_id = m_id.argmax(-1).view(h, w)\n",
    "\n",
    "                if dedup:\n",
    "                    # Merge the masks corresponding to the same stuff class\n",
    "                    for equiv in stuff_equiv_classes.values():\n",
    "                        if len(equiv) > 1:\n",
    "                            for eq_id in equiv:\n",
    "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
    "\n",
    "                final_h, final_w = to_tuple(target_size)\n",
    "\n",
    "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
    "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
    "\n",
    "                np_seg_img = (\n",
    "                    torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
    "                )\n",
    "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
    "\n",
    "                area = []\n",
    "                for i in range(len(scores)):\n",
    "                    area.append(m_id.eq(i).sum().item())\n",
    "                return area, seg_img\n",
    "\n",
    "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
    "            if cur_classes.numel() > 0:\n",
    "                # We know filter empty masks as long as we find some\n",
    "                while True:\n",
    "                    filtered_small = torch.as_tensor(\n",
    "                        [area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device\n",
    "                    )\n",
    "                    if filtered_small.any().item():\n",
    "                        cur_scores = cur_scores[~filtered_small]\n",
    "                        cur_classes = cur_classes[~filtered_small]\n",
    "                        cur_masks = cur_masks[~filtered_small]\n",
    "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
    "\n",
    "            segments_info = []\n",
    "            for i, a in enumerate(area):\n",
    "                cat = cur_classes[i].item()\n",
    "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
    "            del cur_classes\n",
    "\n",
    "            with io.BytesIO() as out:\n",
    "                seg_img.save(out, format=\"PNG\")\n",
    "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
    "            preds.append(predictions)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Part 2 DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableDETR(nn.Module):\n",
    "    \"\"\" This is the Deformable DETR module that performs object detection \"\"\"\n",
    "    def __init__(self, backbone, transformer, num_classes, num_queries, num_feature_levels, \n",
    "                 num_frames = 3, aux_loss=True, with_box_refine=False, two_stage=False):\n",
    "        \"\"\" Initializes the model.\n",
    "        Parameters:\n",
    "            backbone: torch module of the backbone to be used. See backbone.py\n",
    "            transformer: torch module of the transformer architecture. See transformer.py\n",
    "            num_classes: number of object classes\n",
    "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
    "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
    "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
    "            with_box_refine: iterative bounding box refinement\n",
    "            two_stage: two-stage Deformable DETR\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        #self.num_ref_frames = num_ref_frames\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "        self.num_feature_levels = num_feature_levels\n",
    "\n",
    "        self.temp_class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.temp_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "    \n",
    "        if not two_stage:\n",
    "            self.query_embed = nn.Embedding(num_queries, hidden_dim*2)\n",
    "        if num_feature_levels > 1:\n",
    "            num_backbone_outs = len(backbone.strides)\n",
    "            input_proj_list = []\n",
    "            for _ in range(num_backbone_outs):\n",
    "                in_channels = backbone.num_channels[_]\n",
    "                input_proj_list.append(nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, hidden_dim, kernel_size=1),\n",
    "                    nn.GroupNorm(32, hidden_dim),\n",
    "                ))\n",
    "            for _ in range(num_feature_levels - num_backbone_outs):\n",
    "                input_proj_list.append(nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.GroupNorm(32, hidden_dim),\n",
    "                ))\n",
    "                in_channels = hidden_dim\n",
    "            self.input_proj = nn.ModuleList(input_proj_list)\n",
    "        else:\n",
    "            # self.input_proj = nn.ModuleList([\n",
    "            #     nn.Sequential(\n",
    "            #         nn.Conv2d(backbone.num_channels[0], hidden_dim, kernel_size=1),\n",
    "            #         nn.GroupNorm(32, hidden_dim),\n",
    "            #     )])\n",
    "            self.input_proj = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1),\n",
    "                    nn.GroupNorm(32, hidden_dim),\n",
    "                )])\n",
    "        self.backbone = backbone\n",
    "        self.aux_loss = aux_loss\n",
    "        self.with_box_refine = with_box_refine\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        prior_prob = 0.01\n",
    "        bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        self.class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "        ###############\n",
    "        self.temp_class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "        nn.init.constant_(self.temp_bbox_embed.layers[-1].weight.data, 0)\n",
    "        nn.init.constant_(self.temp_bbox_embed.layers[-1].bias.data, 0)\n",
    "        nn.init.constant_(self.temp_bbox_embed.layers[-1].bias.data[2:], -2.0)\n",
    "        ##############\n",
    "\n",
    "        nn.init.constant_(self.bbox_embed.layers[-1].weight.data, 0)\n",
    "        nn.init.constant_(self.bbox_embed.layers[-1].bias.data, 0)\n",
    "        for proj in self.input_proj:\n",
    "            nn.init.xavier_uniform_(proj[0].weight, gain=1)\n",
    "            nn.init.constant_(proj[0].bias, 0)\n",
    "        self.temp_class_embed_list = _get_clones(self.temp_class_embed, 3)\n",
    "        self.temp_bbox_embed_list = _get_clones(self.temp_bbox_embed, 3)\n",
    "        # if two-stage, the last class_embed and bbox_embed is for region proposal generation\n",
    "        num_pred = (transformer.decoder.num_layers + 1) if two_stage else transformer.decoder.num_layers\n",
    "        if with_box_refine:\n",
    "            self.class_embed = _get_clones(self.class_embed, num_pred)\n",
    "            self.bbox_embed = _get_clones(self.bbox_embed, num_pred)\n",
    "            nn.init.constant_(self.bbox_embed[0].layers[-1].bias.data[2:], -2.0)\n",
    "            # hack implementation for iterative bounding box refinement\n",
    "            self.transformer.decoder.bbox_embed = self.bbox_embed\n",
    "        else:\n",
    "            nn.init.constant_(self.bbox_embed.layers[-1].bias.data[2:], -2.0)\n",
    "            self.class_embed = nn.ModuleList([self.class_embed for _ in range(num_pred)])\n",
    "            self.bbox_embed = nn.ModuleList([self.bbox_embed for _ in range(num_pred)])\n",
    "            self.transformer.decoder.bbox_embed = None\n",
    "        if two_stage:\n",
    "            # hack implementation for two-stage\n",
    "            self.transformer.decoder.class_embed = self.class_embed\n",
    "            for box_embed in self.bbox_embed:\n",
    "                nn.init.constant_(box_embed.layers[-1].bias.data[2:], 0.0)\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        \"\"\"The forward expects a NestedTensor, which consists of:\n",
    "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
    "\n",
    "            It returns a dict with the following elements:\n",
    "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
    "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
    "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
    "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
    "                               relative to the size of each individual image (disregarding possible padding).\n",
    "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
    "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
    "                                dictionnaries containing the two above keys for each decoder layer.\n",
    "        \"\"\"\n",
    "        if not isinstance(samples, NestedTensor):\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "        # print('features[-1].tensors.shape', features[-1].tensors.shape)\n",
    "\n",
    "        srcs = []\n",
    "        masks = []\n",
    "        for l, feat in enumerate(features):\n",
    "            src, mask = feat.decompose()\n",
    "            srcs.append(self.input_proj[l](src))\n",
    "            masks.append(mask)\n",
    "            assert mask is not None\n",
    "\n",
    "        if self.num_feature_levels > len(srcs):\n",
    "            _len_srcs = len(srcs)\n",
    "            for l in range(_len_srcs, self.num_feature_levels):\n",
    "                if l == _len_srcs:\n",
    "                    src = self.input_proj[l](features[-1].tensors)\n",
    "                else:\n",
    "                    src = self.input_proj[l](srcs[-1])\n",
    "                m = samples.mask\n",
    "                mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]\n",
    "                pos_l = self.backbone[1](NestedTensor(src, mask)).to(src.dtype)\n",
    "                srcs.append(src)\n",
    "                masks.append(mask)\n",
    "                pos.append(pos_l)\n",
    "\n",
    "        query_embeds = None\n",
    "        if not self.two_stage:\n",
    "            query_embeds = self.query_embed.weight\n",
    "        hs, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact, final_hs, final_references_out, out = self.transformer(srcs, masks, pos, query_embeds, self.class_embed[-1], self.temp_class_embed_list, self.temp_bbox_embed_list)\n",
    "        \n",
    "\n",
    "        outputs_classes = []\n",
    "        outputs_coords = []\n",
    "        \n",
    "       # out = {}\n",
    "        if self.two_stage:\n",
    "            enc_outputs_coord = enc_outputs_coord_unact.sigmoid()\n",
    "            out['enc_outputs'] = {'pred_logits': enc_outputs_class, 'pred_boxes': enc_outputs_coord}\n",
    "     \n",
    "        if final_hs is not None:\n",
    "            reference = inverse_sigmoid(final_references_out)\n",
    "            output_class = self.temp_class_embed_list[2](final_hs)\n",
    "            tmp = self.temp_bbox_embed_list[2](final_hs)\n",
    "            if reference.shape[-1] == 4:\n",
    "                tmp += reference\n",
    "            else:\n",
    "                assert reference.shape[-1] == 2\n",
    "                tmp[..., :2] += reference\n",
    "            output_coord = tmp.sigmoid()\n",
    "            out[\"pred_logits\"] = output_class # [4, 300, 30]\n",
    "            out[\"pred_boxes\"] = output_coord  # [4, 300, 4]\n",
    "        #print(out.keys())\n",
    "        return out\n",
    "\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        # this is a workaround to make torchscript happy, as torchscript\n",
    "        # doesn't support dictionary with non-homogeneous values, such\n",
    "        # as a dict having both a Tensor and a list.\n",
    "        return [{'pred_logits': a, 'pred_boxes': b}\n",
    "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
    "\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, matcher, weight_dict, losses, focal_alpha=0.25):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "            focal_alpha: alpha in Focal Loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.focal_alpha = focal_alpha\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n",
    "                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)\n",
    "        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "\n",
    "        target_classes_onehot = target_classes_onehot[:,:,:-1]\n",
    "        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2) * src_logits.shape[1]\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, h, w), normalized by the image size.\n",
    "        \"\"\"\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
    "            box_cxcywh_to_xyxy(src_boxes),\n",
    "            box_cxcywh_to_xyxy(target_boxes)))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "\n",
    "        # TODO use valid to mask invalid areas due to padding in loss\n",
    "        target_masks, valid = nested_tensor_from_tensor_list([t[\"masks\"] for t in targets]).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "\n",
    "        src_masks = src_masks[src_idx]\n",
    "        # upsample predictions to the target size\n",
    "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks[tgt_idx].flatten(1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'boxes': self.loss_boxes,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs' and k != 'enc_outputs'}\n",
    "        #print(outputs_without_aux)\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs[\"pred_boxes\"])).device)\n",
    "        \n",
    "        num_boxes = torch.clamp(num_boxes / 1, min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            kwargs = {}\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes, **kwargs))\n",
    "\n",
    "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    if loss == 'masks':\n",
    "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
    "                        continue\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        # Logging is enabled only for the last layer\n",
    "                        kwargs['log'] = False\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        if 'enc_outputs' in outputs:\n",
    "            enc_outputs = outputs['enc_outputs']\n",
    "            bin_targets = copy.deepcopy(targets)\n",
    "            for bt in bin_targets:\n",
    "                bt['labels'] = torch.zeros_like(bt['labels'])\n",
    "            indices = self.matcher(enc_outputs, bin_targets)\n",
    "            for loss in self.losses:\n",
    "                if loss == 'masks':\n",
    "                    # Intermediate masks losses are too costly to compute, we ignore them.\n",
    "                    continue\n",
    "                kwargs = {}\n",
    "                if loss == 'labels':\n",
    "                    # Logging is enabled only for the last layer\n",
    "                    kwargs['log'] = False\n",
    "                l_dict = self.get_loss(loss, enc_outputs, bin_targets, indices, num_boxes, **kwargs)\n",
    "                l_dict = {k + f'_enc': v for k, v in l_dict.items()}\n",
    "                losses.update(l_dict)\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "class PostProcess(nn.Module):\n",
    "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        \"\"\" Perform the computation\n",
    "        Parameters:\n",
    "            outputs: raw outputs of the model\n",
    "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
    "                          For evaluation, this must be the original image size (before any data augmentation)\n",
    "                          For visualization, this should be the image size after data augment, but before padding\n",
    "        \"\"\"\n",
    "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
    "\n",
    "        assert len(out_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        prob = out_logits.sigmoid()\n",
    "        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n",
    "        scores = topk_values\n",
    "        topk_boxes = topk_indexes // out_logits.shape[2]\n",
    "        labels = topk_indexes % out_logits.shape[2]\n",
    "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1,1,4))\n",
    "\n",
    "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "        boxes = boxes * scale_fct[:, None, :]\n",
    "\n",
    "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(args):\n",
    "    num_classes = 31 #2 Maybe let original number of classes and fine tune one !!!!\n",
    "    device = torch.device(args[\"device\"])\n",
    "\n",
    "    if 'swin' in args[\"backbone\"]:\n",
    "        backbone = build_swin_backbone(args) \n",
    "    else:\n",
    "        backbone = build_backbone(args)\n",
    "    # backbone = build_backbone(args)\n",
    "\n",
    "    transformer = build_deforamble_transformer(args)\n",
    "    model = DeformableDETR(\n",
    "        backbone,\n",
    "        transformer,\n",
    "        num_classes=num_classes,\n",
    "        num_queries=args[\"num_queries\"],\n",
    "        num_feature_levels=args[\"num_feature_levels\"],\n",
    "        num_frames=args[\"num_frames\"],\n",
    "        aux_loss=args[\"aux_loss\"],\n",
    "        with_box_refine=args[\"with_box_refine\"],\n",
    "        two_stage=args[\"two_stage\"],\n",
    "    )\n",
    "    if args[\"masks\"]:\n",
    "        model = DETRsegm(model, freeze_detr=(args[\"frozen_weights\"] is not None))\n",
    "    matcher = build_matcher(args)\n",
    "    weight_dict = {'loss_ce': args[\"cls_loss_coef\"], 'loss_bbox': args[\"bbox_loss_coef\"]}\n",
    "    weight_dict['loss_giou'] = args[\"giou_loss_coef\"]\n",
    "    if args[\"masks\"]:\n",
    "        weight_dict[\"loss_mask\"] = args[\"mask_loss_coef\"]\n",
    "        weight_dict[\"loss_dice\"] = args[\"dice_loss_coef\"]\n",
    "    # TODO this is a hack\n",
    "    if args[\"aux_loss\"]:\n",
    "        aux_weight_dict = {}\n",
    "        for i in range(args[\"dec_layers\"] - 1):\n",
    "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "        aux_weight_dict.update({k + f'_enc': v for k, v in weight_dict.items()})\n",
    "        weight_dict.update(aux_weight_dict)\n",
    "\n",
    "    losses = ['labels', 'boxes', 'cardinality']\n",
    "    if args[\"masks\"]:\n",
    "        losses += [\"masks\"]\n",
    "    # num_classes, matcher, weight_dict, losses, focal_alpha=0.25\n",
    "    criterion = SetCriterion(num_classes, matcher, weight_dict, losses, focal_alpha=args[\"focal_alpha\"])\n",
    "    criterion.to(device)\n",
    "    postprocessors = {'bbox': PostProcess()}\n",
    "    if args[\"masks\"]:\n",
    "        postprocessors['segm'] = PostProcessSegm()\n",
    "        if args[\"dataset_file\"] == \"coco_panoptic\":\n",
    "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
    "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
    "\n",
    "    return model, criterion, postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_layers 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model, criterion, postprocessors=build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeformableDETR(\n",
      "  (transformer): DeformableTransformer(\n",
      "    (encoder): DeformableTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_embed): ModuleList(\n",
      "        (0-5): 6 x MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (temporal_query_layer1): TemporalQueryEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout4): Dropout(p=0.1, inplace=False)\n",
      "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (temporal_query_layer2): TemporalQueryEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout4): Dropout(p=0.1, inplace=False)\n",
      "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (temporal_query_layer3): TemporalQueryEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout4): Dropout(p=0.1, inplace=False)\n",
      "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (temporal_decoder1): TemporalDeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (temporal_decoder2): TemporalDeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (temporal_decoder3): TemporalDeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=32, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reference_points): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (class_embed): ModuleList(\n",
      "    (0-5): 6 x Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      "  (bbox_embed): ModuleList(\n",
      "    (0-5): 6 x MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (temp_class_embed): Linear(in_features=256, out_features=31, bias=True)\n",
      "  (temp_bbox_embed): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (query_embed): Embedding(100, 512)\n",
      "  (input_proj): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (backbone): Joiner(\n",
      "    (0): Backbone(\n",
      "      (body): SwinTransformer(\n",
      "        (fpn): FeaturePyramidNetwork(\n",
      "          (inner_blocks): ModuleList(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (layer_blocks): ModuleList(\n",
      "            (0-2): 3 x Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.013)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.026)\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.039)\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (2): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.052)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.065)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.078)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.091)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.104)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.117)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.130)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.143)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.157)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.170)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.183)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.196)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (12): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.209)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (13): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.222)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (14): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.235)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (15): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.248)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (16): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.261)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (17): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.274)\n",
      "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (3): BasicLayer(\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.287)\n",
      "                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.300)\n",
      "                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (temp_class_embed_list): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      "  (temp_bbox_embed_list): ModuleList(\n",
      "    (0-2): 3 x MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vid multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.vision import VisionDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import tqdm\n",
    "from io import BytesIO\n",
    "from pycocotools.coco import COCO, _isArrayLike\n",
    "\n",
    "class TvCocoDetection(VisionDataset):\n",
    "    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        annFile (string): Path to json annotation file.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, annFile, transform=None, target_transform=None, transforms=None,\n",
    "                 cache_mode=False, local_rank=0, local_size=1):\n",
    "        super(TvCocoDetection, self).__init__(root, transforms, transform, target_transform)\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.cache_mode = cache_mode\n",
    "        self.local_rank = local_rank\n",
    "        self.local_size = local_size\n",
    "        if cache_mode:\n",
    "            self.cache = {}\n",
    "            self.cache_images()\n",
    "\n",
    "    def cache_images(self):\n",
    "        self.cache = {}\n",
    "        for index, img_id in zip(tqdm.trange(len(self.ids)), self.ids):\n",
    "            if index % self.local_size != self.local_rank:\n",
    "                continue\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "            with open(os.path.join(self.root, path), 'rb') as f:\n",
    "                self.cache[path] = f.read()\n",
    "\n",
    "    def get_image(self, path):\n",
    "        if self.cache_mode:\n",
    "            if path not in self.cache.keys():\n",
    "                with open(os.path.join(self.root, path), 'rb') as f:\n",
    "                    self.cache[path] = f.read()\n",
    "            return Image.open(BytesIO(self.cache[path])).convert('RGB')\n",
    "        return Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = self.get_image(path)\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "class CocoVID(COCO):\n",
    "    \"\"\"Inherit official COCO class in order to parse the annotations of bbox-\n",
    "    related video tasks.\n",
    "    Args:\n",
    "        annotation_file (str): location of annotation file. Defaults to None.\n",
    "        load_img_as_vid (bool): If True, convert image data to video data,\n",
    "            which means each image is converted to a video. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotation_file=None, load_img_as_vid=False):\n",
    "        assert annotation_file, 'Annotation file must be provided.'\n",
    "        self.load_img_as_vid = load_img_as_vid\n",
    "        super(CocoVID, self).__init__(annotation_file=annotation_file)\n",
    "\n",
    "    def convert_img_to_vid(self, dataset):\n",
    "        \"\"\"Convert image data to video data.\"\"\"\n",
    "        if 'images' in self.dataset:\n",
    "            videos = []\n",
    "            for i, img in enumerate(self.dataset['images']):\n",
    "                videos.append(dict(id=img['id'], name=img['file_name']))\n",
    "                img['video_id'] = img['id']\n",
    "                img['frame_id'] = 0\n",
    "            dataset['videos'] = videos\n",
    "\n",
    "        if 'annotations' in self.dataset:\n",
    "            for i, ann in enumerate(self.dataset['annotations']):\n",
    "                ann['video_id'] = ann['image_id']\n",
    "                ann['instance_id'] = ann['id']\n",
    "        return dataset\n",
    "\n",
    "    def createIndex(self):\n",
    "        \"\"\"Create index.\"\"\"\n",
    "        print('creating index...')\n",
    "        anns, cats, imgs, vids = {}, {}, {}, {}\n",
    "        (imgToAnns, catToImgs, vidToImgs, vidToInstances,\n",
    "         instancesToImgs) = defaultdict(list), defaultdict(list), defaultdict(\n",
    "             list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "        if 'videos' not in self.dataset and self.load_img_as_vid:\n",
    "            self.dataset = self.convert_img_to_vid(self.dataset)\n",
    "\n",
    "        if 'videos' in self.dataset:\n",
    "            for video in self.dataset['videos']:\n",
    "                vids[video['id']] = video\n",
    "\n",
    "        if 'annotations' in self.dataset:\n",
    "            for ann in self.dataset['annotations']:\n",
    "                imgToAnns[ann['image_id']].append(ann)\n",
    "                anns[ann['id']] = ann\n",
    "                if 'instance_id' in ann:\n",
    "                    instancesToImgs[ann['instance_id']].append(ann['image_id'])\n",
    "                    if 'video_id' in ann and \\\n",
    "                        ann['instance_id'] not in \\\n",
    "                            vidToInstances[ann['video_id']]:\n",
    "                        vidToInstances[ann['video_id']].append(\n",
    "                            ann['instance_id'])\n",
    "\n",
    "        if 'images' in self.dataset:\n",
    "            for img in self.dataset['images']:\n",
    "                vidToImgs[img['video_id']].append(img)\n",
    "                imgs[img['id']] = img\n",
    "\n",
    "        if 'categories' in self.dataset:\n",
    "            for cat in self.dataset['categories']:\n",
    "                cats[cat['id']] = cat\n",
    "\n",
    "        if 'annotations' in self.dataset and 'categories' in self.dataset:\n",
    "            for ann in self.dataset['annotations']:\n",
    "                catToImgs[ann['category_id']].append(ann['image_id'])\n",
    "\n",
    "        print('index created!')\n",
    "\n",
    "        self.anns = anns\n",
    "        self.imgToAnns = imgToAnns\n",
    "        self.catToImgs = catToImgs\n",
    "        self.imgs = imgs\n",
    "        self.cats = cats\n",
    "        self.videos = vids\n",
    "        self.vidToImgs = vidToImgs\n",
    "        self.vidToInstances = vidToInstances\n",
    "        self.instancesToImgs = instancesToImgs\n",
    "\n",
    "    def get_vid_ids(self, vidIds=[]):\n",
    "        \"\"\"Get video ids that satisfy given filter conditions.\n",
    "        Default return all video ids.\n",
    "        Args:\n",
    "            vidIds (list[int]): The given video ids. Defaults to [].\n",
    "        Returns:\n",
    "            list[int]: Video ids.\n",
    "        \"\"\"\n",
    "        vidIds = vidIds if _isArrayLike(vidIds) else [vidIds]\n",
    "\n",
    "        if len(vidIds) == 0:\n",
    "            ids = self.videos.keys()\n",
    "        else:\n",
    "            ids = set(vidIds)\n",
    "\n",
    "        return list(ids)\n",
    "\n",
    "    def get_img_ids_from_vid(self, vidId):\n",
    "        \"\"\"Get image ids from given video id.\n",
    "        Args:\n",
    "            vidId (int): The given video id.\n",
    "        Returns:\n",
    "            list[int]: Image ids of given video id.\n",
    "        \"\"\"\n",
    "        img_infos = self.vidToImgs[vidId]\n",
    "        ids = list(np.zeros([len(img_infos)], dtype=np.int64))\n",
    "\n",
    "        for i, img_info in enumerate(img_infos):\n",
    "            ids[i] = img_info[\"id\"]\n",
    "        # for img_info in img_infos:\n",
    "        #     ids[img_info['frame_id']] = img_info['id']\n",
    "            \n",
    "        return ids\n",
    "\n",
    "    def get_ins_ids_from_vid(self, vidId):\n",
    "        \"\"\"Get instance ids from given video id.\n",
    "        Args:\n",
    "            vidId (int): The given video id.\n",
    "        Returns:\n",
    "            list[int]: Instance ids of given video id.\n",
    "        \"\"\"\n",
    "        return self.vidToInstances[vidId]\n",
    "\n",
    "    def get_img_ids_from_ins_id(self, insId):\n",
    "        \"\"\"Get image ids from given instance id.\n",
    "        Args:\n",
    "            insId (int): The given instance id.\n",
    "        Returns:\n",
    "            list[int]: Image ids of given instance id.\n",
    "        \"\"\"\n",
    "        return self.instancesToImgs[insId]\n",
    "\n",
    "    def load_vids(self, ids=[]):\n",
    "        \"\"\"Get video information of given video ids.\n",
    "        Default return all videos information.\n",
    "        Args:\n",
    "            ids (list[int]): The given video ids. Defaults to [].\n",
    "        Returns:\n",
    "            list[dict]: List of video information.\n",
    "        \"\"\"\n",
    "        if _isArrayLike(ids):\n",
    "            return [self.videos[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.videos[ids]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "from numpy import random as rand\n",
    "import PIL\n",
    "import torchvision.transforms.functional as Ft\n",
    "\n",
    "\n",
    "def bbox_overlaps(bboxes1, bboxes2, mode='iou', eps=1e-6):\n",
    "    assert mode in ['iou', 'iof']\n",
    "    bboxes1 = bboxes1.astype(np.float32)\n",
    "    bboxes2 = bboxes2.astype(np.float32)\n",
    "    rows = bboxes1.shape[0]\n",
    "    cols = bboxes2.shape[0]\n",
    "    ious = np.zeros((rows, cols), dtype=np.float32)\n",
    "    if rows * cols == 0:\n",
    "        return ious\n",
    "    exchange = False\n",
    "    if bboxes1.shape[0] > bboxes2.shape[0]:\n",
    "        bboxes1, bboxes2 = bboxes2, bboxes1\n",
    "        ious = np.zeros((cols, rows), dtype=np.float32)\n",
    "        exchange = True\n",
    "    area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
    "    area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
    "    for i in range(bboxes1.shape[0]):\n",
    "        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n",
    "        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n",
    "        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n",
    "        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n",
    "        overlap = np.maximum(x_end - x_start, 0) * np.maximum(y_end - y_start, 0)\n",
    "        if mode == 'iou':\n",
    "            union = area1[i] + area2 - overlap\n",
    "        else:\n",
    "            union = area1[i] if not exchange else area2\n",
    "        union = np.maximum(union, eps)\n",
    "        ious[i, :] = overlap / union\n",
    "    if exchange:\n",
    "        ious = ious.T\n",
    "    return ious\n",
    "\n",
    "\n",
    "def crop(clip, target, region):\n",
    "    cropped_image = []\n",
    "    for image in clip:\n",
    "        cropped_image.append(Ft.crop(image, *region))\n",
    "\n",
    "    target = target.copy()\n",
    "    i, j, h, w = region\n",
    "\n",
    "    # should we do something wrt the original size?\n",
    "    target[\"size\"] = torch.tensor([h, w])\n",
    "\n",
    "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
    "\n",
    "    if \"boxes\" in target:\n",
    "        boxes = target[\"boxes\"]\n",
    "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
    "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
    "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
    "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
    "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
    "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
    "        target[\"area\"] = area\n",
    "        fields.append(\"boxes\")\n",
    "\n",
    "    if \"masks\" in target:\n",
    "        # FIXME should we update the area here if there are no boxes?\n",
    "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
    "        fields.append(\"masks\")\n",
    "\n",
    "    return cropped_image, target\n",
    "\n",
    "\n",
    "def hflip(clip, target):\n",
    "    flipped_image = []\n",
    "    for image in clip:\n",
    "        flipped_image.append(Ft.hflip(image))\n",
    "\n",
    "    w, h = clip[0].size\n",
    "\n",
    "    targets = target.copy()\n",
    "    \n",
    "    for target in targets:\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
    "            target[\"boxes\"] = boxes\n",
    "\n",
    "        if \"masks\" in target:\n",
    "            target['masks'] = target['masks'].flip(-1)\n",
    "    \n",
    "    return flipped_image, targets\n",
    "\n",
    "def vflip(clip,target):\n",
    "    flipped_image = []\n",
    "    for image in clip:\n",
    "        flipped_image.append(Ft.vflip(image))\n",
    "    w, h = clip[0].size\n",
    "\n",
    "    targets = target.copy()\n",
    "    for target in targets:\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes = boxes[:, [0, 3, 2, 1]] * torch.as_tensor([1, -1, 1, -1]) + torch.as_tensor([0, h, 0, h])\n",
    "            target[\"boxes\"] = boxes\n",
    "        if \"masks\" in target:\n",
    "            target['masks'] = target['masks'].flip(1)\n",
    "\n",
    "    return flipped_image, targets\n",
    "\n",
    "def resize(clip, target, size, max_size=None):\n",
    "    # size can be min_size (scalar) or (w, h) tuple\n",
    "\n",
    "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
    "        w, h = image_size\n",
    "        if max_size is not None:\n",
    "            min_original_size = float(min((w, h)))\n",
    "            max_original_size = float(max((w, h)))\n",
    "            if max_original_size / min_original_size * size > max_size:\n",
    "                size = int(round(max_size * min_original_size / max_original_size))\n",
    "\n",
    "        if (w <= h and w == size) or (h <= w and h == size):\n",
    "            return (h, w)\n",
    "\n",
    "        if w < h:\n",
    "            ow = size\n",
    "            oh = int(size * h / w)\n",
    "        else:\n",
    "            oh = size\n",
    "            ow = int(size * w / h)\n",
    "\n",
    "        return (oh, ow)\n",
    "\n",
    "    def get_size(image_size, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            return size[::-1]\n",
    "        else:\n",
    "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
    "\n",
    "    size = get_size(clip[0].size, size, max_size)\n",
    "    rescaled_image = []\n",
    "    for image in clip:\n",
    "        rescaled_image.append(Ft.resize(image, size))\n",
    "\n",
    "    if target is None:\n",
    "        return rescaled_image, None\n",
    "\n",
    "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image[0].size, clip[0].size))\n",
    "    ratio_width, ratio_height = ratios\n",
    "\n",
    "\n",
    "    \n",
    "    targets = target.copy()\n",
    "    for target in targets:\n",
    "        # print(\"transforms_nips_164\", target)\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
    "            target[\"boxes\"] = scaled_boxes\n",
    "\n",
    "        if \"area\" in target:\n",
    "            area = target[\"area\"]\n",
    "            scaled_area = area * (ratio_width * ratio_height)\n",
    "            target[\"area\"] = scaled_area\n",
    "\n",
    "        h, w = size\n",
    "        target[\"size\"] = torch.tensor([h, w])\n",
    "\n",
    "        if \"masks\" in target:\n",
    "            if target['masks'].shape[0]>0:\n",
    "                target['masks'] = interpolate(\n",
    "                    target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
    "            else:\n",
    "                target['masks'] = torch.zeros((target['masks'].shape[0],h,w))\n",
    "    return rescaled_image, targets\n",
    "\n",
    "\n",
    "def pad(clip, target, padding):\n",
    "    # assumes that we only pad on the bottom right corners\n",
    "    padded_image = []\n",
    "    for image in clip:\n",
    "        padded_image.append(F.pad(image, (0, 0, padding[0], padding[1])))\n",
    "    if target is None:\n",
    "        return padded_image, None\n",
    "    target = target.copy()\n",
    "    # should we do something wrt the original size?\n",
    "    target[\"size\"] = torch.tensor(padded_image[0].size[::-1])\n",
    "    if \"masks\" in target:\n",
    "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
    "    return padded_image, target\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        region = T.RandomCrop.get_params(img, self.size)\n",
    "        return crop(img, target, region)\n",
    "\n",
    "\n",
    "class RandomSizeCrop(object):\n",
    "    def __init__(self, min_size: int, max_size: int):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
    "        w = random.randint(self.min_size, min(img[0].width, self.max_size))\n",
    "        h = random.randint(self.min_size, min(img[0].height, self.max_size))\n",
    "        region = T.RandomCrop.get_params(img[0], [h, w])\n",
    "        return crop(img, target, region)\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        image_width, image_height = img.size\n",
    "        crop_height, crop_width = self.size\n",
    "        crop_top = int(round((image_height - crop_height) / 2.))\n",
    "        crop_left = int(round((image_width - crop_width) / 2.))\n",
    "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
    "\n",
    "\n",
    "class MinIoURandomCrop(object):\n",
    "    def __init__(self, min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3):\n",
    "        self.min_ious = min_ious\n",
    "        self.sample_mode = (1, *min_ious, 0)\n",
    "        self.min_crop_size = min_crop_size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        w,h = img.size\n",
    "        while True:\n",
    "            mode = random.choice(self.sample_mode)\n",
    "            self.mode = mode\n",
    "            if mode == 1:\n",
    "                return img,target\n",
    "            min_iou = mode\n",
    "            boxes = target['boxes'].numpy()\n",
    "            labels = target['labels']\n",
    "\n",
    "            for i in range(50):\n",
    "                new_w = rand.uniform(self.min_crop_size * w, w)\n",
    "                new_h = rand.uniform(self.min_crop_size * h, h)\n",
    "                if new_h / new_w < 0.5 or new_h / new_w > 2:\n",
    "                    continue\n",
    "                left = rand.uniform(w - new_w)\n",
    "                top = rand.uniform(h - new_h)\n",
    "                patch = np.array((int(left), int(top), int(left + new_w), int(top + new_h)))\n",
    "                if patch[2] == patch[0] or patch[3] == patch[1]:\n",
    "                    continue\n",
    "                overlaps = bbox_overlaps(patch.reshape(-1, 4), boxes.reshape(-1, 4)).reshape(-1)\n",
    "                if len(overlaps) > 0 and overlaps.min() < min_iou:\n",
    "                    continue\n",
    "                \n",
    "                if len(overlaps) > 0:\n",
    "                    def is_center_of_bboxes_in_patch(boxes, patch):\n",
    "                        center = (boxes[:, :2] + boxes[:, 2:]) / 2\n",
    "                        mask = ((center[:, 0] > patch[0]) * (center[:, 1] > patch[1]) * (center[:, 0] < patch[2]) * (center[:, 1] < patch[3]))\n",
    "                        return mask\n",
    "                    mask = is_center_of_bboxes_in_patch(boxes, patch)\n",
    "                    if False in mask:\n",
    "                        continue\n",
    "                    #TODO: use no center boxes\n",
    "                    #if not mask.any():\n",
    "                    #    continue\n",
    "\n",
    "                    boxes[:, 2:] = boxes[:, 2:].clip(max=patch[2:])\n",
    "                    boxes[:, :2] = boxes[:, :2].clip(min=patch[:2])\n",
    "                    boxes -= np.tile(patch[:2], 2)\n",
    "                    target['boxes'] = torch.tensor(boxes)\n",
    "                \n",
    "                img = np.asarray(img)[patch[1]:patch[3], patch[0]:patch[2]]\n",
    "                img = Image.fromarray(img)\n",
    "                width, height = img.size\n",
    "                target['orig_size'] = torch.tensor([height,width])\n",
    "                target['size'] = torch.tensor([height,width])\n",
    "                return img,target \n",
    "\n",
    "\n",
    "class RandomContrast(object):\n",
    "    def __init__(self, lower=0.5, upper=1.5):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n",
    "        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n",
    "    def __call__(self, image, target):\n",
    "        \n",
    "        if rand.randint(2):\n",
    "            alpha = rand.uniform(self.lower, self.upper)\n",
    "            image *= alpha\n",
    "        return image, target\n",
    "\n",
    "class RandomBrightness(object):\n",
    "    def __init__(self, delta=32):\n",
    "        assert delta >= 0.0\n",
    "        assert delta <= 255.0\n",
    "        self.delta = delta\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            delta = rand.uniform(-self.delta, self.delta)\n",
    "            image += delta\n",
    "        return image, target\n",
    "\n",
    "class RandomSaturation(object):\n",
    "    def __init__(self, lower=0.5, upper=1.5):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n",
    "        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            image[:, :, 1] *= rand.uniform(self.lower, self.upper)\n",
    "        return image, target\n",
    "\n",
    "class RandomHue(object): #\n",
    "    def __init__(self, delta=18.0):\n",
    "        assert delta >= 0.0 and delta <= 360.0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            image[:, :, 0] += rand.uniform(-self.delta, self.delta)\n",
    "            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n",
    "            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n",
    "        return image, target\n",
    "\n",
    "class RandomLightingNoise(object):\n",
    "    def __init__(self):\n",
    "        self.perms = ((0, 1, 2), (0, 2, 1),\n",
    "                      (1, 0, 2), (1, 2, 0),\n",
    "                      (2, 0, 1), (2, 1, 0))\n",
    "    def __call__(self, image, target):\n",
    "        if rand.randint(2):\n",
    "            swap = self.perms[rand.randint(len(self.perms))]\n",
    "            shuffle = SwapChannels(swap)  # shuffle channels\n",
    "            image = shuffle(image)\n",
    "        return image, target\n",
    "\n",
    "class ConvertColor(object):\n",
    "    def __init__(self, current='BGR', transform='HSV'):\n",
    "        self.transform = transform\n",
    "        self.current = current\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if self.current == 'BGR' and self.transform == 'HSV':\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        elif self.current == 'HSV' and self.transform == 'BGR':\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return image, target\n",
    "\n",
    "class SwapChannels(object):\n",
    "    def __init__(self, swaps):\n",
    "        self.swaps = swaps\n",
    "    def __call__(self, image):\n",
    "        image = image[:, :, self.swaps]\n",
    "        return image\n",
    "\n",
    "class PhotometricDistort(object):\n",
    "    def __init__(self):\n",
    "        self.pd = [\n",
    "            RandomContrast(),\n",
    "            ConvertColor(transform='HSV'),\n",
    "            RandomSaturation(),\n",
    "            RandomHue(),\n",
    "            ConvertColor(current='HSV', transform='BGR'),\n",
    "            RandomContrast()\n",
    "        ]\n",
    "        self.rand_brightness = RandomBrightness()\n",
    "        self.rand_light_noise = RandomLightingNoise()\n",
    "    \n",
    "    def __call__(self,clip,target):\n",
    "        imgs = []\n",
    "        for img in clip:\n",
    "            img = np.asarray(img).astype('float32')\n",
    "            img, target = self.rand_brightness(img, target)\n",
    "            if rand.randint(2):\n",
    "                distort = Compose(self.pd[:-1])\n",
    "            else:\n",
    "                distort = Compose(self.pd[1:])\n",
    "            img, target = distort(img, target)\n",
    "            img, target = self.rand_light_noise(img, target)\n",
    "            imgs.append(Image.fromarray(img.astype('uint8')))\n",
    "        return imgs, target\n",
    "\n",
    "#NOTICE: if used for mask, need to change\n",
    "class Expand(object):\n",
    "    def __init__(self, mean):\n",
    "        self.mean = mean\n",
    "    def __call__(self, clip, target):\n",
    "        if rand.randint(2):\n",
    "            return clip,target\n",
    "        imgs = []\n",
    "        masks = []\n",
    "        image = np.asarray(clip[0]).astype('float32')\n",
    "        height, width, depth = image.shape\n",
    "        ratio = rand.uniform(1, 4)\n",
    "        left = rand.uniform(0, width*ratio - width)\n",
    "        top = rand.uniform(0, height*ratio - height)\n",
    "        for i in range(len(clip)):\n",
    "            image = np.asarray(clip[i]).astype('float32')\n",
    "            expand_image = np.zeros((int(height*ratio), int(width*ratio), depth),dtype=image.dtype)\n",
    "            expand_image[:, :, :] = self.mean\n",
    "            expand_image[int(top):int(top + height),int(left):int(left + width)] = image\n",
    "            imgs.append(Image.fromarray(expand_image.astype('uint8')))\n",
    "            expand_mask = torch.zeros((int(height*ratio), int(width*ratio)),dtype=torch.uint8)\n",
    "            expand_mask[int(top):int(top + height),int(left):int(left + width)] = target['masks'][i]\n",
    "            masks.append(expand_mask)\n",
    "        boxes = target['boxes'].numpy()\n",
    "        boxes[:, :2] += (int(left), int(top))\n",
    "        boxes[:, 2:] += (int(left), int(top))\n",
    "        target['boxes'] = torch.tensor(boxes)\n",
    "        target['masks']=torch.stack(masks)\n",
    "        return imgs, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return hflip(img, target)\n",
    "        return img, target\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return vflip(img, target)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomResize(object):\n",
    "    def __init__(self, sizes, max_size=None):\n",
    "        assert isinstance(sizes, (list, tuple))\n",
    "        self.sizes = sizes\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img, target=None):\n",
    "        size = random.choice(self.sizes)\n",
    "        return resize(img, target, size, self.max_size)\n",
    "\n",
    "\n",
    "class RandomPad(object):\n",
    "    def __init__(self, max_pad):\n",
    "        self.max_pad = max_pad\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        pad_x = random.randint(0, self.max_pad)\n",
    "        pad_y = random.randint(0, self.max_pad)\n",
    "        return pad(img, target, (pad_x, pad_y))\n",
    "\n",
    "\n",
    "class RandomSelect(object):\n",
    "    \"\"\"\n",
    "    Randomly selects between transforms1 and transforms2,\n",
    "    with probability p for transforms1 and (1 - p) for transforms2\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms1, transforms2, p=0.5):\n",
    "        self.transforms1 = transforms1\n",
    "        self.transforms2 = transforms2\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return self.transforms1(img, target)\n",
    "        return self.transforms2(img, target)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, clip, target):\n",
    "        img = []\n",
    "        for im in clip:\n",
    "            img.append(Ft.to_tensor(im))\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomErasing(object):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.eraser(img), target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, clip, target=None):\n",
    "        image = []\n",
    "        for im in clip:\n",
    "            image.append(Ft.normalize(im, mean=self.mean, std=self.std))\n",
    "        if target is None:\n",
    "            return image, None\n",
    "        targets = target.copy()\n",
    "        for i, target in enumerate(targets):\n",
    "            h, w = image[i].shape[-2:]\n",
    "            if \"boxes\" in target:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes = box_xyxy_to_cxcywh(boxes)\n",
    "                boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "                target[\"boxes\"] = boxes\n",
    "        return image, targets\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += \"    {0}\".format(t)\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "class CocoDetection(TvCocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks, num_frames = 4,\n",
    "        is_train = True,  filter_key_img=True,  cache_mode=False, local_rank=0, local_size=1):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file,\n",
    "                                            cache_mode=cache_mode, local_rank=local_rank, local_size=local_size)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "        # self.prepare_seq = ConvertCocoSeqPolysToMask(return_masks)\n",
    "        self.ann_file = ann_file\n",
    "        self.frame_range = [-2, 2]\n",
    "        self.num_ref_frames = num_frames - 1\n",
    "        self.cocovid = CocoVID(self.ann_file)\n",
    "        self.is_train = is_train\n",
    "        self.filter_key_img = filter_key_img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        imgs = []\n",
    "        tgts = []\n",
    "\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        path = img_info['file_name']\n",
    "        video_id = img_info['video_id']\n",
    "        img = self.get_image(path)\n",
    "        target = {'image_id': img_id,'video_id': video_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        imgs.append(img)\n",
    "        tgts.append(target)\n",
    "        if video_id == -1:\n",
    "            for i in range(self.num_ref_frames):\n",
    "                imgs.append(copy.deepcopy(img))\n",
    "                tgts.append(copy.deepcopy(target))\n",
    "        else:\n",
    "            img_ids = self.cocovid.get_img_ids_from_vid(video_id) \n",
    "            #print(\"length\", len(img_ids))\n",
    "            ref_img_ids = []\n",
    "            if self.is_train:\n",
    "                interval = 5 # *20\n",
    "                left = max(img_ids[0], img_id - interval)\n",
    "                right = min(img_ids[-1], img_id + interval)\n",
    "                sample_range = list(range(left, right))\n",
    "                if self.filter_key_img and img_id in sample_range:\n",
    "                    sample_range.remove(img_id)\n",
    "                if self.num_ref_frames >= 10:\n",
    "                    sample_range = img_ids\n",
    "                while self.num_ref_frames > len(sample_range):\n",
    "                    sample_range.extend(sample_range)\n",
    "                ref_img_ids = random.sample(sample_range, self.num_ref_frames)\n",
    "\n",
    "            else:\n",
    "                #print(\"------------------------------\")i\n",
    "                ref_img_ids = []\n",
    "                Len = len(img_ids)\n",
    "                interval  = max(int(Len // 15), 1)  #\n",
    "                left_indexs = int((img_id - img_ids[0]) // interval)\n",
    "                right_indexs = int((img_ids[-1] - img_id) // interval)\n",
    "                if left_indexs < self.num_ref_frames:\n",
    "                   for i in range(self.num_ref_frames):\n",
    "                       ref_img_ids.append(min(img_id + (i+1)*interval, img_ids[-1]))\n",
    "                else:\n",
    "                   for i in range(self.num_ref_frames):\n",
    "                       ref_img_ids.append(max(img_id - (i+1)* interval, img_ids[0]))\n",
    "\n",
    "                # print(\"ref_img_ids\", ref_img_ids)\n",
    "            for ref_img_id in ref_img_ids:\n",
    "                ref_ann_ids = coco.getAnnIds(imgIds=ref_img_id)\n",
    "                ref_img_info = coco.loadImgs(ref_img_id)[0]\n",
    "                ref_img_path = ref_img_info['file_name']\n",
    "                ref_img = self.get_image(ref_img_path)\n",
    "                ref_target = coco.loadAnns(ref_ann_ids)\n",
    "                ref_target = {'image_id': ref_img_id, 'video_id': video_id, 'annotations': ref_target}\n",
    "                ref_img, ref_target = self.prepare(ref_img, ref_target)\n",
    "                imgs.append(ref_img)\n",
    "                tgts.append(ref_target)\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            imgs, target = self._transforms(imgs, tgts) \n",
    "\n",
    "        return  torch.cat(imgs, dim=0),  target\n",
    "\n",
    "\n",
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "\n",
    "class ConvertCocoPolysToMask(object):\n",
    "    def __init__(self, return_masks=False):\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        if self.return_masks:\n",
    "            masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        if self.return_masks:\n",
    "            target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "def make_coco_transforms(image_set):\n",
    "\n",
    "    normalize = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "    if image_set == 'train_vid' or image_set == \"train_det\" or image_set == \"train_joint\":\n",
    "        return Compose([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomResize([600], max_size=1000),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return Compose([\n",
    "            RandomResize([600], max_size=1000),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "def build_vid_multi(image_set, args):\n",
    "    root = Path(args[\"vid_path\"])\n",
    "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "    mode = 'instances'\n",
    "    PATHS = {\n",
    "        #\"train_det\": [(root / \"Data\" / \"DET\", root / \"annotations\" / 'imagenet_det_30plus1cls_vid_train.json')], #Thoses two files we don't have\n",
    "        \"train_vid\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_train.json')],\n",
    "        #\"train_joint\": [(root / \"Data\" , root / \"annotations\" / 'imagenet_vid_train_joint_30.json')],\n",
    "        \"val\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_val.json')],\n",
    "    }\n",
    "    datasets = []\n",
    "    for (img_folder, ann_file) in PATHS[image_set]:\n",
    "        dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), is_train =(not args[\"eval\"]), return_masks=args[\"masks\"], cache_mode=args[\"cache_mode\"], local_rank=0, local_size=1, num_frames=args[\"num_frames\"])\n",
    "        datasets.append(dataset)\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    return ConcatDataset(datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vid multi eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"eval\"]:\n",
    "    from pycocotools import mask as coco_mask\n",
    "    def ChooseFrame(List, Gap, num_frames):\n",
    "        ret = []\n",
    "        start_id = 0\n",
    "        max_gap = Gap*num_frames\n",
    "        num = len(List) // max_gap\n",
    "        for i in range(num):\n",
    "            start_id = i * max_gap\n",
    "            for j in range(Gap):\n",
    "                tmp = []\n",
    "                for k in range(num_frames):\n",
    "                    tmp.append(List[start_id + j + k * Gap])\n",
    "                ret.append(copy.deepcopy(tmp))\n",
    "\n",
    "        if num * max_gap == len(List):\n",
    "            return ret\n",
    "\n",
    "        new_list = List[num * max_gap:]\n",
    "        random.shuffle(new_list)\n",
    "        ret.extend(np.array_split(new_list, len(new_list) // num_frames))\n",
    "        return ret\n",
    "\n",
    "    class CocoDetection(TvCocoDetection):\n",
    "        def __init__(self, img_folder, ann_file, transforms, return_masks, num_frames= 4,\n",
    "            is_train = True,  filter_key_img=True,  cache_mode=False, local_rank=0, local_size=1, gap = 1, is_shuffle=True):\n",
    "            super(CocoDetection, self).__init__(img_folder, ann_file,\n",
    "                                                cache_mode=cache_mode, local_rank=local_rank, local_size=local_size)\n",
    "            self._transforms = transforms\n",
    "            self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "            self.ann_file = ann_file\n",
    "            self.frame_range = [-2, 2]\n",
    "            self.num_frames = num_frames\n",
    "            self.cocovid = CocoVID(self.ann_file)\n",
    "            self.vid_ids = self.cocovid.get_vid_ids()\n",
    "            self.img_ids = []\n",
    "            import numpy as np\n",
    "            import math\n",
    "            import copy\n",
    "            \n",
    "            for vid_id in self.vid_ids:\n",
    "                single_video_img_ids = self.cocovid.get_img_ids_from_vid(vid_id)\n",
    "                while len(single_video_img_ids) < num_frames:\n",
    "                    single_video_img_ids.extend(copy.deepcopy(single_video_img_ids))\n",
    "                nums = math.ceil(len(single_video_img_ids)* 1.0 / num_frames) # 4\n",
    "                offset = nums * num_frames - len(single_video_img_ids) # 1\n",
    "                if offset != 0 :\n",
    "                    single_video_img_ids.extend(copy.deepcopy(single_video_img_ids[-offset:]))\n",
    "                if is_shuffle:\n",
    "                    random.shuffle(single_video_img_ids) \n",
    "                self.img_ids.extend(ChooseFrame(single_video_img_ids, gap, num_frames))\n",
    "\n",
    "            self.is_train = is_train\n",
    "            self.filter_key_img = filter_key_img\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.img_ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                index (int): Index\n",
    "            Returns:\n",
    "                tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "            \"\"\"\n",
    "            imgs = [] \n",
    "            tgts = []\n",
    "\n",
    "            idxs = self.img_ids[idx]\n",
    "            for i in idxs:    \n",
    "                coco = self.coco\n",
    "                img_id = self.ids[i-1]\n",
    "                ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "                target = coco.loadAnns(ann_ids)\n",
    "                img_info = coco.loadImgs(img_id)[0]\n",
    "                path = img_info['file_name']\n",
    "                video_id = img_info['video_id']\n",
    "                img = self.get_image(path)\n",
    "                target = {'image_id': img_id, 'annotations': target, 'path': path}\n",
    "                img, target = self.prepare(img, target)\n",
    "                imgs.append(img)\n",
    "                tgts.append(target)\n",
    "\n",
    "            if self._transforms is not None:\n",
    "                imgs, tgts = self._transforms(imgs, tgts)\n",
    "\n",
    "            for target_item in tgts:\n",
    "                target_item['path'] = path\n",
    "            \n",
    "            return  torch.cat(imgs, dim=0),  tgts\n",
    "\n",
    "\n",
    "    def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "        masks = []\n",
    "        for polygons in segmentations:\n",
    "            rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "            mask = coco_mask.decode(rles)\n",
    "            if len(mask.shape) < 3:\n",
    "                mask = mask[..., None]\n",
    "            mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "            mask = mask.any(dim=2)\n",
    "            masks.append(mask)\n",
    "        if masks:\n",
    "            masks = torch.stack(masks, dim=0)\n",
    "        else:\n",
    "            masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "        return masks\n",
    "\n",
    "\n",
    "    class ConvertCocoPolysToMask(object):\n",
    "        def __init__(self, return_masks=False):\n",
    "            self.return_masks = return_masks\n",
    "\n",
    "        def __call__(self, image, target):\n",
    "            w, h = image.size\n",
    "\n",
    "            image_id = target[\"image_id\"]\n",
    "            image_id = torch.tensor([image_id])\n",
    "\n",
    "            anno = target[\"annotations\"]\n",
    "\n",
    "            anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "            boxes = [obj[\"bbox\"] for obj in anno]\n",
    "            # guard against no boxes via resizing\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "            boxes[:, 2:] += boxes[:, :2]\n",
    "            boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "            boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "            classes = [obj[\"category_id\"] for obj in anno]\n",
    "            classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "            if self.return_masks:\n",
    "                segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "                masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "            keypoints = None\n",
    "            if anno and \"keypoints\" in anno[0]:\n",
    "                keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "                keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "                num_keypoints = keypoints.shape[0]\n",
    "                if num_keypoints:\n",
    "                    keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "            keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "            boxes = boxes[keep]\n",
    "            classes = classes[keep]\n",
    "            if self.return_masks:\n",
    "                masks = masks[keep]\n",
    "            if keypoints is not None:\n",
    "                keypoints = keypoints[keep]\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = classes\n",
    "            if self.return_masks:\n",
    "                target[\"masks\"] = masks\n",
    "            target[\"image_id\"] = image_id\n",
    "            if keypoints is not None:\n",
    "                target[\"keypoints\"] = keypoints\n",
    "\n",
    "            # for conversion to coco api\n",
    "            area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "            iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "            target[\"area\"] = area[keep]\n",
    "            target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "            target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "            target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "            \n",
    "            return image, target\n",
    "\n",
    "\n",
    "    def make_coco_transforms(image_set):\n",
    "\n",
    "        normalize = Compose([\n",
    "            ToTensor(),\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "        if image_set == 'train_vid' or image_set == \"train_det\" or image_set == \"train_joint\":\n",
    "            return Compose([\n",
    "                RandomHorizontalFlip(),\n",
    "                RandomResize([600], max_size=1000),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "        if image_set == 'val':\n",
    "            return Compose([\n",
    "                RandomResize([600], max_size=1000),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "        raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "    def build_vid_multi_eval(image_set, args):\n",
    "        root = Path(args[\"vid_path\"])\n",
    "        assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "        mode = 'instances'\n",
    "        PATHS = {\n",
    "            \"train_det\": [(root / \"Data\" / \"DET\", root / \"annotations\" / 'imagenet_det_30plus1cls_vid_train.json')],\n",
    "            \"train_vid\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_train.json')], #The only one we will use\n",
    "            \"train_joint\": [(root / \"Data\" , root / \"annotations\" / 'imagenet_vid_train_joint_30.json')],\n",
    "            \"val\": [(root / \"Data\" / \"VID\", root / \"annotations\" / 'imagenet_vid_val.json')],\n",
    "        }\n",
    "        datasets = []\n",
    "        for (img_folder, ann_file) in PATHS[image_set]:\n",
    "            dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), is_train =(not args[\"eval\"]), \n",
    "                                    num_frames = args[\"num_frames\"], return_masks=args[\"masks\"], cache_mode=args[\"cache_mode\"], \n",
    "                                    local_rank=0, local_size=1, gap = args[\"gap\"], is_shuffle=args[\"is_shuffle\"])\n",
    "            datasets.append(dataset)\n",
    "        if len(datasets) == 1:\n",
    "            return datasets[0]\n",
    "        return ConcatDataset(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_api_from_dataset(dataset):\n",
    "    for _ in range(10):\n",
    "        # if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "        #     break\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            dataset = dataset.dataset\n",
    "    if isinstance(dataset, TvCocoDetection):\n",
    "        return dataset.coco\n",
    "\n",
    "\n",
    "def build_dataset(image_set, args):\n",
    "    if args[\"dataset_file\"] == \"vid_multi\":\n",
    "        return build_vid_multi(image_set, args)\n",
    "    if args[\"dataset_file\"] == \"vid_multi_eval\":\n",
    "        return build_vid_multi_eval(image_set, args)\n",
    "    raise ValueError(f'dataset {args.dataset_file} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main, Train the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions used for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not False: #is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "    \n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = 1\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    \n",
    "def get_total_grad_norm(parameters, norm_type=2):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    device = parameters[0].grad.device\n",
    "    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]),\n",
    "                            norm_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import sys\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    metric_logger.add_meter('grad_norm', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 100\n",
    "\n",
    "\n",
    "    # prefetcher = data_prefetcher(data_loader, device, prefetch=True)\n",
    "    # data_loader_iter = iter(data_loader)\n",
    "    # samples, targets = data_loader_iter.next()\n",
    "    # samples = samples.to(device)\n",
    "    # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "    # for _ in metric_logger.log_every(range(len(data_loader)), print_freq, header):\n",
    "        \n",
    "        # assert samples is None, samples\n",
    "        # outputs = model(samples)\n",
    "        samples = samples.to(device)\n",
    "        #print(\"engine_target_shape\",targets)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets[0]]\n",
    "        # print(\"targets\", targets)\n",
    "        # print(\"input model\", type(samples))\n",
    "        outputs = model(samples)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    " \n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict) #won't change anything on single gpu\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if max_norm > 0:\n",
    "            grad_total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        else:\n",
    "            grad_total_norm = get_total_grad_norm(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(grad_norm=grad_total_norm)\n",
    "\n",
    "        # samples, ref_samples, targets = prefetcher.next()\n",
    "        # try: \n",
    "        #     samples, targets = data_loader_iter.next()\n",
    "        # except StopIteration:\n",
    "        #     data_loader_iter = iter(data_loader)\n",
    "        #     samples,targets = data_loader_iter.next()\n",
    "        # samples = samples.to(device)\n",
    "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "import contextlib\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = 1\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "\n",
    "def merge(img_ids, eval_imgs):\n",
    "    all_img_ids = all_gather(img_ids)\n",
    "    all_eval_imgs = all_gather(eval_imgs)\n",
    "\n",
    "    merged_img_ids = []\n",
    "    for p in all_img_ids:\n",
    "        merged_img_ids.extend(p)\n",
    "\n",
    "    merged_eval_imgs = []\n",
    "    for p in all_eval_imgs:\n",
    "        merged_eval_imgs.append(p)\n",
    "\n",
    "    merged_img_ids = np.array(merged_img_ids)\n",
    "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
    "\n",
    "    # keep only unique (and in sorted order) images\n",
    "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
    "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
    "\n",
    "    return merged_img_ids, merged_eval_imgs\n",
    "\n",
    "\n",
    "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
    "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
    "    img_ids = list(img_ids)\n",
    "    eval_imgs = list(eval_imgs.flatten())\n",
    "\n",
    "    coco_eval.evalImgs = eval_imgs\n",
    "    coco_eval.params.imgIds = img_ids\n",
    "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# From pycocotools, just removed the prints and fixed\n",
    "# a Python3 bug about unicode not defined\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def evaluate_coco(self):\n",
    "    '''\n",
    "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
    "    :return: None\n",
    "    '''\n",
    "    # tic = time.time()\n",
    "    # print('Running per image evaluation...')\n",
    "    p = self.params\n",
    "    # add backward compatibility if useSegm is specified in params\n",
    "    if p.useSegm is not None:\n",
    "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
    "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
    "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
    "    p.imgIds = list(np.unique(p.imgIds))\n",
    "    if p.useCats:\n",
    "        p.catIds = list(np.unique(p.catIds))\n",
    "    p.maxDets = sorted(p.maxDets)\n",
    "    self.params = p\n",
    "\n",
    "    self._prepare()\n",
    "    # loop through images, area range, max detection number\n",
    "    catIds = p.catIds if p.useCats else [-1]\n",
    "\n",
    "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
    "        computeIoU = self.computeIoU\n",
    "    elif p.iouType == 'keypoints':\n",
    "        computeIoU = self.computeOks\n",
    "    self.ious = {\n",
    "        (imgId, catId): computeIoU(imgId, catId)\n",
    "        for imgId in p.imgIds\n",
    "        for catId in catIds}\n",
    "\n",
    "    evaluateImg = self.evaluateImg\n",
    "    maxDet = p.maxDets[-1]\n",
    "    evalImgs = [\n",
    "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
    "        for catId in catIds\n",
    "        for areaRng in p.areaRng\n",
    "        for imgId in p.imgIds\n",
    "    ]\n",
    "    # this is NOT in the pycocotools code, but could be done outside\n",
    "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
    "    self._paramsEval = copy.deepcopy(self.params)\n",
    "    # toc = time.time()\n",
    "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
    "    return p.imgIds, evalImgs\n",
    "\n",
    "class CocoEvaluator(object):\n",
    "    def __init__(self, coco_gt, iou_types):\n",
    "        assert isinstance(iou_types, (list, tuple))\n",
    "        coco_gt = copy.deepcopy(coco_gt)\n",
    "        self.coco_gt = coco_gt\n",
    "\n",
    "        self.iou_types = iou_types\n",
    "        self.coco_eval = {}\n",
    "        for iou_type in iou_types:\n",
    "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
    "\n",
    "        self.img_ids = []\n",
    "        self.eval_imgs = {k: [] for k in iou_types}\n",
    "\n",
    "    def update(self, predictions):\n",
    "        img_ids = list(np.unique(list(predictions.keys())))\n",
    "        self.img_ids.extend(img_ids)\n",
    "\n",
    "        for iou_type in self.iou_types:\n",
    "            results = self.prepare(predictions, iou_type)\n",
    "\n",
    "            # suppress pycocotools prints\n",
    "            with open(os.devnull, 'w') as devnull:\n",
    "                with contextlib.redirect_stdout(devnull):\n",
    "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
    "            coco_eval = self.coco_eval[iou_type]\n",
    "\n",
    "            coco_eval.cocoDt = coco_dt\n",
    "            coco_eval.params.imgIds = list(img_ids)\n",
    "            img_ids, eval_imgs = evaluate_coco(coco_eval)\n",
    "\n",
    "            self.eval_imgs[iou_type].append(eval_imgs)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
    "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
    "\n",
    "    def accumulate(self):\n",
    "        for coco_eval in self.coco_eval.values():\n",
    "            coco_eval.accumulate()\n",
    "\n",
    "    def summarize(self):\n",
    "        for iou_type, coco_eval in self.coco_eval.items():\n",
    "            print(\"IoU metric: {}\".format(iou_type))\n",
    "            coco_eval.summarize()\n",
    "\n",
    "    def prepare(self, predictions, iou_type):\n",
    "        if iou_type == \"bbox\":\n",
    "            return self.prepare_for_coco_detection(predictions)\n",
    "        elif iou_type == \"segm\":\n",
    "            return self.prepare_for_coco_segmentation(predictions)\n",
    "        elif iou_type == \"keypoints\":\n",
    "            return self.prepare_for_coco_keypoint(predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
    "\n",
    "    def prepare_for_coco_detection(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"bbox\": box,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, box in enumerate(boxes)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_segmentation(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            scores = prediction[\"scores\"]\n",
    "            labels = prediction[\"labels\"]\n",
    "            masks = prediction[\"masks\"]\n",
    "\n",
    "            masks = masks > 0.5\n",
    "\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            rles = [\n",
    "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
    "                for mask in masks\n",
    "            ]\n",
    "            for rle in rles:\n",
    "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"segmentation\": rle,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, rle in enumerate(rles)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_keypoint(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "            keypoints = prediction[\"keypoints\"]\n",
    "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        'keypoints': keypoint,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, keypoint in enumerate(keypoints)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Test:'\n",
    "\n",
    "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
    "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
    "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
    "\n",
    "    panoptic_evaluator = None\n",
    "   \n",
    "\n",
    "    for samples, targets  in metric_logger.log_every(data_loader, 50, header):\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items() if k!='path'} for t in targets[0]]\n",
    "\n",
    "        outputs = model(samples)\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                             **loss_dict_reduced_scaled,\n",
    "                             **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "\n",
    "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
    "        if 'segm' in postprocessors.keys():\n",
    "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
    "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
    "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
    "        if coco_evaluator is not None:\n",
    "            coco_evaluator.update(res)\n",
    "\n",
    "        if panoptic_evaluator is not None:\n",
    "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
    "            for i, target in enumerate(targets):\n",
    "                image_id = target[\"image_id\"].item()\n",
    "                file_name = f\"{image_id:012d}.png\"\n",
    "                res_pano[i][\"image_id\"] = image_id\n",
    "                res_pano[i][\"file_name\"] = file_name\n",
    "\n",
    "            panoptic_evaluator.update(res_pano)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "    if panoptic_evaluator is not None:\n",
    "        panoptic_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "    panoptic_res = None\n",
    "    if panoptic_evaluator is not None:\n",
    "        panoptic_res = panoptic_evaluator.summarize()\n",
    "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "    if coco_evaluator is not None:\n",
    "        if 'bbox' in postprocessors.keys():\n",
    "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
    "        if 'segm' in postprocessors.keys():\n",
    "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
    "    if panoptic_res is not None:\n",
    "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
    "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
    "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n",
    "    return stats, coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_master(*args, **kwargs):\n",
    "    torch.save(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0002, 'lr_backbone_names': ['backbone.0'], 'lr_backbone': 2e-05, 'lr_linear_proj_names': ['reference_points', 'sampling_offsets'], 'lr_linear_proj_mult': 0.1, 'batch_size': 1, 'weight_decay': 0.0001, 'epochs': 7, 'lr_drop': 5, 'lr_drop_epochs': [5, 6], 'clip_max_norm': 0.1, 'num_ref_frames': 3, 'num_frames': 1, 'sgd': False, 'gap': 2, 'with_box_refine': True, 'two_stage': False, 'frozen_weights': None, 'pretrained': None, 'backbone': 'swin_b_p4w7', 'dilation': True, 'position_embedding': 'sine', 'position_embedding_scale': 6.283185307179586, 'num_feature_levels': 1, 'checkpoint': False, 'enc_layers': 6, 'dec_layers': 6, 'dim_feedforward': 1024, 'hidden_dim': 256, 'dropout': 0.1, 'nheads': 8, 'num_queries': 100, 'dec_n_points': 4, 'enc_n_points': 4, 'n_temporal_decoder_layers': 1, 'interval1': 20, 'interval2': 60, 'fixed_pretrained_model': False, 'is_shuffle': False, 'masks': False, 'aux_loss': False, 'set_cost_class': 2, 'set_cost_bbox': 5, 'set_cost_giou': 2, 'mask_loss_coef': 1, 'dice_loss_coef': 1, 'cls_loss_coef': 2, 'bbox_loss_coef': 5, 'giou_loss_coef': 2, 'focal_alpha': 0.25, 'dataset_file': 'vid_multi', 'coco_path': './data/coco', 'vid_path': './data/vid', 'coco_pretrain': False, 'coco_panoptic_path': '', 'remove_difficult': False, 'output_dir': 'Final_output', 'device': 'cuda', 'seed': 42, 'resume': './exps/exps_single/swinb_88.3/checkpoint0006.pth', 'start_epoch': 0, 'eval': True, 'num_workers': 0, 'cache_mode': False}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vid_multi 11111111\n",
      "vid_multi\n",
      "self.num_layers 4\n",
      "number of params: 106274744\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "transformer.level_embed\n",
      "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.0.norm1.weight\n",
      "transformer.encoder.layers.0.norm1.bias\n",
      "transformer.encoder.layers.0.linear1.weight\n",
      "transformer.encoder.layers.0.linear1.bias\n",
      "transformer.encoder.layers.0.linear2.weight\n",
      "transformer.encoder.layers.0.linear2.bias\n",
      "transformer.encoder.layers.0.norm2.weight\n",
      "transformer.encoder.layers.0.norm2.bias\n",
      "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.1.norm1.weight\n",
      "transformer.encoder.layers.1.norm1.bias\n",
      "transformer.encoder.layers.1.linear1.weight\n",
      "transformer.encoder.layers.1.linear1.bias\n",
      "transformer.encoder.layers.1.linear2.weight\n",
      "transformer.encoder.layers.1.linear2.bias\n",
      "transformer.encoder.layers.1.norm2.weight\n",
      "transformer.encoder.layers.1.norm2.bias\n",
      "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.2.norm1.weight\n",
      "transformer.encoder.layers.2.norm1.bias\n",
      "transformer.encoder.layers.2.linear1.weight\n",
      "transformer.encoder.layers.2.linear1.bias\n",
      "transformer.encoder.layers.2.linear2.weight\n",
      "transformer.encoder.layers.2.linear2.bias\n",
      "transformer.encoder.layers.2.norm2.weight\n",
      "transformer.encoder.layers.2.norm2.bias\n",
      "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.3.norm1.weight\n",
      "transformer.encoder.layers.3.norm1.bias\n",
      "transformer.encoder.layers.3.linear1.weight\n",
      "transformer.encoder.layers.3.linear1.bias\n",
      "transformer.encoder.layers.3.linear2.weight\n",
      "transformer.encoder.layers.3.linear2.bias\n",
      "transformer.encoder.layers.3.norm2.weight\n",
      "transformer.encoder.layers.3.norm2.bias\n",
      "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.4.norm1.weight\n",
      "transformer.encoder.layers.4.norm1.bias\n",
      "transformer.encoder.layers.4.linear1.weight\n",
      "transformer.encoder.layers.4.linear1.bias\n",
      "transformer.encoder.layers.4.linear2.weight\n",
      "transformer.encoder.layers.4.linear2.bias\n",
      "transformer.encoder.layers.4.norm2.weight\n",
      "transformer.encoder.layers.4.norm2.bias\n",
      "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.5.norm1.weight\n",
      "transformer.encoder.layers.5.norm1.bias\n",
      "transformer.encoder.layers.5.linear1.weight\n",
      "transformer.encoder.layers.5.linear1.bias\n",
      "transformer.encoder.layers.5.linear2.weight\n",
      "transformer.encoder.layers.5.linear2.bias\n",
      "transformer.encoder.layers.5.norm2.weight\n",
      "transformer.encoder.layers.5.norm2.bias\n",
      "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.0.norm1.weight\n",
      "transformer.decoder.layers.0.norm1.bias\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.0.norm2.weight\n",
      "transformer.decoder.layers.0.norm2.bias\n",
      "transformer.decoder.layers.0.linear1.weight\n",
      "transformer.decoder.layers.0.linear1.bias\n",
      "transformer.decoder.layers.0.linear2.weight\n",
      "transformer.decoder.layers.0.linear2.bias\n",
      "transformer.decoder.layers.0.norm3.weight\n",
      "transformer.decoder.layers.0.norm3.bias\n",
      "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.1.norm1.weight\n",
      "transformer.decoder.layers.1.norm1.bias\n",
      "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.1.norm2.weight\n",
      "transformer.decoder.layers.1.norm2.bias\n",
      "transformer.decoder.layers.1.linear1.weight\n",
      "transformer.decoder.layers.1.linear1.bias\n",
      "transformer.decoder.layers.1.linear2.weight\n",
      "transformer.decoder.layers.1.linear2.bias\n",
      "transformer.decoder.layers.1.norm3.weight\n",
      "transformer.decoder.layers.1.norm3.bias\n",
      "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.2.norm1.weight\n",
      "transformer.decoder.layers.2.norm1.bias\n",
      "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.2.norm2.weight\n",
      "transformer.decoder.layers.2.norm2.bias\n",
      "transformer.decoder.layers.2.linear1.weight\n",
      "transformer.decoder.layers.2.linear1.bias\n",
      "transformer.decoder.layers.2.linear2.weight\n",
      "transformer.decoder.layers.2.linear2.bias\n",
      "transformer.decoder.layers.2.norm3.weight\n",
      "transformer.decoder.layers.2.norm3.bias\n",
      "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.3.norm1.weight\n",
      "transformer.decoder.layers.3.norm1.bias\n",
      "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.3.norm2.weight\n",
      "transformer.decoder.layers.3.norm2.bias\n",
      "transformer.decoder.layers.3.linear1.weight\n",
      "transformer.decoder.layers.3.linear1.bias\n",
      "transformer.decoder.layers.3.linear2.weight\n",
      "transformer.decoder.layers.3.linear2.bias\n",
      "transformer.decoder.layers.3.norm3.weight\n",
      "transformer.decoder.layers.3.norm3.bias\n",
      "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.4.norm1.weight\n",
      "transformer.decoder.layers.4.norm1.bias\n",
      "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.4.norm2.weight\n",
      "transformer.decoder.layers.4.norm2.bias\n",
      "transformer.decoder.layers.4.linear1.weight\n",
      "transformer.decoder.layers.4.linear1.bias\n",
      "transformer.decoder.layers.4.linear2.weight\n",
      "transformer.decoder.layers.4.linear2.bias\n",
      "transformer.decoder.layers.4.norm3.weight\n",
      "transformer.decoder.layers.4.norm3.bias\n",
      "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.5.norm1.weight\n",
      "transformer.decoder.layers.5.norm1.bias\n",
      "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.5.norm2.weight\n",
      "transformer.decoder.layers.5.norm2.bias\n",
      "transformer.decoder.layers.5.linear1.weight\n",
      "transformer.decoder.layers.5.linear1.bias\n",
      "transformer.decoder.layers.5.linear2.weight\n",
      "transformer.decoder.layers.5.linear2.bias\n",
      "transformer.decoder.layers.5.norm3.weight\n",
      "transformer.decoder.layers.5.norm3.bias\n",
      "transformer.decoder.bbox_embed.0.layers.0.weight\n",
      "transformer.decoder.bbox_embed.0.layers.0.bias\n",
      "transformer.decoder.bbox_embed.0.layers.1.weight\n",
      "transformer.decoder.bbox_embed.0.layers.1.bias\n",
      "transformer.decoder.bbox_embed.0.layers.2.weight\n",
      "transformer.decoder.bbox_embed.0.layers.2.bias\n",
      "transformer.decoder.bbox_embed.1.layers.0.weight\n",
      "transformer.decoder.bbox_embed.1.layers.0.bias\n",
      "transformer.decoder.bbox_embed.1.layers.1.weight\n",
      "transformer.decoder.bbox_embed.1.layers.1.bias\n",
      "transformer.decoder.bbox_embed.1.layers.2.weight\n",
      "transformer.decoder.bbox_embed.1.layers.2.bias\n",
      "transformer.decoder.bbox_embed.2.layers.0.weight\n",
      "transformer.decoder.bbox_embed.2.layers.0.bias\n",
      "transformer.decoder.bbox_embed.2.layers.1.weight\n",
      "transformer.decoder.bbox_embed.2.layers.1.bias\n",
      "transformer.decoder.bbox_embed.2.layers.2.weight\n",
      "transformer.decoder.bbox_embed.2.layers.2.bias\n",
      "transformer.decoder.bbox_embed.3.layers.0.weight\n",
      "transformer.decoder.bbox_embed.3.layers.0.bias\n",
      "transformer.decoder.bbox_embed.3.layers.1.weight\n",
      "transformer.decoder.bbox_embed.3.layers.1.bias\n",
      "transformer.decoder.bbox_embed.3.layers.2.weight\n",
      "transformer.decoder.bbox_embed.3.layers.2.bias\n",
      "transformer.decoder.bbox_embed.4.layers.0.weight\n",
      "transformer.decoder.bbox_embed.4.layers.0.bias\n",
      "transformer.decoder.bbox_embed.4.layers.1.weight\n",
      "transformer.decoder.bbox_embed.4.layers.1.bias\n",
      "transformer.decoder.bbox_embed.4.layers.2.weight\n",
      "transformer.decoder.bbox_embed.4.layers.2.bias\n",
      "transformer.decoder.bbox_embed.5.layers.0.weight\n",
      "transformer.decoder.bbox_embed.5.layers.0.bias\n",
      "transformer.decoder.bbox_embed.5.layers.1.weight\n",
      "transformer.decoder.bbox_embed.5.layers.1.bias\n",
      "transformer.decoder.bbox_embed.5.layers.2.weight\n",
      "transformer.decoder.bbox_embed.5.layers.2.bias\n",
      "transformer.temporal_query_layer1.self_attn.in_proj_weight\n",
      "transformer.temporal_query_layer1.self_attn.in_proj_bias\n",
      "transformer.temporal_query_layer1.self_attn.out_proj.weight\n",
      "transformer.temporal_query_layer1.self_attn.out_proj.bias\n",
      "transformer.temporal_query_layer1.norm2.weight\n",
      "transformer.temporal_query_layer1.norm2.bias\n",
      "transformer.temporal_query_layer1.cross_attn.in_proj_weight\n",
      "transformer.temporal_query_layer1.cross_attn.in_proj_bias\n",
      "transformer.temporal_query_layer1.cross_attn.out_proj.weight\n",
      "transformer.temporal_query_layer1.cross_attn.out_proj.bias\n",
      "transformer.temporal_query_layer1.norm1.weight\n",
      "transformer.temporal_query_layer1.norm1.bias\n",
      "transformer.temporal_query_layer1.linear1.weight\n",
      "transformer.temporal_query_layer1.linear1.bias\n",
      "transformer.temporal_query_layer1.linear2.weight\n",
      "transformer.temporal_query_layer1.linear2.bias\n",
      "transformer.temporal_query_layer1.norm3.weight\n",
      "transformer.temporal_query_layer1.norm3.bias\n",
      "transformer.temporal_query_layer2.self_attn.in_proj_weight\n",
      "transformer.temporal_query_layer2.self_attn.in_proj_bias\n",
      "transformer.temporal_query_layer2.self_attn.out_proj.weight\n",
      "transformer.temporal_query_layer2.self_attn.out_proj.bias\n",
      "transformer.temporal_query_layer2.norm2.weight\n",
      "transformer.temporal_query_layer2.norm2.bias\n",
      "transformer.temporal_query_layer2.cross_attn.in_proj_weight\n",
      "transformer.temporal_query_layer2.cross_attn.in_proj_bias\n",
      "transformer.temporal_query_layer2.cross_attn.out_proj.weight\n",
      "transformer.temporal_query_layer2.cross_attn.out_proj.bias\n",
      "transformer.temporal_query_layer2.norm1.weight\n",
      "transformer.temporal_query_layer2.norm1.bias\n",
      "transformer.temporal_query_layer2.linear1.weight\n",
      "transformer.temporal_query_layer2.linear1.bias\n",
      "transformer.temporal_query_layer2.linear2.weight\n",
      "transformer.temporal_query_layer2.linear2.bias\n",
      "transformer.temporal_query_layer2.norm3.weight\n",
      "transformer.temporal_query_layer2.norm3.bias\n",
      "transformer.temporal_query_layer3.self_attn.in_proj_weight\n",
      "transformer.temporal_query_layer3.self_attn.in_proj_bias\n",
      "transformer.temporal_query_layer3.self_attn.out_proj.weight\n",
      "transformer.temporal_query_layer3.self_attn.out_proj.bias\n",
      "transformer.temporal_query_layer3.norm2.weight\n",
      "transformer.temporal_query_layer3.norm2.bias\n",
      "transformer.temporal_query_layer3.cross_attn.in_proj_weight\n",
      "transformer.temporal_query_layer3.cross_attn.in_proj_bias\n",
      "transformer.temporal_query_layer3.cross_attn.out_proj.weight\n",
      "transformer.temporal_query_layer3.cross_attn.out_proj.bias\n",
      "transformer.temporal_query_layer3.norm1.weight\n",
      "transformer.temporal_query_layer3.norm1.bias\n",
      "transformer.temporal_query_layer3.linear1.weight\n",
      "transformer.temporal_query_layer3.linear1.bias\n",
      "transformer.temporal_query_layer3.linear2.weight\n",
      "transformer.temporal_query_layer3.linear2.bias\n",
      "transformer.temporal_query_layer3.norm3.weight\n",
      "transformer.temporal_query_layer3.norm3.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.value_proj.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.value_proj.bias\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.output_proj.weight\n",
      "transformer.temporal_decoder1.layers.0.cross_attn.output_proj.bias\n",
      "transformer.temporal_decoder1.layers.0.norm1.weight\n",
      "transformer.temporal_decoder1.layers.0.norm1.bias\n",
      "transformer.temporal_decoder1.layers.0.self_attn.in_proj_weight\n",
      "transformer.temporal_decoder1.layers.0.self_attn.in_proj_bias\n",
      "transformer.temporal_decoder1.layers.0.self_attn.out_proj.weight\n",
      "transformer.temporal_decoder1.layers.0.self_attn.out_proj.bias\n",
      "transformer.temporal_decoder1.layers.0.norm2.weight\n",
      "transformer.temporal_decoder1.layers.0.norm2.bias\n",
      "transformer.temporal_decoder1.layers.0.linear1.weight\n",
      "transformer.temporal_decoder1.layers.0.linear1.bias\n",
      "transformer.temporal_decoder1.layers.0.linear2.weight\n",
      "transformer.temporal_decoder1.layers.0.linear2.bias\n",
      "transformer.temporal_decoder1.layers.0.norm3.weight\n",
      "transformer.temporal_decoder1.layers.0.norm3.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.value_proj.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.value_proj.bias\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.output_proj.weight\n",
      "transformer.temporal_decoder2.layers.0.cross_attn.output_proj.bias\n",
      "transformer.temporal_decoder2.layers.0.norm1.weight\n",
      "transformer.temporal_decoder2.layers.0.norm1.bias\n",
      "transformer.temporal_decoder2.layers.0.self_attn.in_proj_weight\n",
      "transformer.temporal_decoder2.layers.0.self_attn.in_proj_bias\n",
      "transformer.temporal_decoder2.layers.0.self_attn.out_proj.weight\n",
      "transformer.temporal_decoder2.layers.0.self_attn.out_proj.bias\n",
      "transformer.temporal_decoder2.layers.0.norm2.weight\n",
      "transformer.temporal_decoder2.layers.0.norm2.bias\n",
      "transformer.temporal_decoder2.layers.0.linear1.weight\n",
      "transformer.temporal_decoder2.layers.0.linear1.bias\n",
      "transformer.temporal_decoder2.layers.0.linear2.weight\n",
      "transformer.temporal_decoder2.layers.0.linear2.bias\n",
      "transformer.temporal_decoder2.layers.0.norm3.weight\n",
      "transformer.temporal_decoder2.layers.0.norm3.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.value_proj.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.value_proj.bias\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.output_proj.weight\n",
      "transformer.temporal_decoder3.layers.0.cross_attn.output_proj.bias\n",
      "transformer.temporal_decoder3.layers.0.norm1.weight\n",
      "transformer.temporal_decoder3.layers.0.norm1.bias\n",
      "transformer.temporal_decoder3.layers.0.self_attn.in_proj_weight\n",
      "transformer.temporal_decoder3.layers.0.self_attn.in_proj_bias\n",
      "transformer.temporal_decoder3.layers.0.self_attn.out_proj.weight\n",
      "transformer.temporal_decoder3.layers.0.self_attn.out_proj.bias\n",
      "transformer.temporal_decoder3.layers.0.norm2.weight\n",
      "transformer.temporal_decoder3.layers.0.norm2.bias\n",
      "transformer.temporal_decoder3.layers.0.linear1.weight\n",
      "transformer.temporal_decoder3.layers.0.linear1.bias\n",
      "transformer.temporal_decoder3.layers.0.linear2.weight\n",
      "transformer.temporal_decoder3.layers.0.linear2.bias\n",
      "transformer.temporal_decoder3.layers.0.norm3.weight\n",
      "transformer.temporal_decoder3.layers.0.norm3.bias\n",
      "transformer.reference_points.weight\n",
      "transformer.reference_points.bias\n",
      "class_embed.0.weight\n",
      "class_embed.0.bias\n",
      "class_embed.1.weight\n",
      "class_embed.1.bias\n",
      "class_embed.2.weight\n",
      "class_embed.2.bias\n",
      "class_embed.3.weight\n",
      "class_embed.3.bias\n",
      "class_embed.4.weight\n",
      "class_embed.4.bias\n",
      "class_embed.5.weight\n",
      "class_embed.5.bias\n",
      "temp_class_embed.weight\n",
      "temp_class_embed.bias\n",
      "temp_bbox_embed.layers.0.weight\n",
      "temp_bbox_embed.layers.0.bias\n",
      "temp_bbox_embed.layers.1.weight\n",
      "temp_bbox_embed.layers.1.bias\n",
      "temp_bbox_embed.layers.2.weight\n",
      "temp_bbox_embed.layers.2.bias\n",
      "query_embed.weight\n",
      "input_proj.0.0.weight\n",
      "input_proj.0.0.bias\n",
      "input_proj.0.1.weight\n",
      "input_proj.0.1.bias\n",
      "backbone.0.body.fpn.inner_blocks.0.0.weight\n",
      "backbone.0.body.fpn.inner_blocks.0.0.bias\n",
      "backbone.0.body.fpn.inner_blocks.1.0.weight\n",
      "backbone.0.body.fpn.inner_blocks.1.0.bias\n",
      "backbone.0.body.fpn.inner_blocks.2.0.weight\n",
      "backbone.0.body.fpn.inner_blocks.2.0.bias\n",
      "backbone.0.body.fpn.layer_blocks.0.0.weight\n",
      "backbone.0.body.fpn.layer_blocks.0.0.bias\n",
      "backbone.0.body.fpn.layer_blocks.1.0.weight\n",
      "backbone.0.body.fpn.layer_blocks.1.0.bias\n",
      "backbone.0.body.fpn.layer_blocks.2.0.weight\n",
      "backbone.0.body.fpn.layer_blocks.2.0.bias\n",
      "backbone.0.body.patch_embed.proj.weight\n",
      "backbone.0.body.patch_embed.proj.bias\n",
      "backbone.0.body.patch_embed.norm.weight\n",
      "backbone.0.body.patch_embed.norm.bias\n",
      "backbone.0.body.layers.0.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.0.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.0.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.0.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.0.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.0.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.0.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.0.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.0.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.0.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.0.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.0.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.0.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.0.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.0.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.0.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.0.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.0.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.0.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.0.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.layers.0.downsample.reduction.weight\n",
      "backbone.0.body.layers.0.downsample.norm.weight\n",
      "backbone.0.body.layers.0.downsample.norm.bias\n",
      "backbone.0.body.layers.1.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.1.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.1.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.1.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.1.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.1.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.1.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.1.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.1.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.1.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.1.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.1.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.1.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.1.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.1.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.1.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.1.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.1.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.1.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.1.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.layers.1.downsample.reduction.weight\n",
      "backbone.0.body.layers.1.downsample.norm.weight\n",
      "backbone.0.body.layers.1.downsample.norm.bias\n",
      "backbone.0.body.layers.2.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.2.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.2.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.2.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.2.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.2.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.2.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.2.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.2.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.2.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.2.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.3.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.3.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.3.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.3.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.3.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.3.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.3.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.3.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.3.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.3.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.4.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.4.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.4.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.4.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.4.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.4.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.4.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.4.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.4.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.4.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.5.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.5.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.5.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.5.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.5.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.5.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.5.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.5.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.5.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.5.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.6.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.6.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.6.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.6.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.6.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.6.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.6.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.6.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.6.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.6.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.7.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.7.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.7.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.7.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.7.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.7.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.7.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.7.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.7.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.7.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.8.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.8.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.8.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.8.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.8.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.8.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.8.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.8.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.8.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.8.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.9.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.9.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.9.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.9.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.9.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.9.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.9.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.9.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.9.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.9.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.10.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.10.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.10.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.10.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.10.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.10.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.10.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.10.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.10.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.10.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.11.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.11.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.11.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.11.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.11.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.11.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.11.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.11.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.11.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.11.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.12.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.12.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.12.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.12.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.12.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.12.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.12.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.12.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.12.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.12.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.13.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.13.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.13.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.13.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.13.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.13.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.13.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.13.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.13.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.13.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.14.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.14.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.14.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.14.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.14.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.14.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.14.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.14.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.14.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.14.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.15.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.15.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.15.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.15.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.15.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.15.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.15.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.15.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.15.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.15.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.16.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.16.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.16.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.16.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.16.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.16.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.16.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.16.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.16.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.16.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.blocks.17.norm1.weight\n",
      "backbone.0.body.layers.2.blocks.17.norm1.bias\n",
      "backbone.0.body.layers.2.blocks.17.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.2.blocks.17.attn.qkv.weight\n",
      "backbone.0.body.layers.2.blocks.17.attn.qkv.bias\n",
      "backbone.0.body.layers.2.blocks.17.attn.proj.weight\n",
      "backbone.0.body.layers.2.blocks.17.attn.proj.bias\n",
      "backbone.0.body.layers.2.blocks.17.norm2.weight\n",
      "backbone.0.body.layers.2.blocks.17.norm2.bias\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc1.weight\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc1.bias\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc2.weight\n",
      "backbone.0.body.layers.2.blocks.17.mlp.fc2.bias\n",
      "backbone.0.body.layers.2.downsample.reduction.weight\n",
      "backbone.0.body.layers.2.downsample.norm.weight\n",
      "backbone.0.body.layers.2.downsample.norm.bias\n",
      "backbone.0.body.layers.3.blocks.0.norm1.weight\n",
      "backbone.0.body.layers.3.blocks.0.norm1.bias\n",
      "backbone.0.body.layers.3.blocks.0.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.3.blocks.0.attn.qkv.weight\n",
      "backbone.0.body.layers.3.blocks.0.attn.qkv.bias\n",
      "backbone.0.body.layers.3.blocks.0.attn.proj.weight\n",
      "backbone.0.body.layers.3.blocks.0.attn.proj.bias\n",
      "backbone.0.body.layers.3.blocks.0.norm2.weight\n",
      "backbone.0.body.layers.3.blocks.0.norm2.bias\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc1.weight\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc1.bias\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc2.weight\n",
      "backbone.0.body.layers.3.blocks.0.mlp.fc2.bias\n",
      "backbone.0.body.layers.3.blocks.1.norm1.weight\n",
      "backbone.0.body.layers.3.blocks.1.norm1.bias\n",
      "backbone.0.body.layers.3.blocks.1.attn.relative_position_bias_table\n",
      "backbone.0.body.layers.3.blocks.1.attn.qkv.weight\n",
      "backbone.0.body.layers.3.blocks.1.attn.qkv.bias\n",
      "backbone.0.body.layers.3.blocks.1.attn.proj.weight\n",
      "backbone.0.body.layers.3.blocks.1.attn.proj.bias\n",
      "backbone.0.body.layers.3.blocks.1.norm2.weight\n",
      "backbone.0.body.layers.3.blocks.1.norm2.bias\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc1.weight\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc1.bias\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc2.weight\n",
      "backbone.0.body.layers.3.blocks.1.mlp.fc2.bias\n",
      "backbone.0.body.norm1.weight\n",
      "backbone.0.body.norm1.bias\n",
      "backbone.0.body.norm2.weight\n",
      "backbone.0.body.norm2.bias\n",
      "backbone.0.body.norm3.weight\n",
      "backbone.0.body.norm3.bias\n",
      "temp_class_embed_list.0.weight\n",
      "temp_class_embed_list.0.bias\n",
      "temp_class_embed_list.1.weight\n",
      "temp_class_embed_list.1.bias\n",
      "temp_class_embed_list.2.weight\n",
      "temp_class_embed_list.2.bias\n",
      "temp_bbox_embed_list.0.layers.0.weight\n",
      "temp_bbox_embed_list.0.layers.0.bias\n",
      "temp_bbox_embed_list.0.layers.1.weight\n",
      "temp_bbox_embed_list.0.layers.1.bias\n",
      "temp_bbox_embed_list.0.layers.2.weight\n",
      "temp_bbox_embed_list.0.layers.2.bias\n",
      "temp_bbox_embed_list.1.layers.0.weight\n",
      "temp_bbox_embed_list.1.layers.0.bias\n",
      "temp_bbox_embed_list.1.layers.1.weight\n",
      "temp_bbox_embed_list.1.layers.1.bias\n",
      "temp_bbox_embed_list.1.layers.2.weight\n",
      "temp_bbox_embed_list.1.layers.2.bias\n",
      "temp_bbox_embed_list.2.layers.0.weight\n",
      "temp_bbox_embed_list.2.layers.0.bias\n",
      "temp_bbox_embed_list.2.layers.1.weight\n",
      "temp_bbox_embed_list.2.layers.1.bias\n",
      "temp_bbox_embed_list.2.layers.2.weight\n",
      "temp_bbox_embed_list.2.layers.2.bias\n",
      "[5, 6]\n",
      "Missing Keys: ['transformer.temporal_query_layer1.self_attn.in_proj_weight', 'transformer.temporal_query_layer1.self_attn.in_proj_bias', 'transformer.temporal_query_layer1.self_attn.out_proj.weight', 'transformer.temporal_query_layer1.self_attn.out_proj.bias', 'transformer.temporal_query_layer1.norm2.weight', 'transformer.temporal_query_layer1.norm2.bias', 'transformer.temporal_query_layer1.cross_attn.in_proj_weight', 'transformer.temporal_query_layer1.cross_attn.in_proj_bias', 'transformer.temporal_query_layer1.cross_attn.out_proj.weight', 'transformer.temporal_query_layer1.cross_attn.out_proj.bias', 'transformer.temporal_query_layer1.norm1.weight', 'transformer.temporal_query_layer1.norm1.bias', 'transformer.temporal_query_layer1.linear1.weight', 'transformer.temporal_query_layer1.linear1.bias', 'transformer.temporal_query_layer1.linear2.weight', 'transformer.temporal_query_layer1.linear2.bias', 'transformer.temporal_query_layer1.norm3.weight', 'transformer.temporal_query_layer1.norm3.bias', 'transformer.temporal_query_layer2.self_attn.in_proj_weight', 'transformer.temporal_query_layer2.self_attn.in_proj_bias', 'transformer.temporal_query_layer2.self_attn.out_proj.weight', 'transformer.temporal_query_layer2.self_attn.out_proj.bias', 'transformer.temporal_query_layer2.norm2.weight', 'transformer.temporal_query_layer2.norm2.bias', 'transformer.temporal_query_layer2.cross_attn.in_proj_weight', 'transformer.temporal_query_layer2.cross_attn.in_proj_bias', 'transformer.temporal_query_layer2.cross_attn.out_proj.weight', 'transformer.temporal_query_layer2.cross_attn.out_proj.bias', 'transformer.temporal_query_layer2.norm1.weight', 'transformer.temporal_query_layer2.norm1.bias', 'transformer.temporal_query_layer2.linear1.weight', 'transformer.temporal_query_layer2.linear1.bias', 'transformer.temporal_query_layer2.linear2.weight', 'transformer.temporal_query_layer2.linear2.bias', 'transformer.temporal_query_layer2.norm3.weight', 'transformer.temporal_query_layer2.norm3.bias', 'transformer.temporal_query_layer3.self_attn.in_proj_weight', 'transformer.temporal_query_layer3.self_attn.in_proj_bias', 'transformer.temporal_query_layer3.self_attn.out_proj.weight', 'transformer.temporal_query_layer3.self_attn.out_proj.bias', 'transformer.temporal_query_layer3.norm2.weight', 'transformer.temporal_query_layer3.norm2.bias', 'transformer.temporal_query_layer3.cross_attn.in_proj_weight', 'transformer.temporal_query_layer3.cross_attn.in_proj_bias', 'transformer.temporal_query_layer3.cross_attn.out_proj.weight', 'transformer.temporal_query_layer3.cross_attn.out_proj.bias', 'transformer.temporal_query_layer3.norm1.weight', 'transformer.temporal_query_layer3.norm1.bias', 'transformer.temporal_query_layer3.linear1.weight', 'transformer.temporal_query_layer3.linear1.bias', 'transformer.temporal_query_layer3.linear2.weight', 'transformer.temporal_query_layer3.linear2.bias', 'transformer.temporal_query_layer3.norm3.weight', 'transformer.temporal_query_layer3.norm3.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.sampling_offsets.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.attention_weights.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.value_proj.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.value_proj.bias', 'transformer.temporal_decoder1.layers.0.cross_attn.output_proj.weight', 'transformer.temporal_decoder1.layers.0.cross_attn.output_proj.bias', 'transformer.temporal_decoder1.layers.0.norm1.weight', 'transformer.temporal_decoder1.layers.0.norm1.bias', 'transformer.temporal_decoder1.layers.0.self_attn.in_proj_weight', 'transformer.temporal_decoder1.layers.0.self_attn.in_proj_bias', 'transformer.temporal_decoder1.layers.0.self_attn.out_proj.weight', 'transformer.temporal_decoder1.layers.0.self_attn.out_proj.bias', 'transformer.temporal_decoder1.layers.0.norm2.weight', 'transformer.temporal_decoder1.layers.0.norm2.bias', 'transformer.temporal_decoder1.layers.0.linear1.weight', 'transformer.temporal_decoder1.layers.0.linear1.bias', 'transformer.temporal_decoder1.layers.0.linear2.weight', 'transformer.temporal_decoder1.layers.0.linear2.bias', 'transformer.temporal_decoder1.layers.0.norm3.weight', 'transformer.temporal_decoder1.layers.0.norm3.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.sampling_offsets.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.attention_weights.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.value_proj.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.value_proj.bias', 'transformer.temporal_decoder2.layers.0.cross_attn.output_proj.weight', 'transformer.temporal_decoder2.layers.0.cross_attn.output_proj.bias', 'transformer.temporal_decoder2.layers.0.norm1.weight', 'transformer.temporal_decoder2.layers.0.norm1.bias', 'transformer.temporal_decoder2.layers.0.self_attn.in_proj_weight', 'transformer.temporal_decoder2.layers.0.self_attn.in_proj_bias', 'transformer.temporal_decoder2.layers.0.self_attn.out_proj.weight', 'transformer.temporal_decoder2.layers.0.self_attn.out_proj.bias', 'transformer.temporal_decoder2.layers.0.norm2.weight', 'transformer.temporal_decoder2.layers.0.norm2.bias', 'transformer.temporal_decoder2.layers.0.linear1.weight', 'transformer.temporal_decoder2.layers.0.linear1.bias', 'transformer.temporal_decoder2.layers.0.linear2.weight', 'transformer.temporal_decoder2.layers.0.linear2.bias', 'transformer.temporal_decoder2.layers.0.norm3.weight', 'transformer.temporal_decoder2.layers.0.norm3.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.sampling_offsets.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.attention_weights.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.value_proj.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.value_proj.bias', 'transformer.temporal_decoder3.layers.0.cross_attn.output_proj.weight', 'transformer.temporal_decoder3.layers.0.cross_attn.output_proj.bias', 'transformer.temporal_decoder3.layers.0.norm1.weight', 'transformer.temporal_decoder3.layers.0.norm1.bias', 'transformer.temporal_decoder3.layers.0.self_attn.in_proj_weight', 'transformer.temporal_decoder3.layers.0.self_attn.in_proj_bias', 'transformer.temporal_decoder3.layers.0.self_attn.out_proj.weight', 'transformer.temporal_decoder3.layers.0.self_attn.out_proj.bias', 'transformer.temporal_decoder3.layers.0.norm2.weight', 'transformer.temporal_decoder3.layers.0.norm2.bias', 'transformer.temporal_decoder3.layers.0.linear1.weight', 'transformer.temporal_decoder3.layers.0.linear1.bias', 'transformer.temporal_decoder3.layers.0.linear2.weight', 'transformer.temporal_decoder3.layers.0.linear2.bias', 'transformer.temporal_decoder3.layers.0.norm3.weight', 'transformer.temporal_decoder3.layers.0.norm3.bias', 'temp_class_embed.weight', 'temp_class_embed.bias', 'temp_bbox_embed.layers.0.weight', 'temp_bbox_embed.layers.0.bias', 'temp_bbox_embed.layers.1.weight', 'temp_bbox_embed.layers.1.bias', 'temp_bbox_embed.layers.2.weight', 'temp_bbox_embed.layers.2.bias', 'temp_class_embed_list.0.weight', 'temp_class_embed_list.0.bias', 'temp_class_embed_list.1.weight', 'temp_class_embed_list.1.bias', 'temp_class_embed_list.2.weight', 'temp_class_embed_list.2.bias', 'temp_bbox_embed_list.0.layers.0.weight', 'temp_bbox_embed_list.0.layers.0.bias', 'temp_bbox_embed_list.0.layers.1.weight', 'temp_bbox_embed_list.0.layers.1.bias', 'temp_bbox_embed_list.0.layers.2.weight', 'temp_bbox_embed_list.0.layers.2.bias', 'temp_bbox_embed_list.1.layers.0.weight', 'temp_bbox_embed_list.1.layers.0.bias', 'temp_bbox_embed_list.1.layers.1.weight', 'temp_bbox_embed_list.1.layers.1.bias', 'temp_bbox_embed_list.1.layers.2.weight', 'temp_bbox_embed_list.1.layers.2.bias', 'temp_bbox_embed_list.2.layers.0.weight', 'temp_bbox_embed_list.2.layers.0.bias', 'temp_bbox_embed_list.2.layers.1.weight', 'temp_bbox_embed_list.2.layers.1.bias', 'temp_bbox_embed_list.2.layers.2.weight', 'temp_bbox_embed_list.2.layers.2.bias']\n",
      "Test:  [   0/4410]  eta: 0:27:59  class_error: 100.00  loss: 5.5884 (5.5884)  loss_ce: 2.6433 (2.6433)  loss_bbox: 1.7548 (1.7548)  loss_giou: 1.1903 (1.1903)  loss_ce_unscaled: 1.3216 (1.3216)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3510 (0.3510)  loss_giou_unscaled: 0.5951 (0.5951)  cardinality_error_unscaled: 29.0000 (29.0000)  loss_ce_0_unscaled: 1.1814 (1.1814)  loss_bbox_0_unscaled: 0.3510 (0.3510)  loss_giou_0_unscaled: 0.5951 (0.5951)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.1590 (1.1590)  loss_bbox_1_unscaled: 0.3510 (0.3510)  loss_giou_1_unscaled: 0.5951 (0.5951)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.3808  data: 0.0244  max mem: 594\n",
      "Test:  [  10/4410]  eta: 0:09:28  class_error: 100.00  loss: 7.5962 (7.6402)  loss_ce: 2.6834 (2.7206)  loss_bbox: 3.3305 (3.3778)  loss_giou: 1.6069 (1.5419)  loss_ce_unscaled: 1.3417 (1.3603)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6661 (0.6756)  loss_giou_unscaled: 0.8034 (0.7709)  cardinality_error_unscaled: 29.0000 (28.6364)  loss_ce_0_unscaled: 1.2275 (1.2251)  loss_bbox_0_unscaled: 0.5955 (0.5787)  loss_giou_0_unscaled: 0.7238 (0.7298)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.1943 (1.1893)  loss_bbox_1_unscaled: 0.6460 (0.5957)  loss_giou_1_unscaled: 0.7191 (0.7134)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1293  data: 0.0079  max mem: 594\n",
      "Test:  [  20/4410]  eta: 0:08:39  class_error: 100.00  loss: 7.7131 (7.6320)  loss_ce: 2.7000 (2.7256)  loss_bbox: 3.5599 (3.3958)  loss_giou: 1.5427 (1.5106)  loss_ce_unscaled: 1.3500 (1.3628)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7120 (0.6792)  loss_giou_unscaled: 0.7713 (0.7553)  cardinality_error_unscaled: 29.0000 (28.6190)  loss_ce_0_unscaled: 1.2202 (1.2204)  loss_bbox_0_unscaled: 0.6504 (0.6121)  loss_giou_0_unscaled: 0.6915 (0.7121)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.1897 (1.1841)  loss_bbox_1_unscaled: 0.6516 (0.6283)  loss_giou_1_unscaled: 0.6915 (0.6964)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1052  data: 0.0063  max mem: 594\n",
      "Test:  [  30/4410]  eta: 0:08:18  class_error: 100.00  loss: 7.6808 (7.5971)  loss_ce: 2.7151 (2.7352)  loss_bbox: 3.3955 (3.3655)  loss_giou: 1.5135 (1.4963)  loss_ce_unscaled: 1.3576 (1.3676)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6791 (0.6731)  loss_giou_unscaled: 0.7568 (0.7482)  cardinality_error_unscaled: 29.0000 (28.6452)  loss_ce_0_unscaled: 1.2148 (1.2213)  loss_bbox_0_unscaled: 0.6271 (0.6101)  loss_giou_0_unscaled: 0.7101 (0.7189)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.1893 (1.1915)  loss_bbox_1_unscaled: 0.6601 (0.6264)  loss_giou_1_unscaled: 0.6947 (0.7111)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1052  data: 0.0064  max mem: 594\n",
      "Test:  [  40/4410]  eta: 0:08:06  class_error: 100.00  loss: 7.1690 (7.4217)  loss_ce: 2.8433 (2.7723)  loss_bbox: 3.0214 (3.2144)  loss_giou: 1.3265 (1.4350)  loss_ce_unscaled: 1.4216 (1.3861)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6043 (0.6429)  loss_giou_unscaled: 0.6632 (0.7175)  cardinality_error_unscaled: 28.0000 (28.3415)  loss_ce_0_unscaled: 1.2665 (1.2418)  loss_bbox_0_unscaled: 0.5440 (0.5779)  loss_giou_0_unscaled: 0.6417 (0.6821)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2317 (1.2078)  loss_bbox_1_unscaled: 0.5766 (0.5914)  loss_giou_1_unscaled: 0.6120 (0.6777)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1040  data: 0.0065  max mem: 594\n",
      "Test:  [  50/4410]  eta: 0:08:01  class_error: 100.00  loss: 6.8941 (7.3029)  loss_ce: 2.9039 (2.8008)  loss_bbox: 2.6537 (3.1128)  loss_giou: 1.1978 (1.3893)  loss_ce_unscaled: 1.4520 (1.4004)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5307 (0.6226)  loss_giou_unscaled: 0.5989 (0.6946)  cardinality_error_unscaled: 28.0000 (28.2941)  loss_ce_0_unscaled: 1.2999 (1.2511)  loss_bbox_0_unscaled: 0.4899 (0.5656)  loss_giou_0_unscaled: 0.5475 (0.6709)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2626 (1.2192)  loss_bbox_1_unscaled: 0.4899 (0.5773)  loss_giou_1_unscaled: 0.5979 (0.6642)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1051  data: 0.0066  max mem: 594\n",
      "Test:  [  60/4410]  eta: 0:08:00  class_error: 100.00  loss: 6.7123 (7.1817)  loss_ce: 2.9109 (2.8184)  loss_bbox: 2.6259 (3.0187)  loss_giou: 1.0660 (1.3447)  loss_ce_unscaled: 1.4554 (1.4092)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5252 (0.6037)  loss_giou_unscaled: 0.5330 (0.6723)  cardinality_error_unscaled: 28.0000 (28.3279)  loss_ce_0_unscaled: 1.3012 (1.2612)  loss_bbox_0_unscaled: 0.5090 (0.5500)  loss_giou_0_unscaled: 0.5431 (0.6520)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2648 (1.2276)  loss_bbox_1_unscaled: 0.5090 (0.5624)  loss_giou_1_unscaled: 0.5431 (0.6462)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1089  data: 0.0067  max mem: 594\n",
      "Test:  [  70/4410]  eta: 0:07:57  class_error: 100.00  loss: 6.9609 (7.1830)  loss_ce: 2.9032 (2.8277)  loss_bbox: 2.8174 (3.0170)  loss_giou: 1.1085 (1.3383)  loss_ce_unscaled: 1.4516 (1.4139)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5635 (0.6034)  loss_giou_unscaled: 0.5543 (0.6691)  cardinality_error_unscaled: 29.0000 (28.4225)  loss_ce_0_unscaled: 1.2968 (1.2625)  loss_bbox_0_unscaled: 0.5575 (0.5563)  loss_giou_0_unscaled: 0.5663 (0.6497)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2672 (1.2334)  loss_bbox_1_unscaled: 0.5708 (0.5682)  loss_giou_1_unscaled: 0.5543 (0.6431)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1093  data: 0.0065  max mem: 594\n",
      "Test:  [  80/4410]  eta: 0:07:54  class_error: 100.00  loss: 6.9507 (7.1660)  loss_ce: 2.8921 (2.8346)  loss_bbox: 2.8559 (2.9944)  loss_giou: 1.3246 (1.3370)  loss_ce_unscaled: 1.4460 (1.4173)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5712 (0.5989)  loss_giou_unscaled: 0.6623 (0.6685)  cardinality_error_unscaled: 29.0000 (28.4691)  loss_ce_0_unscaled: 1.2741 (1.2664)  loss_bbox_0_unscaled: 0.5712 (0.5578)  loss_giou_0_unscaled: 0.5795 (0.6464)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2720 (1.2378)  loss_bbox_1_unscaled: 0.5712 (0.5684)  loss_giou_1_unscaled: 0.5930 (0.6412)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1066  data: 0.0063  max mem: 594\n",
      "Test:  [  90/4410]  eta: 0:07:50  class_error: 100.00  loss: 6.9507 (7.1945)  loss_ce: 2.8959 (2.8420)  loss_bbox: 2.8931 (3.0029)  loss_giou: 1.3794 (1.3497)  loss_ce_unscaled: 1.4479 (1.4210)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5786 (0.6006)  loss_giou_unscaled: 0.6897 (0.6748)  cardinality_error_unscaled: 29.0000 (28.5275)  loss_ce_0_unscaled: 1.2757 (1.2662)  loss_bbox_0_unscaled: 0.5594 (0.5599)  loss_giou_0_unscaled: 0.6513 (0.6506)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2562 (1.2385)  loss_bbox_1_unscaled: 0.5594 (0.5699)  loss_giou_1_unscaled: 0.6513 (0.6462)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1050  data: 0.0062  max mem: 594\n",
      "Test:  [ 100/4410]  eta: 0:07:47  class_error: 100.00  loss: 7.4877 (7.2296)  loss_ce: 2.8640 (2.8411)  loss_bbox: 3.0480 (3.0170)  loss_giou: 1.4387 (1.3715)  loss_ce_unscaled: 1.4320 (1.4206)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6096 (0.6034)  loss_giou_unscaled: 0.7194 (0.6857)  cardinality_error_unscaled: 29.0000 (28.4950)  loss_ce_0_unscaled: 1.2681 (1.2673)  loss_bbox_0_unscaled: 0.5635 (0.5635)  loss_giou_0_unscaled: 0.7040 (0.6618)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2455 (1.2382)  loss_bbox_1_unscaled: 0.5635 (0.5725)  loss_giou_1_unscaled: 0.7040 (0.6578)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1039  data: 0.0062  max mem: 594\n",
      "Test:  [ 110/4410]  eta: 0:07:45  class_error: 100.00  loss: 7.5878 (7.2800)  loss_ce: 2.8381 (2.8395)  loss_bbox: 3.0025 (3.0469)  loss_giou: 1.5895 (1.3937)  loss_ce_unscaled: 1.4191 (1.4197)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6005 (0.6094)  loss_giou_unscaled: 0.7947 (0.6968)  cardinality_error_unscaled: 29.0000 (28.5045)  loss_ce_0_unscaled: 1.2697 (1.2673)  loss_bbox_0_unscaled: 0.5884 (0.5672)  loss_giou_0_unscaled: 0.7373 (0.6718)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.2408 (1.2367)  loss_bbox_1_unscaled: 0.5884 (0.5772)  loss_giou_1_unscaled: 0.7313 (0.6666)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 0.1056  data: 0.0064  max mem: 594\n",
      "Test:  [ 120/4410]  eta: 0:07:43  class_error: 100.00  loss: 7.6705 (7.3358)  loss_ce: 2.8223 (2.8340)  loss_bbox: 3.3477 (3.0861)  loss_giou: 1.6118 (1.4157)  loss_ce_unscaled: 1.4112 (1.4170)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6695 (0.6172)  loss_giou_unscaled: 0.8059 (0.7078)  cardinality_error_unscaled: 29.0000 (28.5289)  loss_ce_0_unscaled: 1.2600 (1.2668)  loss_bbox_0_unscaled: 0.6032 (0.5711)  loss_giou_0_unscaled: 0.7774 (0.6818)  cardinality_error_0_unscaled: 79.0000 (78.9917)  loss_ce_1_unscaled: 1.2338 (1.2357)  loss_bbox_1_unscaled: 0.6321 (0.5850)  loss_giou_1_unscaled: 0.7927 (0.6796)  cardinality_error_1_unscaled: 49.0000 (48.9917)  time: 0.1059  data: 0.0066  max mem: 594\n",
      "Test:  [ 130/4410]  eta: 0:07:41  class_error: 100.00  loss: 7.8340 (7.3751)  loss_ce: 2.8573 (2.8360)  loss_bbox: 3.2542 (3.0944)  loss_giou: 1.7250 (1.4447)  loss_ce_unscaled: 1.4286 (1.4180)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6508 (0.6189)  loss_giou_unscaled: 0.8625 (0.7223)  cardinality_error_unscaled: 28.0000 (28.3282)  loss_ce_0_unscaled: 1.2620 (1.2672)  loss_bbox_0_unscaled: 0.5977 (0.5698)  loss_giou_0_unscaled: 0.7883 (0.6930)  cardinality_error_0_unscaled: 78.0000 (78.9160)  loss_ce_1_unscaled: 1.2336 (1.2358)  loss_bbox_1_unscaled: 0.6171 (0.5838)  loss_giou_1_unscaled: 0.8311 (0.6927)  cardinality_error_1_unscaled: 48.0000 (48.9160)  time: 0.1045  data: 0.0064  max mem: 594\n",
      "Test:  [ 140/4410]  eta: 0:07:39  class_error: 100.00  loss: 7.8904 (7.4348)  loss_ce: 2.8594 (2.8368)  loss_bbox: 3.2542 (3.1234)  loss_giou: 1.8599 (1.4746)  loss_ce_unscaled: 1.4297 (1.4184)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6508 (0.6247)  loss_giou_unscaled: 0.9299 (0.7373)  cardinality_error_unscaled: 27.0000 (28.0426)  loss_ce_0_unscaled: 1.2713 (1.2673)  loss_bbox_0_unscaled: 0.5334 (0.5683)  loss_giou_0_unscaled: 0.8050 (0.7019)  cardinality_error_0_unscaled: 78.0000 (78.8511)  loss_ce_1_unscaled: 1.2390 (1.2354)  loss_bbox_1_unscaled: 0.5936 (0.5889)  loss_giou_1_unscaled: 0.8684 (0.7057)  cardinality_error_1_unscaled: 48.0000 (48.8511)  time: 0.1054  data: 0.0062  max mem: 594\n",
      "Test:  [ 150/4410]  eta: 0:07:38  class_error: 100.00  loss: 7.7145 (7.4512)  loss_ce: 2.8462 (2.8379)  loss_bbox: 3.0155 (3.1204)  loss_giou: 1.8208 (1.4929)  loss_ce_unscaled: 1.4231 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6031 (0.6241)  loss_giou_unscaled: 0.9104 (0.7465)  cardinality_error_unscaled: 28.0000 (27.9934)  loss_ce_0_unscaled: 1.2732 (1.2680)  loss_bbox_0_unscaled: 0.5291 (0.5664)  loss_giou_0_unscaled: 0.7981 (0.7102)  cardinality_error_0_unscaled: 78.0000 (78.7947)  loss_ce_1_unscaled: 1.2447 (1.2369)  loss_bbox_1_unscaled: 0.5809 (0.5873)  loss_giou_1_unscaled: 0.8430 (0.7151)  cardinality_error_1_unscaled: 48.0000 (48.7947)  time: 0.1070  data: 0.0061  max mem: 594\n",
      "Test:  [ 160/4410]  eta: 0:07:36  class_error: 100.00  loss: 7.6438 (7.4844)  loss_ce: 2.8470 (2.8384)  loss_bbox: 2.9052 (3.1182)  loss_giou: 1.8208 (1.5278)  loss_ce_unscaled: 1.4235 (1.4192)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5810 (0.6236)  loss_giou_unscaled: 0.9104 (0.7639)  cardinality_error_unscaled: 28.0000 (27.9565)  loss_ce_0_unscaled: 1.2617 (1.2669)  loss_bbox_0_unscaled: 0.5389 (0.5664)  loss_giou_0_unscaled: 0.8695 (0.7236)  cardinality_error_0_unscaled: 78.0000 (78.7453)  loss_ce_1_unscaled: 1.2416 (1.2369)  loss_bbox_1_unscaled: 0.5484 (0.5861)  loss_giou_1_unscaled: 0.8752 (0.7279)  cardinality_error_1_unscaled: 48.0000 (48.7453)  time: 0.1060  data: 0.0062  max mem: 594\n",
      "Test:  [ 170/4410]  eta: 0:07:34  class_error: 100.00  loss: 7.8117 (7.5195)  loss_ce: 2.8436 (2.8391)  loss_bbox: 2.9736 (3.1192)  loss_giou: 2.1198 (1.5612)  loss_ce_unscaled: 1.4218 (1.4195)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5947 (0.6238)  loss_giou_unscaled: 1.0599 (0.7806)  cardinality_error_unscaled: 28.0000 (27.9474)  loss_ce_0_unscaled: 1.2617 (1.2674)  loss_bbox_0_unscaled: 0.5404 (0.5652)  loss_giou_0_unscaled: 0.8874 (0.7343)  cardinality_error_0_unscaled: 78.0000 (78.7018)  loss_ce_1_unscaled: 1.2386 (1.2368)  loss_bbox_1_unscaled: 0.5611 (0.5846)  loss_giou_1_unscaled: 0.8815 (0.7381)  cardinality_error_1_unscaled: 48.0000 (48.7018)  time: 0.1035  data: 0.0063  max mem: 594\n",
      "Test:  [ 180/4410]  eta: 0:07:33  class_error: 100.00  loss: 7.8509 (7.5567)  loss_ce: 2.8436 (2.8385)  loss_bbox: 2.9649 (3.1244)  loss_giou: 2.1198 (1.5938)  loss_ce_unscaled: 1.4218 (1.4192)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5930 (0.6249)  loss_giou_unscaled: 1.0599 (0.7969)  cardinality_error_unscaled: 28.0000 (27.8066)  loss_ce_0_unscaled: 1.2690 (1.2672)  loss_bbox_0_unscaled: 0.5404 (0.5653)  loss_giou_0_unscaled: 0.8607 (0.7427)  cardinality_error_0_unscaled: 78.0000 (78.6685)  loss_ce_1_unscaled: 1.2456 (1.2371)  loss_bbox_1_unscaled: 0.5600 (0.5844)  loss_giou_1_unscaled: 0.8815 (0.7479)  cardinality_error_1_unscaled: 48.0000 (48.6685)  time: 0.1043  data: 0.0061  max mem: 594\n",
      "Test:  [ 190/4410]  eta: 0:07:31  class_error: 100.00  loss: 8.1979 (7.6155)  loss_ce: 2.6613 (2.8279)  loss_bbox: 3.7023 (3.1677)  loss_giou: 2.0764 (1.6199)  loss_ce_unscaled: 1.3307 (1.4140)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7405 (0.6335)  loss_giou_unscaled: 1.0382 (0.8099)  cardinality_error_unscaled: 16.0000 (26.9319)  loss_ce_0_unscaled: 1.2339 (1.2640)  loss_bbox_0_unscaled: 0.6057 (0.5732)  loss_giou_0_unscaled: 0.8763 (0.7503)  cardinality_error_0_unscaled: 79.0000 (78.6859)  loss_ce_1_unscaled: 1.2428 (1.2376)  loss_bbox_1_unscaled: 0.6057 (0.5900)  loss_giou_1_unscaled: 0.8883 (0.7594)  cardinality_error_1_unscaled: 49.0000 (48.6859)  time: 0.1048  data: 0.0061  max mem: 594\n",
      "Test:  [ 200/4410]  eta: 0:07:29  class_error: 100.00  loss: 8.6744 (7.6615)  loss_ce: 2.6426 (2.8202)  loss_bbox: 3.8116 (3.1944)  loss_giou: 2.0970 (1.6469)  loss_ce_unscaled: 1.3213 (1.4101)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7623 (0.6389)  loss_giou_unscaled: 1.0485 (0.8234)  cardinality_error_unscaled: 9.0000 (26.0846)  loss_ce_0_unscaled: 1.1931 (1.2607)  loss_bbox_0_unscaled: 0.7217 (0.5789)  loss_giou_0_unscaled: 0.8888 (0.7597)  cardinality_error_0_unscaled: 79.0000 (78.7015)  loss_ce_1_unscaled: 1.2422 (1.2376)  loss_bbox_1_unscaled: 0.6838 (0.5951)  loss_giou_1_unscaled: 0.9235 (0.7703)  cardinality_error_1_unscaled: 49.0000 (48.7015)  time: 0.1043  data: 0.0061  max mem: 594\n",
      "Test:  [ 210/4410]  eta: 0:07:27  class_error: 100.00  loss: 8.6744 (7.7167)  loss_ce: 2.6539 (2.8123)  loss_bbox: 3.7684 (3.2413)  loss_giou: 2.0299 (1.6631)  loss_ce_unscaled: 1.3270 (1.4061)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7537 (0.6483)  loss_giou_unscaled: 1.0149 (0.8315)  cardinality_error_unscaled: 11.0000 (25.6161)  loss_ce_0_unscaled: 1.1997 (1.2580)  loss_bbox_0_unscaled: 0.7217 (0.5867)  loss_giou_0_unscaled: 0.8911 (0.7674)  cardinality_error_0_unscaled: 79.0000 (78.7156)  loss_ce_1_unscaled: 1.2389 (1.2374)  loss_bbox_1_unscaled: 0.7217 (0.6032)  loss_giou_1_unscaled: 0.8995 (0.7772)  cardinality_error_1_unscaled: 49.0000 (48.7156)  time: 0.1042  data: 0.0061  max mem: 594\n",
      "Test:  [ 220/4410]  eta: 0:07:26  class_error: 100.00  loss: 8.1763 (7.7188)  loss_ce: 2.6463 (2.8047)  loss_bbox: 3.6942 (3.2436)  loss_giou: 1.8032 (1.6705)  loss_ce_unscaled: 1.3232 (1.4024)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7388 (0.6487)  loss_giou_unscaled: 0.9016 (0.8352)  cardinality_error_unscaled: 18.0000 (25.3303)  loss_ce_0_unscaled: 1.2103 (1.2559)  loss_bbox_0_unscaled: 0.7044 (0.5899)  loss_giou_0_unscaled: 0.8996 (0.7740)  cardinality_error_0_unscaled: 79.0000 (78.7285)  loss_ce_1_unscaled: 1.2410 (1.2374)  loss_bbox_1_unscaled: 0.7044 (0.6057)  loss_giou_1_unscaled: 0.8995 (0.7834)  cardinality_error_1_unscaled: 49.0000 (48.7285)  time: 0.1036  data: 0.0063  max mem: 594\n",
      "Test:  [ 230/4410]  eta: 0:07:25  class_error: 100.00  loss: 7.7441 (7.7468)  loss_ce: 2.6418 (2.7975)  loss_bbox: 3.2847 (3.2739)  loss_giou: 1.8032 (1.6754)  loss_ce_unscaled: 1.3209 (1.3988)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6569 (0.6548)  loss_giou_unscaled: 0.9016 (0.8377)  cardinality_error_unscaled: 20.0000 (25.0476)  loss_ce_0_unscaled: 1.2137 (1.2543)  loss_bbox_0_unscaled: 0.6351 (0.5946)  loss_giou_0_unscaled: 0.9016 (0.7786)  cardinality_error_0_unscaled: 79.0000 (78.7403)  loss_ce_1_unscaled: 1.2427 (1.2376)  loss_bbox_1_unscaled: 0.6351 (0.6097)  loss_giou_1_unscaled: 0.9016 (0.7876)  cardinality_error_1_unscaled: 49.0000 (48.7403)  time: 0.1047  data: 0.0063  max mem: 594\n",
      "Test:  [ 240/4410]  eta: 0:07:23  class_error: 100.00  loss: 8.3388 (7.7625)  loss_ce: 2.6499 (2.7920)  loss_bbox: 3.7096 (3.2879)  loss_giou: 1.7822 (1.6827)  loss_ce_unscaled: 1.3249 (1.3960)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7419 (0.6576)  loss_giou_unscaled: 0.8911 (0.8413)  cardinality_error_unscaled: 21.0000 (24.8340)  loss_ce_0_unscaled: 1.2189 (1.2529)  loss_bbox_0_unscaled: 0.7170 (0.5989)  loss_giou_0_unscaled: 0.8736 (0.7832)  cardinality_error_0_unscaled: 79.0000 (78.7469)  loss_ce_1_unscaled: 1.2454 (1.2377)  loss_bbox_1_unscaled: 0.7170 (0.6134)  loss_giou_1_unscaled: 0.8736 (0.7918)  cardinality_error_1_unscaled: 49.0000 (48.7469)  time: 0.1052  data: 0.0064  max mem: 594\n",
      "Test:  [ 250/4410]  eta: 0:07:22  class_error: 100.00  loss: 8.3128 (7.7876)  loss_ce: 2.7451 (2.7926)  loss_bbox: 3.7109 (3.3085)  loss_giou: 1.8263 (1.6865)  loss_ce_unscaled: 1.3726 (1.3963)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7422 (0.6617)  loss_giou_unscaled: 0.9132 (0.8432)  cardinality_error_unscaled: 24.0000 (24.8805)  loss_ce_0_unscaled: 1.2618 (1.2550)  loss_bbox_0_unscaled: 0.7077 (0.6023)  loss_giou_0_unscaled: 0.8736 (0.7877)  cardinality_error_0_unscaled: 78.0000 (78.7171)  loss_ce_1_unscaled: 1.2454 (1.2381)  loss_bbox_1_unscaled: 0.7178 (0.6177)  loss_giou_1_unscaled: 0.8736 (0.7955)  cardinality_error_1_unscaled: 48.0000 (48.7171)  time: 0.1052  data: 0.0063  max mem: 594\n",
      "Test:  [ 260/4410]  eta: 0:07:21  class_error: 100.00  loss: 8.3128 (7.8086)  loss_ce: 2.8183 (2.7936)  loss_bbox: 3.7160 (3.3259)  loss_giou: 1.7137 (1.6892)  loss_ce_unscaled: 1.4092 (1.3968)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7432 (0.6652)  loss_giou_unscaled: 0.8569 (0.8446)  cardinality_error_unscaled: 26.0000 (24.8966)  loss_ce_0_unscaled: 1.2992 (1.2567)  loss_bbox_0_unscaled: 0.7030 (0.6055)  loss_giou_0_unscaled: 0.8663 (0.7916)  cardinality_error_0_unscaled: 78.0000 (78.6897)  loss_ce_1_unscaled: 1.2489 (1.2385)  loss_bbox_1_unscaled: 0.7357 (0.6216)  loss_giou_1_unscaled: 0.8569 (0.7990)  cardinality_error_1_unscaled: 48.0000 (48.6897)  time: 0.1058  data: 0.0063  max mem: 594\n",
      "Test:  [ 270/4410]  eta: 0:07:20  class_error: 100.00  loss: 8.2893 (7.8299)  loss_ce: 2.8194 (2.7947)  loss_bbox: 3.8156 (3.3460)  loss_giou: 1.6741 (1.6892)  loss_ce_unscaled: 1.4097 (1.3974)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7631 (0.6692)  loss_giou_unscaled: 0.8371 (0.8446)  cardinality_error_unscaled: 26.0000 (24.9520)  loss_ce_0_unscaled: 1.2964 (1.2581)  loss_bbox_0_unscaled: 0.7090 (0.6097)  loss_giou_0_unscaled: 0.8478 (0.7939)  cardinality_error_0_unscaled: 78.0000 (78.6679)  loss_ce_1_unscaled: 1.2516 (1.2390)  loss_bbox_1_unscaled: 0.7302 (0.6258)  loss_giou_1_unscaled: 0.8422 (0.8010)  cardinality_error_1_unscaled: 48.0000 (48.6679)  time: 0.1066  data: 0.0062  max mem: 594\n",
      "Test:  [ 280/4410]  eta: 0:07:19  class_error: 100.00  loss: 8.4690 (7.8546)  loss_ce: 2.8442 (2.7977)  loss_bbox: 3.8311 (3.3642)  loss_giou: 1.6741 (1.6928)  loss_ce_unscaled: 1.4221 (1.3988)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7662 (0.6728)  loss_giou_unscaled: 0.8371 (0.8464)  cardinality_error_unscaled: 28.0000 (25.0605)  loss_ce_0_unscaled: 1.2672 (1.2578)  loss_bbox_0_unscaled: 0.7251 (0.6155)  loss_giou_0_unscaled: 0.8478 (0.7975)  cardinality_error_0_unscaled: 78.0000 (78.6762)  loss_ce_1_unscaled: 1.2327 (1.2383)  loss_bbox_1_unscaled: 0.7302 (0.6310)  loss_giou_1_unscaled: 0.8478 (0.8044)  cardinality_error_1_unscaled: 49.0000 (48.6797)  time: 0.1074  data: 0.0062  max mem: 594\n",
      "Test:  [ 290/4410]  eta: 0:07:18  class_error: 100.00  loss: 8.4690 (7.8738)  loss_ce: 2.8699 (2.7994)  loss_bbox: 3.6793 (3.3734)  loss_giou: 1.8513 (1.7011)  loss_ce_unscaled: 1.4350 (1.3997)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7359 (0.6747)  loss_giou_unscaled: 0.9257 (0.8506)  cardinality_error_unscaled: 28.0000 (25.1649)  loss_ce_0_unscaled: 1.2447 (1.2574)  loss_bbox_0_unscaled: 0.7359 (0.6192)  loss_giou_0_unscaled: 0.9257 (0.8032)  cardinality_error_0_unscaled: 79.0000 (78.6838)  loss_ce_1_unscaled: 1.2135 (1.2371)  loss_bbox_1_unscaled: 0.7359 (0.6340)  loss_giou_1_unscaled: 0.9257 (0.8103)  cardinality_error_1_unscaled: 49.0000 (48.6907)  time: 0.1081  data: 0.0064  max mem: 594\n",
      "Test:  [ 300/4410]  eta: 0:07:17  class_error: 100.00  loss: 8.4447 (7.8953)  loss_ce: 2.8416 (2.8012)  loss_bbox: 3.7521 (3.3882)  loss_giou: 1.8895 (1.7059)  loss_ce_unscaled: 1.4208 (1.4006)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7504 (0.6776)  loss_giou_unscaled: 0.9447 (0.8529)  cardinality_error_unscaled: 28.0000 (25.2392)  loss_ce_0_unscaled: 1.2483 (1.2571)  loss_bbox_0_unscaled: 0.7426 (0.6237)  loss_giou_0_unscaled: 0.9447 (0.8073)  cardinality_error_0_unscaled: 79.0000 (78.6910)  loss_ce_1_unscaled: 1.2106 (1.2363)  loss_bbox_1_unscaled: 0.7426 (0.6382)  loss_giou_1_unscaled: 0.9447 (0.8140)  cardinality_error_1_unscaled: 49.0000 (48.6977)  time: 0.1080  data: 0.0063  max mem: 594\n",
      "Test:  [ 310/4410]  eta: 0:07:16  class_error: 100.00  loss: 8.4447 (7.9127)  loss_ce: 2.7987 (2.8008)  loss_bbox: 3.5418 (3.3876)  loss_giou: 2.0180 (1.7243)  loss_ce_unscaled: 1.3993 (1.4004)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7084 (0.6775)  loss_giou_unscaled: 1.0090 (0.8621)  cardinality_error_unscaled: 26.0000 (25.2444)  loss_ce_0_unscaled: 1.2527 (1.2571)  loss_bbox_0_unscaled: 0.5347 (0.6200)  loss_giou_0_unscaled: 1.0101 (0.8147)  cardinality_error_0_unscaled: 78.0000 (78.6688)  loss_ce_1_unscaled: 1.1550 (1.2335)  loss_bbox_1_unscaled: 0.6337 (0.6369)  loss_giou_1_unscaled: 0.9982 (0.8203)  cardinality_error_1_unscaled: 48.0000 (48.6752)  time: 0.1067  data: 0.0064  max mem: 594\n",
      "Test:  [ 320/4410]  eta: 0:07:15  class_error: 100.00  loss: 8.2790 (7.9312)  loss_ce: 2.7780 (2.8000)  loss_bbox: 3.2951 (3.3894)  loss_giou: 2.2688 (1.7418)  loss_ce_unscaled: 1.3890 (1.4000)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6590 (0.6779)  loss_giou_unscaled: 1.1344 (0.8709)  cardinality_error_unscaled: 26.0000 (25.2648)  loss_ce_0_unscaled: 1.2568 (1.2572)  loss_bbox_0_unscaled: 0.5027 (0.6162)  loss_giou_0_unscaled: 1.0320 (0.8212)  cardinality_error_0_unscaled: 78.0000 (78.6480)  loss_ce_1_unscaled: 1.1454 (1.2307)  loss_bbox_1_unscaled: 0.5905 (0.6353)  loss_giou_1_unscaled: 1.0090 (0.8261)  cardinality_error_1_unscaled: 48.0000 (48.6542)  time: 0.1063  data: 0.0065  max mem: 594\n",
      "Test:  [ 330/4410]  eta: 0:07:14  class_error: 100.00  loss: 7.8865 (7.9346)  loss_ce: 2.7707 (2.8001)  loss_bbox: 3.0432 (3.3841)  loss_giou: 2.0083 (1.7503)  loss_ce_unscaled: 1.3854 (1.4001)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6086 (0.6768)  loss_giou_unscaled: 1.0042 (0.8752)  cardinality_error_unscaled: 26.0000 (25.2840)  loss_ce_0_unscaled: 1.2636 (1.2574)  loss_bbox_0_unscaled: 0.4888 (0.6116)  loss_giou_0_unscaled: 1.0166 (0.8265)  cardinality_error_0_unscaled: 78.0000 (78.6314)  loss_ce_1_unscaled: 1.1442 (1.2284)  loss_bbox_1_unscaled: 0.5734 (0.6332)  loss_giou_1_unscaled: 0.9994 (0.8307)  cardinality_error_1_unscaled: 48.0000 (48.6375)  time: 0.1076  data: 0.0064  max mem: 594\n",
      "Test:  [ 340/4410]  eta: 0:07:13  class_error: 100.00  loss: 7.7592 (7.9291)  loss_ce: 2.9286 (2.8046)  loss_bbox: 3.2641 (3.3816)  loss_giou: 1.5605 (1.7429)  loss_ce_unscaled: 1.4643 (1.4023)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6528 (0.6763)  loss_giou_unscaled: 0.7802 (0.8715)  cardinality_error_unscaled: 29.0000 (25.3930)  loss_ce_0_unscaled: 1.2636 (1.2575)  loss_bbox_0_unscaled: 0.5655 (0.6128)  loss_giou_0_unscaled: 0.7852 (0.8243)  cardinality_error_0_unscaled: 79.0000 (78.6422)  loss_ce_1_unscaled: 1.2413 (1.2296)  loss_bbox_1_unscaled: 0.5948 (0.6338)  loss_giou_1_unscaled: 0.7852 (0.8284)  cardinality_error_1_unscaled: 49.0000 (48.6481)  time: 0.1082  data: 0.0065  max mem: 594\n",
      "Test:  [ 350/4410]  eta: 0:07:13  class_error: 100.00  loss: 7.6952 (7.9219)  loss_ce: 2.9488 (2.8088)  loss_bbox: 3.2616 (3.3766)  loss_giou: 1.5041 (1.7365)  loss_ce_unscaled: 1.4744 (1.4044)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6523 (0.6753)  loss_giou_unscaled: 0.7520 (0.8683)  cardinality_error_unscaled: 29.0000 (25.4957)  loss_ce_0_unscaled: 1.2586 (1.2579)  loss_bbox_0_unscaled: 0.6403 (0.6133)  loss_giou_0_unscaled: 0.7560 (0.8229)  cardinality_error_0_unscaled: 79.0000 (78.6524)  loss_ce_1_unscaled: 1.2758 (1.2310)  loss_bbox_1_unscaled: 0.6410 (0.6338)  loss_giou_1_unscaled: 0.7560 (0.8267)  cardinality_error_1_unscaled: 49.0000 (48.6581)  time: 0.1077  data: 0.0066  max mem: 594\n",
      "Test:  [ 360/4410]  eta: 0:07:11  class_error: 100.00  loss: 7.6425 (7.9125)  loss_ce: 2.9554 (2.8123)  loss_bbox: 3.1672 (3.3693)  loss_giou: 1.5270 (1.7309)  loss_ce_unscaled: 1.4777 (1.4061)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6334 (0.6739)  loss_giou_unscaled: 0.7635 (0.8654)  cardinality_error_unscaled: 29.0000 (25.5817)  loss_ce_0_unscaled: 1.2729 (1.2584)  loss_bbox_0_unscaled: 0.6165 (0.6132)  loss_giou_0_unscaled: 0.7748 (0.8216)  cardinality_error_0_unscaled: 79.0000 (78.6620)  loss_ce_1_unscaled: 1.2811 (1.2323)  loss_bbox_1_unscaled: 0.6295 (0.6335)  loss_giou_1_unscaled: 0.7713 (0.8250)  cardinality_error_1_unscaled: 49.0000 (48.6676)  time: 0.1071  data: 0.0066  max mem: 594\n",
      "Test:  [ 370/4410]  eta: 0:07:10  class_error: 100.00  loss: 7.2602 (7.8886)  loss_ce: 2.7993 (2.8101)  loss_bbox: 3.0107 (3.3540)  loss_giou: 1.5270 (1.7246)  loss_ce_unscaled: 1.3996 (1.4051)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6021 (0.6708)  loss_giou_unscaled: 0.7635 (0.8623)  cardinality_error_unscaled: 29.0000 (25.5876)  loss_ce_0_unscaled: 1.2625 (1.2583)  loss_bbox_0_unscaled: 0.5745 (0.6107)  loss_giou_0_unscaled: 0.7411 (0.8183)  cardinality_error_0_unscaled: 79.0000 (78.6712)  loss_ce_1_unscaled: 1.2628 (1.2314)  loss_bbox_1_unscaled: 0.6021 (0.6312)  loss_giou_1_unscaled: 0.7635 (0.8229)  cardinality_error_1_unscaled: 49.0000 (48.6765)  time: 0.1062  data: 0.0065  max mem: 594\n",
      "Test:  [ 380/4410]  eta: 0:07:09  class_error: 100.00  loss: 7.0977 (7.8630)  loss_ce: 2.7016 (2.8072)  loss_bbox: 2.8723 (3.3401)  loss_giou: 1.4453 (1.7156)  loss_ce_unscaled: 1.3508 (1.4036)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5745 (0.6680)  loss_giou_unscaled: 0.7226 (0.8578)  cardinality_error_unscaled: 26.0000 (25.6220)  loss_ce_0_unscaled: 1.2457 (1.2583)  loss_bbox_0_unscaled: 0.5238 (0.6089)  loss_giou_0_unscaled: 0.7011 (0.8146)  cardinality_error_0_unscaled: 79.0000 (78.6798)  loss_ce_1_unscaled: 1.1956 (1.2315)  loss_bbox_1_unscaled: 0.5506 (0.6288)  loss_giou_1_unscaled: 0.7160 (0.8192)  cardinality_error_1_unscaled: 49.0000 (48.6850)  time: 0.1048  data: 0.0064  max mem: 594\n",
      "Test:  [ 390/4410]  eta: 0:07:08  class_error: 100.00  loss: 7.0722 (7.8439)  loss_ce: 2.7210 (2.8055)  loss_bbox: 2.9314 (3.3273)  loss_giou: 1.4377 (1.7110)  loss_ce_unscaled: 1.3605 (1.4028)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5863 (0.6655)  loss_giou_unscaled: 0.7189 (0.8555)  cardinality_error_unscaled: 27.0000 (25.6343)  loss_ce_0_unscaled: 1.2478 (1.2584)  loss_bbox_0_unscaled: 0.5548 (0.6073)  loss_giou_0_unscaled: 0.7160 (0.8129)  cardinality_error_0_unscaled: 79.0000 (78.6880)  loss_ce_1_unscaled: 1.2470 (1.2316)  loss_bbox_1_unscaled: 0.5506 (0.6266)  loss_giou_1_unscaled: 0.7189 (0.8180)  cardinality_error_1_unscaled: 49.0000 (48.6931)  time: 0.1066  data: 0.0063  max mem: 594\n",
      "Test:  [ 400/4410]  eta: 0:07:07  class_error: 100.00  loss: 7.4634 (7.8431)  loss_ce: 2.7460 (2.8044)  loss_bbox: 2.9314 (3.3226)  loss_giou: 1.7205 (1.7162)  loss_ce_unscaled: 1.3730 (1.4022)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5863 (0.6645)  loss_giou_unscaled: 0.8603 (0.8581)  cardinality_error_unscaled: 27.0000 (25.6783)  loss_ce_0_unscaled: 1.2852 (1.2594)  loss_bbox_0_unscaled: 0.5548 (0.6049)  loss_giou_0_unscaled: 0.7442 (0.8131)  cardinality_error_0_unscaled: 79.0000 (78.6933)  loss_ce_1_unscaled: 1.2355 (1.2318)  loss_bbox_1_unscaled: 0.5849 (0.6261)  loss_giou_1_unscaled: 0.8173 (0.8199)  cardinality_error_1_unscaled: 49.0000 (48.7007)  time: 0.1074  data: 0.0063  max mem: 594\n",
      "Test:  [ 410/4410]  eta: 0:07:06  class_error: 100.00  loss: 7.7031 (7.8417)  loss_ce: 2.7457 (2.8040)  loss_bbox: 3.1194 (3.3183)  loss_giou: 1.9310 (1.7194)  loss_ce_unscaled: 1.3729 (1.4020)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6239 (0.6637)  loss_giou_unscaled: 0.9655 (0.8597)  cardinality_error_unscaled: 29.0000 (25.7275)  loss_ce_0_unscaled: 1.2959 (1.2603)  loss_bbox_0_unscaled: 0.5336 (0.6031)  loss_giou_0_unscaled: 0.7486 (0.8118)  cardinality_error_0_unscaled: 79.0000 (78.7007)  loss_ce_1_unscaled: 1.2320 (1.2318)  loss_bbox_1_unscaled: 0.5968 (0.6262)  loss_giou_1_unscaled: 0.8186 (0.8203)  cardinality_error_1_unscaled: 49.0000 (48.7080)  time: 0.1053  data: 0.0063  max mem: 594\n",
      "Test:  [ 420/4410]  eta: 0:07:05  class_error: 100.00  loss: 7.7031 (7.8409)  loss_ce: 2.7787 (2.8036)  loss_bbox: 3.0175 (3.3152)  loss_giou: 1.8539 (1.7221)  loss_ce_unscaled: 1.3893 (1.4018)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6035 (0.6630)  loss_giou_unscaled: 0.9269 (0.8611)  cardinality_error_unscaled: 29.0000 (25.7648)  loss_ce_0_unscaled: 1.2928 (1.2611)  loss_bbox_0_unscaled: 0.5704 (0.6022)  loss_giou_0_unscaled: 0.7602 (0.8124)  cardinality_error_0_unscaled: 79.0000 (78.7055)  loss_ce_1_unscaled: 1.2396 (1.2323)  loss_bbox_1_unscaled: 0.5933 (0.6249)  loss_giou_1_unscaled: 0.8379 (0.8220)  cardinality_error_1_unscaled: 49.0000 (48.7126)  time: 0.1053  data: 0.0065  max mem: 594\n",
      "Test:  [ 430/4410]  eta: 0:07:04  class_error: 100.00  loss: 7.4676 (7.8248)  loss_ce: 2.7661 (2.8028)  loss_bbox: 2.8522 (3.2996)  loss_giou: 1.8260 (1.7224)  loss_ce_unscaled: 1.3830 (1.4014)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5704 (0.6599)  loss_giou_unscaled: 0.9130 (0.8612)  cardinality_error_unscaled: 28.0000 (25.8097)  loss_ce_0_unscaled: 1.2742 (1.2610)  loss_bbox_0_unscaled: 0.4855 (0.5988)  loss_giou_0_unscaled: 0.8317 (0.8125)  cardinality_error_0_unscaled: 78.0000 (78.6845)  loss_ce_1_unscaled: 1.2396 (1.2315)  loss_bbox_1_unscaled: 0.5607 (0.6222)  loss_giou_1_unscaled: 0.8591 (0.8225)  cardinality_error_1_unscaled: 48.0000 (48.6961)  time: 0.1061  data: 0.0066  max mem: 594\n",
      "Test:  [ 440/4410]  eta: 0:07:03  class_error: 100.00  loss: 6.9514 (7.7978)  loss_ce: 2.7485 (2.8023)  loss_bbox: 2.4054 (3.2755)  loss_giou: 1.7038 (1.7200)  loss_ce_unscaled: 1.3742 (1.4012)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4811 (0.6551)  loss_giou_unscaled: 0.8519 (0.8600)  cardinality_error_unscaled: 28.0000 (25.8571)  loss_ce_0_unscaled: 1.2523 (1.2610)  loss_bbox_0_unscaled: 0.4436 (0.5948)  loss_giou_0_unscaled: 0.8131 (0.8115)  cardinality_error_0_unscaled: 78.0000 (78.6689)  loss_ce_1_unscaled: 1.2248 (1.2315)  loss_bbox_1_unscaled: 0.4782 (0.6181)  loss_giou_1_unscaled: 0.8022 (0.8219)  cardinality_error_1_unscaled: 48.0000 (48.6803)  time: 0.1077  data: 0.0063  max mem: 594\n",
      "Test:  [ 450/4410]  eta: 0:07:02  class_error: 100.00  loss: 6.7321 (7.7819)  loss_ce: 2.8082 (2.8027)  loss_bbox: 2.3291 (3.2607)  loss_giou: 1.6835 (1.7185)  loss_ce_unscaled: 1.4041 (1.4013)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4658 (0.6521)  loss_giou_unscaled: 0.8417 (0.8593)  cardinality_error_unscaled: 28.0000 (25.9069)  loss_ce_0_unscaled: 1.2551 (1.2608)  loss_bbox_0_unscaled: 0.4236 (0.5911)  loss_giou_0_unscaled: 0.7983 (0.8110)  cardinality_error_0_unscaled: 78.0000 (78.6563)  loss_ce_1_unscaled: 1.2060 (1.2300)  loss_bbox_1_unscaled: 0.4658 (0.6151)  loss_giou_1_unscaled: 0.8022 (0.8213)  cardinality_error_1_unscaled: 48.0000 (48.6674)  time: 0.1091  data: 0.0063  max mem: 594\n",
      "Test:  [ 460/4410]  eta: 0:07:01  class_error: 100.00  loss: 7.5553 (7.7930)  loss_ce: 2.8020 (2.8022)  loss_bbox: 2.9508 (3.2739)  loss_giou: 1.6841 (1.7169)  loss_ce_unscaled: 1.4010 (1.4011)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5902 (0.6548)  loss_giou_unscaled: 0.8420 (0.8584)  cardinality_error_unscaled: 29.0000 (25.9740)  loss_ce_0_unscaled: 1.2647 (1.2613)  loss_bbox_0_unscaled: 0.5103 (0.5924)  loss_giou_0_unscaled: 0.7739 (0.8084)  cardinality_error_0_unscaled: 79.0000 (78.6638)  loss_ce_1_unscaled: 1.1602 (1.2290)  loss_bbox_1_unscaled: 0.5337 (0.6175)  loss_giou_1_unscaled: 0.7756 (0.8191)  cardinality_error_1_unscaled: 49.0000 (48.6746)  time: 0.1083  data: 0.0062  max mem: 594\n",
      "Test:  [ 470/4410]  eta: 0:07:00  class_error: 100.00  loss: 8.2723 (7.7983)  loss_ce: 2.7981 (2.8023)  loss_bbox: 4.0085 (3.2839)  loss_giou: 1.4816 (1.7121)  loss_ce_unscaled: 1.3991 (1.4012)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8017 (0.6568)  loss_giou_unscaled: 0.7408 (0.8560)  cardinality_error_unscaled: 29.0000 (26.0361)  loss_ce_0_unscaled: 1.2704 (1.2616)  loss_bbox_0_unscaled: 0.6226 (0.5916)  loss_giou_0_unscaled: 0.7046 (0.8059)  cardinality_error_0_unscaled: 79.0000 (78.6709)  loss_ce_1_unscaled: 1.1894 (1.2281)  loss_bbox_1_unscaled: 0.7013 (0.6177)  loss_giou_1_unscaled: 0.7408 (0.8187)  cardinality_error_1_unscaled: 49.0000 (48.6815)  time: 0.1070  data: 0.0062  max mem: 594\n",
      "Test:  [ 480/4410]  eta: 0:06:59  class_error: 100.00  loss: 8.2515 (7.8049)  loss_ce: 2.7781 (2.8019)  loss_bbox: 3.7449 (3.2919)  loss_giou: 1.5076 (1.7111)  loss_ce_unscaled: 1.3890 (1.4009)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7490 (0.6584)  loss_giou_unscaled: 0.7538 (0.8555)  cardinality_error_unscaled: 29.0000 (26.0977)  loss_ce_0_unscaled: 1.2769 (1.2624)  loss_bbox_0_unscaled: 0.4950 (0.5899)  loss_giou_0_unscaled: 0.6402 (0.8022)  cardinality_error_0_unscaled: 79.0000 (78.6757)  loss_ce_1_unscaled: 1.1967 (1.2272)  loss_bbox_1_unscaled: 0.6957 (0.6188)  loss_giou_1_unscaled: 0.7538 (0.8169)  cardinality_error_1_unscaled: 49.0000 (48.6881)  time: 0.1073  data: 0.0062  max mem: 594\n",
      "Test:  [ 490/4410]  eta: 0:06:58  class_error: 100.00  loss: 7.3919 (7.7860)  loss_ce: 2.7772 (2.8019)  loss_bbox: 3.0032 (3.2795)  loss_giou: 1.4816 (1.7046)  loss_ce_unscaled: 1.3886 (1.4009)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6006 (0.6559)  loss_giou_unscaled: 0.7408 (0.8523)  cardinality_error_unscaled: 29.0000 (26.1568)  loss_ce_0_unscaled: 1.3081 (1.2634)  loss_bbox_0_unscaled: 0.4402 (0.5864)  loss_giou_0_unscaled: 0.5539 (0.7965)  cardinality_error_0_unscaled: 79.0000 (78.6823)  loss_ce_1_unscaled: 1.2266 (1.2275)  loss_bbox_1_unscaled: 0.5660 (0.6166)  loss_giou_1_unscaled: 0.6779 (0.8127)  cardinality_error_1_unscaled: 49.0000 (48.6945)  time: 0.1078  data: 0.0061  max mem: 594\n",
      "Test:  [ 500/4410]  eta: 0:06:57  class_error: 100.00  loss: 6.5804 (7.7512)  loss_ce: 2.8431 (2.8030)  loss_bbox: 2.4589 (3.2573)  loss_giou: 1.2423 (1.6910)  loss_ce_unscaled: 1.4216 (1.4015)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4918 (0.6515)  loss_giou_unscaled: 0.6212 (0.8455)  cardinality_error_unscaled: 29.0000 (26.2136)  loss_ce_0_unscaled: 1.3144 (1.2648)  loss_bbox_0_unscaled: 0.3898 (0.5822)  loss_giou_0_unscaled: 0.4939 (0.7899)  cardinality_error_0_unscaled: 79.0000 (78.6846)  loss_ce_1_unscaled: 1.2398 (1.2279)  loss_bbox_1_unscaled: 0.4964 (0.6129)  loss_giou_1_unscaled: 0.4986 (0.8060)  cardinality_error_1_unscaled: 49.0000 (48.7006)  time: 0.1088  data: 0.0063  max mem: 594\n",
      "Test:  [ 510/4410]  eta: 0:06:56  class_error: 100.00  loss: 6.3510 (7.7317)  loss_ce: 2.8443 (2.8033)  loss_bbox: 2.2319 (3.2439)  loss_giou: 1.1155 (1.6844)  loss_ce_unscaled: 1.4221 (1.4017)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4464 (0.6488)  loss_giou_unscaled: 0.5577 (0.8422)  cardinality_error_unscaled: 29.0000 (26.2661)  loss_ce_0_unscaled: 1.3369 (1.2661)  loss_bbox_0_unscaled: 0.3898 (0.5785)  loss_giou_0_unscaled: 0.4773 (0.7859)  cardinality_error_0_unscaled: 79.0000 (78.6869)  loss_ce_1_unscaled: 1.2498 (1.2286)  loss_bbox_1_unscaled: 0.4462 (0.6100)  loss_giou_1_unscaled: 0.4979 (0.8024)  cardinality_error_1_unscaled: 49.0000 (48.7045)  time: 0.1090  data: 0.0064  max mem: 594\n",
      "Test:  [ 520/4410]  eta: 0:06:55  class_error: 100.00  loss: 7.4377 (7.7426)  loss_ce: 2.8150 (2.8031)  loss_bbox: 3.0043 (3.2495)  loss_giou: 1.6186 (1.6900)  loss_ce_unscaled: 1.4075 (1.4015)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6009 (0.6499)  loss_giou_unscaled: 0.8093 (0.8450)  cardinality_error_unscaled: 28.0000 (26.2994)  loss_ce_0_unscaled: 1.2975 (1.2665)  loss_bbox_0_unscaled: 0.4525 (0.5774)  loss_giou_0_unscaled: 0.6742 (0.7842)  cardinality_error_0_unscaled: 78.0000 (78.6718)  loss_ce_1_unscaled: 1.2404 (1.2285)  loss_bbox_1_unscaled: 0.5686 (0.6098)  loss_giou_1_unscaled: 0.7199 (0.8019)  cardinality_error_1_unscaled: 48.0000 (48.6910)  time: 0.1084  data: 0.0064  max mem: 594\n",
      "Test:  [ 530/4410]  eta: 0:06:54  class_error: 100.00  loss: 8.1795 (7.7477)  loss_ce: 2.7841 (2.8029)  loss_bbox: 3.3551 (3.2507)  loss_giou: 1.8436 (1.6941)  loss_ce_unscaled: 1.3920 (1.4015)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6710 (0.6501)  loss_giou_unscaled: 0.9218 (0.8470)  cardinality_error_unscaled: 28.0000 (26.3315)  loss_ce_0_unscaled: 1.2875 (1.2670)  loss_bbox_0_unscaled: 0.5454 (0.5765)  loss_giou_0_unscaled: 0.6802 (0.7826)  cardinality_error_0_unscaled: 78.0000 (78.6573)  loss_ce_1_unscaled: 1.2243 (1.2283)  loss_bbox_1_unscaled: 0.6005 (0.6098)  loss_giou_1_unscaled: 0.7996 (0.8023)  cardinality_error_1_unscaled: 48.0000 (48.6780)  time: 0.1081  data: 0.0064  max mem: 594\n",
      "Test:  [ 540/4410]  eta: 0:06:53  class_error: 100.00  loss: 7.7709 (7.7561)  loss_ce: 2.7835 (2.8026)  loss_bbox: 3.1460 (3.2557)  loss_giou: 1.8156 (1.6979)  loss_ce_unscaled: 1.3918 (1.4013)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6292 (0.6511)  loss_giou_unscaled: 0.9078 (0.8490)  cardinality_error_unscaled: 28.0000 (26.3623)  loss_ce_0_unscaled: 1.2896 (1.2675)  loss_bbox_0_unscaled: 0.5448 (0.5758)  loss_giou_0_unscaled: 0.6759 (0.7815)  cardinality_error_0_unscaled: 78.0000 (78.6451)  loss_ce_1_unscaled: 1.2217 (1.2282)  loss_bbox_1_unscaled: 0.6230 (0.6098)  loss_giou_1_unscaled: 0.8214 (0.8025)  cardinality_error_1_unscaled: 48.0000 (48.6654)  time: 0.1057  data: 0.0063  max mem: 594\n",
      "Test:  [ 550/4410]  eta: 0:06:52  class_error: 100.00  loss: 7.6482 (7.7514)  loss_ce: 2.7749 (2.8022)  loss_bbox: 3.0716 (3.2496)  loss_giou: 1.8120 (1.6996)  loss_ce_unscaled: 1.3875 (1.4011)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6143 (0.6499)  loss_giou_unscaled: 0.9060 (0.8498)  cardinality_error_unscaled: 28.0000 (26.3884)  loss_ce_0_unscaled: 1.2942 (1.2681)  loss_bbox_0_unscaled: 0.5346 (0.5759)  loss_giou_0_unscaled: 0.8252 (0.7831)  cardinality_error_0_unscaled: 78.0000 (78.6334)  loss_ce_1_unscaled: 1.2305 (1.2283)  loss_bbox_1_unscaled: 0.6216 (0.6095)  loss_giou_1_unscaled: 0.8428 (0.8036)  cardinality_error_1_unscaled: 48.0000 (48.6534)  time: 0.1070  data: 0.0061  max mem: 594\n",
      "Test:  [ 560/4410]  eta: 0:06:51  class_error: 100.00  loss: 7.5802 (7.7481)  loss_ce: 2.7831 (2.8014)  loss_bbox: 3.0063 (3.2463)  loss_giou: 1.7769 (1.7003)  loss_ce_unscaled: 1.3915 (1.4007)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6013 (0.6493)  loss_giou_unscaled: 0.8884 (0.8502)  cardinality_error_unscaled: 28.0000 (26.4135)  loss_ce_0_unscaled: 1.2988 (1.2688)  loss_bbox_0_unscaled: 0.5892 (0.5762)  loss_giou_0_unscaled: 0.8795 (0.7847)  cardinality_error_0_unscaled: 78.0000 (78.6221)  loss_ce_1_unscaled: 1.2393 (1.2285)  loss_bbox_1_unscaled: 0.5911 (0.6093)  loss_giou_1_unscaled: 0.8654 (0.8047)  cardinality_error_1_unscaled: 48.0000 (48.6417)  time: 0.1077  data: 0.0063  max mem: 594\n",
      "Test:  [ 570/4410]  eta: 0:06:50  class_error: 100.00  loss: 7.5828 (7.7456)  loss_ce: 2.7574 (2.8008)  loss_bbox: 3.1028 (3.2441)  loss_giou: 1.7310 (1.7007)  loss_ce_unscaled: 1.3787 (1.4004)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6206 (0.6488)  loss_giou_unscaled: 0.8655 (0.8504)  cardinality_error_unscaled: 28.0000 (26.4413)  loss_ce_0_unscaled: 1.3137 (1.2694)  loss_bbox_0_unscaled: 0.5904 (0.5765)  loss_giou_0_unscaled: 0.8431 (0.7853)  cardinality_error_0_unscaled: 78.0000 (78.6130)  loss_ce_1_unscaled: 1.2236 (1.2287)  loss_bbox_1_unscaled: 0.6037 (0.6094)  loss_giou_1_unscaled: 0.8474 (0.8051)  cardinality_error_1_unscaled: 48.0000 (48.6322)  time: 0.1066  data: 0.0065  max mem: 594\n",
      "Test:  [ 580/4410]  eta: 0:06:49  class_error: 100.00  loss: 7.6115 (7.7457)  loss_ce: 2.7862 (2.8012)  loss_bbox: 3.1669 (3.2461)  loss_giou: 1.6771 (1.6984)  loss_ce_unscaled: 1.3931 (1.4006)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6334 (0.6492)  loss_giou_unscaled: 0.8386 (0.8492)  cardinality_error_unscaled: 29.0000 (26.4854)  loss_ce_0_unscaled: 1.3065 (1.2701)  loss_bbox_0_unscaled: 0.6037 (0.5765)  loss_giou_0_unscaled: 0.7715 (0.7829)  cardinality_error_0_unscaled: 79.0000 (78.6196)  loss_ce_1_unscaled: 1.2236 (1.2289)  loss_bbox_1_unscaled: 0.6334 (0.6105)  loss_giou_1_unscaled: 0.8223 (0.8044)  cardinality_error_1_unscaled: 49.0000 (48.6386)  time: 0.1077  data: 0.0066  max mem: 594\n",
      "Test:  [ 590/4410]  eta: 0:06:48  class_error: 100.00  loss: 7.9192 (7.7503)  loss_ce: 2.7975 (2.8018)  loss_bbox: 3.6328 (3.2544)  loss_giou: 1.4916 (1.6940)  loss_ce_unscaled: 1.3988 (1.4009)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7266 (0.6509)  loss_giou_unscaled: 0.7458 (0.8470)  cardinality_error_unscaled: 29.0000 (26.5279)  loss_ce_0_unscaled: 1.3032 (1.2706)  loss_bbox_0_unscaled: 0.5880 (0.5761)  loss_giou_0_unscaled: 0.5744 (0.7793)  cardinality_error_0_unscaled: 79.0000 (78.6210)  loss_ce_1_unscaled: 1.2241 (1.2291)  loss_bbox_1_unscaled: 0.7164 (0.6126)  loss_giou_1_unscaled: 0.7300 (0.8028)  cardinality_error_1_unscaled: 49.0000 (48.6447)  time: 0.1083  data: 0.0065  max mem: 594\n",
      "Test:  [ 600/4410]  eta: 0:06:47  class_error: 100.00  loss: 7.8431 (7.7493)  loss_ce: 2.8133 (2.8021)  loss_bbox: 3.4319 (3.2551)  loss_giou: 1.4937 (1.6921)  loss_ce_unscaled: 1.4067 (1.4011)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6864 (0.6510)  loss_giou_unscaled: 0.7468 (0.8460)  cardinality_error_unscaled: 29.0000 (26.5674)  loss_ce_0_unscaled: 1.3073 (1.2713)  loss_bbox_0_unscaled: 0.5357 (0.5761)  loss_giou_0_unscaled: 0.6213 (0.7779)  cardinality_error_0_unscaled: 79.0000 (78.6256)  loss_ce_1_unscaled: 1.2717 (1.2298)  loss_bbox_1_unscaled: 0.6864 (0.6136)  loss_giou_1_unscaled: 0.7370 (0.8022)  cardinality_error_1_unscaled: 49.0000 (48.6489)  time: 0.1077  data: 0.0065  max mem: 594\n",
      "Test:  [ 610/4410]  eta: 0:06:46  class_error: 100.00  loss: 7.9083 (7.7557)  loss_ce: 2.8374 (2.8029)  loss_bbox: 3.4459 (3.2594)  loss_giou: 1.6449 (1.6935)  loss_ce_unscaled: 1.4187 (1.4014)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6892 (0.6519)  loss_giou_unscaled: 0.8225 (0.8467)  cardinality_error_unscaled: 28.0000 (26.5908)  loss_ce_0_unscaled: 1.2685 (1.2710)  loss_bbox_0_unscaled: 0.5711 (0.5765)  loss_giou_0_unscaled: 0.7190 (0.7773)  cardinality_error_0_unscaled: 78.0000 (78.6154)  loss_ce_1_unscaled: 1.2582 (1.2297)  loss_bbox_1_unscaled: 0.6719 (0.6138)  loss_giou_1_unscaled: 0.7815 (0.8021)  cardinality_error_1_unscaled: 48.0000 (48.6383)  time: 0.1072  data: 0.0064  max mem: 594\n",
      "Test:  [ 620/4410]  eta: 0:06:45  class_error: 100.00  loss: 7.9083 (7.7553)  loss_ce: 2.8319 (2.8030)  loss_bbox: 3.4000 (3.2583)  loss_giou: 1.6958 (1.6939)  loss_ce_unscaled: 1.4159 (1.4015)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6800 (0.6517)  loss_giou_unscaled: 0.8479 (0.8470)  cardinality_error_unscaled: 28.0000 (26.6135)  loss_ce_0_unscaled: 1.2551 (1.2707)  loss_bbox_0_unscaled: 0.5837 (0.5764)  loss_giou_0_unscaled: 0.7539 (0.7770)  cardinality_error_0_unscaled: 78.0000 (78.6055)  loss_ce_1_unscaled: 1.2148 (1.2297)  loss_bbox_1_unscaled: 0.6149 (0.6139)  loss_giou_1_unscaled: 0.8056 (0.8029)  cardinality_error_1_unscaled: 48.0000 (48.6280)  time: 0.1084  data: 0.0065  max mem: 594\n",
      "Test:  [ 630/4410]  eta: 0:06:44  class_error: 100.00  loss: 7.4602 (7.7510)  loss_ce: 2.8026 (2.8029)  loss_bbox: 2.9678 (3.2531)  loss_giou: 1.6540 (1.6949)  loss_ce_unscaled: 1.4013 (1.4015)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5936 (0.6506)  loss_giou_unscaled: 0.8270 (0.8474)  cardinality_error_unscaled: 28.0000 (26.6339)  loss_ce_0_unscaled: 1.2590 (1.2707)  loss_bbox_0_unscaled: 0.5541 (0.5760)  loss_giou_0_unscaled: 0.7803 (0.7775)  cardinality_error_0_unscaled: 78.0000 (78.5943)  loss_ce_1_unscaled: 1.2635 (1.2302)  loss_bbox_1_unscaled: 0.5749 (0.6131)  loss_giou_1_unscaled: 0.8190 (0.8038)  cardinality_error_1_unscaled: 48.0000 (48.6165)  time: 0.1096  data: 0.0067  max mem: 594\n",
      "Test:  [ 640/4410]  eta: 0:06:43  class_error: 100.00  loss: 7.3558 (7.7445)  loss_ce: 2.7846 (2.8024)  loss_bbox: 2.7808 (3.2460)  loss_giou: 1.8151 (1.6960)  loss_ce_unscaled: 1.3923 (1.4012)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5562 (0.6492)  loss_giou_unscaled: 0.9076 (0.8480)  cardinality_error_unscaled: 27.0000 (26.6022)  loss_ce_0_unscaled: 1.2705 (1.2707)  loss_bbox_0_unscaled: 0.5140 (0.5750)  loss_giou_0_unscaled: 0.8276 (0.7786)  cardinality_error_0_unscaled: 77.0000 (78.5694)  loss_ce_1_unscaled: 1.2593 (1.2305)  loss_bbox_1_unscaled: 0.5140 (0.6115)  loss_giou_1_unscaled: 0.8401 (0.8045)  cardinality_error_1_unscaled: 47.0000 (48.5913)  time: 0.1088  data: 0.0065  max mem: 594\n",
      "Test:  [ 650/4410]  eta: 0:06:42  class_error: 100.00  loss: 7.1969 (7.7360)  loss_ce: 2.7797 (2.8022)  loss_bbox: 2.7395 (3.2364)  loss_giou: 1.7268 (1.6974)  loss_ce_unscaled: 1.3898 (1.4011)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5479 (0.6473)  loss_giou_unscaled: 0.8634 (0.8487)  cardinality_error_unscaled: 25.0000 (26.5776)  loss_ce_0_unscaled: 1.2705 (1.2706)  loss_bbox_0_unscaled: 0.4823 (0.5736)  loss_giou_0_unscaled: 0.8428 (0.7798)  cardinality_error_0_unscaled: 77.0000 (78.5453)  loss_ce_1_unscaled: 1.2450 (1.2306)  loss_bbox_1_unscaled: 0.4826 (0.6095)  loss_giou_1_unscaled: 0.8428 (0.8053)  cardinality_error_1_unscaled: 47.0000 (48.5668)  time: 0.1081  data: 0.0062  max mem: 594\n",
      "Test:  [ 660/4410]  eta: 0:06:41  class_error: 100.00  loss: 7.1969 (7.7302)  loss_ce: 2.7873 (2.8021)  loss_bbox: 2.5954 (3.2296)  loss_giou: 1.7575 (1.6985)  loss_ce_unscaled: 1.3936 (1.4011)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5191 (0.6459)  loss_giou_unscaled: 0.8788 (0.8492)  cardinality_error_unscaled: 24.0000 (26.5113)  loss_ce_0_unscaled: 1.2659 (1.2705)  loss_bbox_0_unscaled: 0.4868 (0.5728)  loss_giou_0_unscaled: 0.8427 (0.7809)  cardinality_error_0_unscaled: 77.0000 (78.5219)  loss_ce_1_unscaled: 1.2402 (1.2308)  loss_bbox_1_unscaled: 0.4868 (0.6083)  loss_giou_1_unscaled: 0.8427 (0.8061)  cardinality_error_1_unscaled: 47.0000 (48.5431)  time: 0.1075  data: 0.0064  max mem: 594\n",
      "Test:  [ 670/4410]  eta: 0:06:40  class_error: 100.00  loss: 7.6034 (7.7305)  loss_ce: 2.8015 (2.8025)  loss_bbox: 2.9661 (3.2270)  loss_giou: 1.8137 (1.7011)  loss_ce_unscaled: 1.4008 (1.4012)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5932 (0.6454)  loss_giou_unscaled: 0.9069 (0.8505)  cardinality_error_unscaled: 23.0000 (26.4724)  loss_ce_0_unscaled: 1.2601 (1.2703)  loss_bbox_0_unscaled: 0.5076 (0.5719)  loss_giou_0_unscaled: 0.8416 (0.7821)  cardinality_error_0_unscaled: 77.0000 (78.4993)  loss_ce_1_unscaled: 1.2363 (1.2308)  loss_bbox_1_unscaled: 0.5275 (0.6072)  loss_giou_1_unscaled: 0.8613 (0.8074)  cardinality_error_1_unscaled: 47.0000 (48.5201)  time: 0.1072  data: 0.0064  max mem: 594\n",
      "Test:  [ 680/4410]  eta: 0:06:39  class_error: 100.00  loss: 7.7445 (7.7311)  loss_ce: 2.8171 (2.8029)  loss_bbox: 3.0787 (3.2241)  loss_giou: 1.8526 (1.7041)  loss_ce_unscaled: 1.4086 (1.4015)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6157 (0.6448)  loss_giou_unscaled: 0.9263 (0.8521)  cardinality_error_unscaled: 24.0000 (26.4493)  loss_ce_0_unscaled: 1.2511 (1.2701)  loss_bbox_0_unscaled: 0.5135 (0.5715)  loss_giou_0_unscaled: 0.8585 (0.7834)  cardinality_error_0_unscaled: 77.0000 (78.4772)  loss_ce_1_unscaled: 1.2312 (1.2307)  loss_bbox_1_unscaled: 0.5576 (0.6066)  loss_giou_1_unscaled: 0.8992 (0.8089)  cardinality_error_1_unscaled: 47.0000 (48.4978)  time: 0.1084  data: 0.0065  max mem: 594\n",
      "Test:  [ 690/4410]  eta: 0:06:38  class_error: 100.00  loss: 7.7625 (7.7337)  loss_ce: 2.8232 (2.8033)  loss_bbox: 3.0765 (3.2229)  loss_giou: 1.8651 (1.7075)  loss_ce_unscaled: 1.4116 (1.4017)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6153 (0.6446)  loss_giou_unscaled: 0.9325 (0.8538)  cardinality_error_unscaled: 26.0000 (26.4240)  loss_ce_0_unscaled: 1.2562 (1.2700)  loss_bbox_0_unscaled: 0.5216 (0.5708)  loss_giou_0_unscaled: 0.8892 (0.7850)  cardinality_error_0_unscaled: 77.0000 (78.4573)  loss_ce_1_unscaled: 1.2213 (1.2306)  loss_bbox_1_unscaled: 0.5576 (0.6057)  loss_giou_1_unscaled: 0.9159 (0.8106)  cardinality_error_1_unscaled: 47.0000 (48.4776)  time: 0.1090  data: 0.0067  max mem: 594\n",
      "Test:  [ 700/4410]  eta: 0:06:37  class_error: 100.00  loss: 8.0556 (7.7388)  loss_ce: 2.8069 (2.8033)  loss_bbox: 3.2215 (3.2242)  loss_giou: 1.9617 (1.7113)  loss_ce_unscaled: 1.4035 (1.4016)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6443 (0.6448)  loss_giou_unscaled: 0.9808 (0.8556)  cardinality_error_unscaled: 28.0000 (26.4465)  loss_ce_0_unscaled: 1.2643 (1.2700)  loss_bbox_0_unscaled: 0.5053 (0.5696)  loss_giou_0_unscaled: 0.8168 (0.7849)  cardinality_error_0_unscaled: 78.0000 (78.4508)  loss_ce_1_unscaled: 1.2045 (1.2299)  loss_bbox_1_unscaled: 0.5685 (0.6059)  loss_giou_1_unscaled: 0.9721 (0.8133)  cardinality_error_1_unscaled: 48.0000 (48.4708)  time: 0.1112  data: 0.0068  max mem: 594\n",
      "Test:  [ 710/4410]  eta: 0:06:36  class_error: 100.00  loss: 8.1562 (7.7438)  loss_ce: 2.8012 (2.8034)  loss_bbox: 3.3328 (3.2262)  loss_giou: 1.9383 (1.7142)  loss_ce_unscaled: 1.4006 (1.4017)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6666 (0.6452)  loss_giou_unscaled: 0.9691 (0.8571)  cardinality_error_unscaled: 28.0000 (26.4684)  loss_ce_0_unscaled: 1.2704 (1.2701)  loss_bbox_0_unscaled: 0.4765 (0.5685)  loss_giou_0_unscaled: 0.7713 (0.7849)  cardinality_error_0_unscaled: 78.0000 (78.4444)  loss_ce_1_unscaled: 1.1968 (1.2296)  loss_bbox_1_unscaled: 0.6062 (0.6061)  loss_giou_1_unscaled: 1.0177 (0.8164)  cardinality_error_1_unscaled: 48.0000 (48.4641)  time: 0.1116  data: 0.0066  max mem: 594\n",
      "Test:  [ 720/4410]  eta: 0:06:35  class_error: 100.00  loss: 8.1632 (7.7486)  loss_ce: 2.8040 (2.8034)  loss_bbox: 3.3108 (3.2275)  loss_giou: 1.8934 (1.7177)  loss_ce_unscaled: 1.4020 (1.4017)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6622 (0.6455)  loss_giou_unscaled: 0.9467 (0.8588)  cardinality_error_unscaled: 28.0000 (26.4854)  loss_ce_0_unscaled: 1.2704 (1.2700)  loss_bbox_0_unscaled: 0.4765 (0.5673)  loss_giou_0_unscaled: 0.7667 (0.7849)  cardinality_error_0_unscaled: 78.0000 (78.4355)  loss_ce_1_unscaled: 1.1945 (1.2291)  loss_bbox_1_unscaled: 0.6086 (0.6063)  loss_giou_1_unscaled: 1.0007 (0.8186)  cardinality_error_1_unscaled: 48.0000 (48.4563)  time: 0.1120  data: 0.0065  max mem: 594\n",
      "Test:  [ 730/4410]  eta: 0:06:34  class_error: 100.00  loss: 7.4021 (7.7411)  loss_ce: 2.8122 (2.8036)  loss_bbox: 2.6559 (3.2169)  loss_giou: 1.9017 (1.7205)  loss_ce_unscaled: 1.4061 (1.4018)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5312 (0.6434)  loss_giou_unscaled: 0.9509 (0.8603)  cardinality_error_unscaled: 27.0000 (26.4679)  loss_ce_0_unscaled: 1.2592 (1.2698)  loss_bbox_0_unscaled: 0.4715 (0.5659)  loss_giou_0_unscaled: 0.8537 (0.7871)  cardinality_error_0_unscaled: 77.0000 (78.4159)  loss_ce_1_unscaled: 1.2147 (1.2294)  loss_bbox_1_unscaled: 0.5132 (0.6044)  loss_giou_1_unscaled: 0.9475 (0.8204)  cardinality_error_1_unscaled: 47.0000 (48.4364)  time: 0.1114  data: 0.0067  max mem: 594\n",
      "Test:  [ 740/4410]  eta: 0:06:33  class_error: 100.00  loss: 7.1554 (7.7339)  loss_ce: 2.8341 (2.8042)  loss_bbox: 2.4101 (3.2067)  loss_giou: 1.9100 (1.7230)  loss_ce_unscaled: 1.4171 (1.4021)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4820 (0.6413)  loss_giou_unscaled: 0.9550 (0.8615)  cardinality_error_unscaled: 25.0000 (26.4494)  loss_ce_0_unscaled: 1.2524 (1.2696)  loss_bbox_0_unscaled: 0.4715 (0.5647)  loss_giou_0_unscaled: 0.9439 (0.7891)  cardinality_error_0_unscaled: 77.0000 (78.3968)  loss_ce_1_unscaled: 1.2571 (1.2298)  loss_bbox_1_unscaled: 0.4750 (0.6027)  loss_giou_1_unscaled: 0.9439 (0.8219)  cardinality_error_1_unscaled: 47.0000 (48.4170)  time: 0.1085  data: 0.0065  max mem: 594\n",
      "Test:  [ 750/4410]  eta: 0:06:32  class_error: 100.00  loss: 7.2736 (7.7272)  loss_ce: 2.8515 (2.8045)  loss_bbox: 2.5533 (3.1984)  loss_giou: 1.9222 (1.7243)  loss_ce_unscaled: 1.4258 (1.4023)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5107 (0.6397)  loss_giou_unscaled: 0.9611 (0.8622)  cardinality_error_unscaled: 26.0000 (26.4381)  loss_ce_0_unscaled: 1.2588 (1.2696)  loss_bbox_0_unscaled: 0.4809 (0.5638)  loss_giou_0_unscaled: 0.9528 (0.7908)  cardinality_error_0_unscaled: 77.0000 (78.3808)  loss_ce_1_unscaled: 1.2468 (1.2299)  loss_bbox_1_unscaled: 0.4826 (0.6014)  loss_giou_1_unscaled: 0.9528 (0.8230)  cardinality_error_1_unscaled: 47.0000 (48.4008)  time: 0.1072  data: 0.0063  max mem: 594\n",
      "Test:  [ 760/4410]  eta: 0:06:31  class_error: 100.00  loss: 6.7339 (7.7064)  loss_ce: 2.8589 (2.8058)  loss_bbox: 2.4347 (3.1843)  loss_giou: 1.3442 (1.7163)  loss_ce_unscaled: 1.4295 (1.4029)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4869 (0.6369)  loss_giou_unscaled: 0.6721 (0.8581)  cardinality_error_unscaled: 28.0000 (26.4691)  loss_ce_0_unscaled: 1.2897 (1.2705)  loss_bbox_0_unscaled: 0.4680 (0.5614)  loss_giou_0_unscaled: 0.5836 (0.7867)  cardinality_error_0_unscaled: 78.0000 (78.3876)  loss_ce_1_unscaled: 1.2632 (1.2308)  loss_bbox_1_unscaled: 0.4786 (0.5991)  loss_giou_1_unscaled: 0.6721 (0.8195)  cardinality_error_1_unscaled: 49.0000 (48.4087)  time: 0.1080  data: 0.0066  max mem: 594\n",
      "Test:  [ 770/4410]  eta: 0:06:30  class_error: 100.00  loss: 6.2087 (7.6881)  loss_ce: 2.8963 (2.8071)  loss_bbox: 2.1185 (3.1724)  loss_giou: 1.1304 (1.7086)  loss_ce_unscaled: 1.4482 (1.4036)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4237 (0.6345)  loss_giou_unscaled: 0.5652 (0.8543)  cardinality_error_unscaled: 29.0000 (26.4968)  loss_ce_0_unscaled: 1.3519 (1.2718)  loss_bbox_0_unscaled: 0.4159 (0.5595)  loss_giou_0_unscaled: 0.4651 (0.7829)  cardinality_error_0_unscaled: 79.0000 (78.3956)  loss_ce_1_unscaled: 1.3029 (1.2316)  loss_bbox_1_unscaled: 0.4237 (0.5972)  loss_giou_1_unscaled: 0.5652 (0.8161)  cardinality_error_1_unscaled: 49.0000 (48.4163)  time: 0.1099  data: 0.0068  max mem: 594\n",
      "Test:  [ 780/4410]  eta: 0:06:30  class_error: 100.00  loss: 6.1653 (7.6677)  loss_ce: 2.9052 (2.8084)  loss_bbox: 2.1185 (3.1586)  loss_giou: 1.1111 (1.7008)  loss_ce_unscaled: 1.4526 (1.4042)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4237 (0.6317)  loss_giou_unscaled: 0.5556 (0.8504)  cardinality_error_unscaled: 29.0000 (26.5262)  loss_ce_0_unscaled: 1.3614 (1.2728)  loss_bbox_0_unscaled: 0.4126 (0.5574)  loss_giou_0_unscaled: 0.4778 (0.7795)  cardinality_error_0_unscaled: 79.0000 (78.3995)  loss_ce_1_unscaled: 1.3013 (1.2324)  loss_bbox_1_unscaled: 0.4399 (0.5950)  loss_giou_1_unscaled: 0.5061 (0.8125)  cardinality_error_1_unscaled: 49.0000 (48.4225)  time: 0.1107  data: 0.0065  max mem: 594\n",
      "Test:  [ 790/4410]  eta: 0:06:29  class_error: 100.00  loss: 6.1566 (7.6481)  loss_ce: 2.8711 (2.8091)  loss_bbox: 2.0582 (3.1430)  loss_giou: 1.2487 (1.6960)  loss_ce_unscaled: 1.4355 (1.4045)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4116 (0.6286)  loss_giou_unscaled: 0.6244 (0.8480)  cardinality_error_unscaled: 28.0000 (26.5373)  loss_ce_0_unscaled: 1.3266 (1.2731)  loss_bbox_0_unscaled: 0.3926 (0.5549)  loss_giou_0_unscaled: 0.5703 (0.7779)  cardinality_error_0_unscaled: 78.0000 (78.3919)  loss_ce_1_unscaled: 1.2983 (1.2332)  loss_bbox_1_unscaled: 0.4116 (0.5922)  loss_giou_1_unscaled: 0.5757 (0.8104)  cardinality_error_1_unscaled: 48.0000 (48.4172)  time: 0.1114  data: 0.0065  max mem: 594\n",
      "Test:  [ 800/4410]  eta: 0:06:28  class_error: 100.00  loss: 6.0218 (7.6275)  loss_ce: 2.8421 (2.8095)  loss_bbox: 1.8683 (3.1266)  loss_giou: 1.3287 (1.6914)  loss_ce_unscaled: 1.4211 (1.4047)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3737 (0.6253)  loss_giou_unscaled: 0.6644 (0.8457)  cardinality_error_unscaled: 28.0000 (26.5518)  loss_ce_0_unscaled: 1.2944 (1.2734)  loss_bbox_0_unscaled: 0.3474 (0.5522)  loss_giou_0_unscaled: 0.6512 (0.7762)  cardinality_error_0_unscaled: 78.0000 (78.3858)  loss_ce_1_unscaled: 1.3015 (1.2342)  loss_bbox_1_unscaled: 0.3474 (0.5890)  loss_giou_1_unscaled: 0.6512 (0.8082)  cardinality_error_1_unscaled: 48.0000 (48.4120)  time: 0.1095  data: 0.0068  max mem: 594\n",
      "Test:  [ 810/4410]  eta: 0:06:27  class_error: 100.00  loss: 6.0068 (7.6096)  loss_ce: 2.8382 (2.8099)  loss_bbox: 1.8587 (3.1127)  loss_giou: 1.3191 (1.6869)  loss_ce_unscaled: 1.4191 (1.4050)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3717 (0.6225)  loss_giou_unscaled: 0.6595 (0.8435)  cardinality_error_unscaled: 28.0000 (26.5697)  loss_ce_0_unscaled: 1.3068 (1.2739)  loss_bbox_0_unscaled: 0.3474 (0.5501)  loss_giou_0_unscaled: 0.6015 (0.7740)  cardinality_error_0_unscaled: 78.0000 (78.3798)  loss_ce_1_unscaled: 1.3002 (1.2349)  loss_bbox_1_unscaled: 0.3474 (0.5865)  loss_giou_1_unscaled: 0.6144 (0.8062)  cardinality_error_1_unscaled: 48.0000 (48.4081)  time: 0.1077  data: 0.0068  max mem: 594\n",
      "Test:  [ 820/4410]  eta: 0:06:25  class_error: 100.00  loss: 7.5210 (7.6181)  loss_ce: 2.8381 (2.8102)  loss_bbox: 3.0114 (3.1197)  loss_giou: 1.6183 (1.6882)  loss_ce_unscaled: 1.4190 (1.4051)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6023 (0.6239)  loss_giou_unscaled: 0.8091 (0.8441)  cardinality_error_unscaled: 28.0000 (26.5956)  loss_ce_0_unscaled: 1.3125 (1.2743)  loss_bbox_0_unscaled: 0.4302 (0.5494)  loss_giou_0_unscaled: 0.5968 (0.7714)  cardinality_error_0_unscaled: 79.0000 (78.3873)  loss_ce_1_unscaled: 1.2630 (1.2349)  loss_bbox_1_unscaled: 0.4351 (0.5866)  loss_giou_1_unscaled: 0.6015 (0.8037)  cardinality_error_1_unscaled: 49.0000 (48.4153)  time: 0.1079  data: 0.0066  max mem: 594\n",
      "Test:  [ 830/4410]  eta: 0:06:24  class_error: 100.00  loss: 8.0788 (7.6214)  loss_ce: 2.8342 (2.8107)  loss_bbox: 3.4868 (3.1227)  loss_giou: 1.7609 (1.6880)  loss_ce_unscaled: 1.4171 (1.4053)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6974 (0.6245)  loss_giou_unscaled: 0.8805 (0.8440)  cardinality_error_unscaled: 29.0000 (26.6185)  loss_ce_0_unscaled: 1.3199 (1.2750)  loss_bbox_0_unscaled: 0.4629 (0.5490)  loss_giou_0_unscaled: 0.6149 (0.7695)  cardinality_error_0_unscaled: 79.0000 (78.3947)  loss_ce_1_unscaled: 1.2514 (1.2352)  loss_bbox_1_unscaled: 0.6718 (0.5878)  loss_giou_1_unscaled: 0.6142 (0.8019)  cardinality_error_1_unscaled: 49.0000 (48.4224)  time: 0.1080  data: 0.0066  max mem: 594\n",
      "Test:  [ 840/4410]  eta: 0:06:23  class_error: 100.00  loss: 7.8852 (7.6231)  loss_ce: 2.8560 (2.8114)  loss_bbox: 3.2555 (3.1234)  loss_giou: 1.6935 (1.6883)  loss_ce_unscaled: 1.4280 (1.4057)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6511 (0.6247)  loss_giou_unscaled: 0.8467 (0.8441)  cardinality_error_unscaled: 29.0000 (26.6457)  loss_ce_0_unscaled: 1.3199 (1.2754)  loss_bbox_0_unscaled: 0.4651 (0.5482)  loss_giou_0_unscaled: 0.6269 (0.7680)  cardinality_error_0_unscaled: 79.0000 (78.4019)  loss_ce_1_unscaled: 1.2553 (1.2355)  loss_bbox_1_unscaled: 0.6451 (0.5876)  loss_giou_1_unscaled: 0.6800 (0.8016)  cardinality_error_1_unscaled: 49.0000 (48.4293)  time: 0.1094  data: 0.0065  max mem: 594\n",
      "Test:  [ 850/4410]  eta: 0:06:22  class_error: 100.00  loss: 7.5194 (7.6224)  loss_ce: 2.8889 (2.8123)  loss_bbox: 2.7675 (3.1145)  loss_giou: 2.0403 (1.6956)  loss_ce_unscaled: 1.4444 (1.4062)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5535 (0.6229)  loss_giou_unscaled: 1.0202 (0.8478)  cardinality_error_unscaled: 29.0000 (26.6722)  loss_ce_0_unscaled: 1.2961 (1.2756)  loss_bbox_0_unscaled: 0.4545 (0.5471)  loss_giou_0_unscaled: 0.9737 (0.7725)  cardinality_error_0_unscaled: 79.0000 (78.4066)  loss_ce_1_unscaled: 1.2164 (1.2351)  loss_bbox_1_unscaled: 0.4866 (0.5860)  loss_giou_1_unscaled: 1.0202 (0.8057)  cardinality_error_1_unscaled: 49.0000 (48.4360)  time: 0.1103  data: 0.0065  max mem: 594\n",
      "Test:  [ 860/4410]  eta: 0:06:21  class_error: 100.00  loss: 7.3994 (7.6203)  loss_ce: 2.9013 (2.8134)  loss_bbox: 2.1659 (3.1052)  loss_giou: 2.2614 (1.7017)  loss_ce_unscaled: 1.4506 (1.4067)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4332 (0.6210)  loss_giou_unscaled: 1.1307 (0.8509)  cardinality_error_unscaled: 29.0000 (26.6992)  loss_ce_0_unscaled: 1.2913 (1.2757)  loss_bbox_0_unscaled: 0.4332 (0.5458)  loss_giou_0_unscaled: 1.1307 (0.7766)  cardinality_error_0_unscaled: 79.0000 (78.4111)  loss_ce_1_unscaled: 1.1956 (1.2345)  loss_bbox_1_unscaled: 0.4332 (0.5843)  loss_giou_1_unscaled: 1.1307 (0.8094)  cardinality_error_1_unscaled: 49.0000 (48.4425)  time: 0.1093  data: 0.0064  max mem: 594\n",
      "Test:  [ 870/4410]  eta: 0:06:21  class_error: 100.00  loss: 7.3421 (7.6210)  loss_ce: 2.9076 (2.8145)  loss_bbox: 2.1846 (3.0997)  loss_giou: 2.2365 (1.7068)  loss_ce_unscaled: 1.4538 (1.4072)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4369 (0.6199)  loss_giou_unscaled: 1.1183 (0.8534)  cardinality_error_unscaled: 29.0000 (26.7233)  loss_ce_0_unscaled: 1.2837 (1.2760)  loss_bbox_0_unscaled: 0.4369 (0.5449)  loss_giou_0_unscaled: 1.1054 (0.7797)  cardinality_error_0_unscaled: 79.0000 (78.4145)  loss_ce_1_unscaled: 1.1908 (1.2341)  loss_bbox_1_unscaled: 0.4369 (0.5830)  loss_giou_1_unscaled: 1.1183 (0.8123)  cardinality_error_1_unscaled: 49.0000 (48.4489)  time: 0.1101  data: 0.0064  max mem: 594\n",
      "Test:  [ 880/4410]  eta: 0:06:20  class_error: 100.00  loss: 6.7004 (7.6008)  loss_ce: 2.9083 (2.8155)  loss_bbox: 2.1024 (3.0862)  loss_giou: 1.2097 (1.6991)  loss_ce_unscaled: 1.4542 (1.4078)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4205 (0.6172)  loss_giou_unscaled: 0.6048 (0.8495)  cardinality_error_unscaled: 29.0000 (26.7469)  loss_ce_0_unscaled: 1.3013 (1.2766)  loss_bbox_0_unscaled: 0.4205 (0.5431)  loss_giou_0_unscaled: 0.5547 (0.7763)  cardinality_error_0_unscaled: 79.0000 (78.4200)  loss_ce_1_unscaled: 1.2283 (1.2344)  loss_bbox_1_unscaled: 0.4205 (0.5808)  loss_giou_1_unscaled: 0.5457 (0.8086)  cardinality_error_1_unscaled: 49.0000 (48.4552)  time: 0.1113  data: 0.0069  max mem: 594\n",
      "Test:  [ 890/4410]  eta: 0:06:19  class_error: 100.00  loss: 5.6622 (7.5776)  loss_ce: 2.8956 (2.8163)  loss_bbox: 1.8054 (3.0703)  loss_giou: 0.9898 (1.6909)  loss_ce_unscaled: 1.4478 (1.4082)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3611 (0.6141)  loss_giou_unscaled: 0.4949 (0.8455)  cardinality_error_unscaled: 29.0000 (26.7677)  loss_ce_0_unscaled: 1.3369 (1.2772)  loss_bbox_0_unscaled: 0.3611 (0.5408)  loss_giou_0_unscaled: 0.4715 (0.7731)  cardinality_error_0_unscaled: 79.0000 (78.4242)  loss_ce_1_unscaled: 1.2647 (1.2349)  loss_bbox_1_unscaled: 0.3611 (0.5780)  loss_giou_1_unscaled: 0.4715 (0.8050)  cardinality_error_1_unscaled: 49.0000 (48.4613)  time: 0.1105  data: 0.0066  max mem: 594\n",
      "Test:  [ 900/4410]  eta: 0:06:18  class_error: 100.00  loss: 5.8874 (7.5629)  loss_ce: 2.8956 (2.8171)  loss_bbox: 1.8894 (3.0603)  loss_giou: 1.0586 (1.6855)  loss_ce_unscaled: 1.4478 (1.4086)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3779 (0.6121)  loss_giou_unscaled: 0.5293 (0.8428)  cardinality_error_unscaled: 29.0000 (26.7902)  loss_ce_0_unscaled: 1.3369 (1.2779)  loss_bbox_0_unscaled: 0.3611 (0.5390)  loss_giou_0_unscaled: 0.4993 (0.7705)  cardinality_error_0_unscaled: 79.0000 (78.4295)  loss_ce_1_unscaled: 1.2664 (1.2351)  loss_bbox_1_unscaled: 0.3611 (0.5758)  loss_giou_1_unscaled: 0.4993 (0.8021)  cardinality_error_1_unscaled: 49.0000 (48.4661)  time: 0.1118  data: 0.0064  max mem: 594\n",
      "Test:  [ 910/4410]  eta: 0:06:17  class_error: 100.00  loss: 6.8702 (7.5639)  loss_ce: 2.8359 (2.8169)  loss_bbox: 2.3587 (3.0578)  loss_giou: 1.8128 (1.6892)  loss_ce_unscaled: 1.4179 (1.4085)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4717 (0.6116)  loss_giou_unscaled: 0.9064 (0.8446)  cardinality_error_unscaled: 28.0000 (26.8035)  loss_ce_0_unscaled: 1.2855 (1.2778)  loss_bbox_0_unscaled: 0.4287 (0.5386)  loss_giou_0_unscaled: 0.7815 (0.7717)  cardinality_error_0_unscaled: 78.0000 (78.4226)  loss_ce_1_unscaled: 1.2522 (1.2352)  loss_bbox_1_unscaled: 0.4519 (0.5758)  loss_giou_1_unscaled: 0.8394 (0.8037)  cardinality_error_1_unscaled: 48.0000 (48.4610)  time: 0.1132  data: 0.0065  max mem: 594\n",
      "Test:  [ 920/4410]  eta: 0:06:16  class_error: 100.00  loss: 7.7456 (7.5660)  loss_ce: 2.8164 (2.8172)  loss_bbox: 2.7786 (3.0559)  loss_giou: 1.9451 (1.6929)  loss_ce_unscaled: 1.4082 (1.4086)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5557 (0.6112)  loss_giou_unscaled: 0.9725 (0.8465)  cardinality_error_unscaled: 28.0000 (26.8154)  loss_ce_0_unscaled: 1.2740 (1.2777)  loss_bbox_0_unscaled: 0.4911 (0.5379)  loss_giou_0_unscaled: 0.8600 (0.7730)  cardinality_error_0_unscaled: 78.0000 (78.4148)  loss_ce_1_unscaled: 1.2317 (1.2352)  loss_bbox_1_unscaled: 0.5557 (0.5756)  loss_giou_1_unscaled: 0.9563 (0.8057)  cardinality_error_1_unscaled: 48.0000 (48.4560)  time: 0.1119  data: 0.0065  max mem: 594\n",
      "Test:  [ 930/4410]  eta: 0:06:15  class_error: 100.00  loss: 7.4834 (7.5634)  loss_ce: 2.8164 (2.8172)  loss_bbox: 2.7047 (3.0505)  loss_giou: 1.9499 (1.6957)  loss_ce_unscaled: 1.4082 (1.4086)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5409 (0.6101)  loss_giou_unscaled: 0.9750 (0.8479)  cardinality_error_unscaled: 28.0000 (26.8271)  loss_ce_0_unscaled: 1.2711 (1.2778)  loss_bbox_0_unscaled: 0.4640 (0.5370)  loss_giou_0_unscaled: 0.8582 (0.7739)  cardinality_error_0_unscaled: 78.0000 (78.4092)  loss_ce_1_unscaled: 1.2249 (1.2352)  loss_bbox_1_unscaled: 0.5183 (0.5749)  loss_giou_1_unscaled: 0.9633 (0.8070)  cardinality_error_1_unscaled: 48.0000 (48.4522)  time: 0.1105  data: 0.0069  max mem: 594\n",
      "Test:  [ 940/4410]  eta: 0:06:14  class_error: 100.00  loss: 7.7270 (7.5792)  loss_ce: 2.8477 (2.8179)  loss_bbox: 3.2470 (3.0653)  loss_giou: 1.8037 (1.6960)  loss_ce_unscaled: 1.4238 (1.4089)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6494 (0.6131)  loss_giou_unscaled: 0.9019 (0.8480)  cardinality_error_unscaled: 29.0000 (26.8502)  loss_ce_0_unscaled: 1.2949 (1.2781)  loss_bbox_0_unscaled: 0.5106 (0.5380)  loss_giou_0_unscaled: 0.7482 (0.7727)  cardinality_error_0_unscaled: 79.0000 (78.4155)  loss_ce_1_unscaled: 1.2319 (1.2351)  loss_bbox_1_unscaled: 0.6536 (0.5777)  loss_giou_1_unscaled: 0.8637 (0.8071)  cardinality_error_1_unscaled: 49.0000 (48.4580)  time: 0.1134  data: 0.0069  max mem: 594\n",
      "Test:  [ 950/4410]  eta: 0:06:13  class_error: 100.00  loss: 9.0256 (7.5937)  loss_ce: 2.8773 (2.8183)  loss_bbox: 4.3675 (3.0784)  loss_giou: 1.7275 (1.6970)  loss_ce_unscaled: 1.4387 (1.4091)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8735 (0.6157)  loss_giou_unscaled: 0.8637 (0.8485)  cardinality_error_unscaled: 29.0000 (26.8728)  loss_ce_0_unscaled: 1.3076 (1.2784)  loss_bbox_0_unscaled: 0.6819 (0.5389)  loss_giou_0_unscaled: 0.6853 (0.7720)  cardinality_error_0_unscaled: 79.0000 (78.4196)  loss_ce_1_unscaled: 1.2190 (1.2348)  loss_bbox_1_unscaled: 0.8270 (0.5802)  loss_giou_1_unscaled: 0.8637 (0.8077)  cardinality_error_1_unscaled: 49.0000 (48.4637)  time: 0.1134  data: 0.0067  max mem: 594\n",
      "Test:  [ 960/4410]  eta: 0:06:12  class_error: 100.00  loss: 8.7558 (7.6027)  loss_ce: 2.8542 (2.8186)  loss_bbox: 4.0625 (3.0856)  loss_giou: 1.8055 (1.6985)  loss_ce_unscaled: 1.4271 (1.4093)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8125 (0.6171)  loss_giou_unscaled: 0.9028 (0.8493)  cardinality_error_unscaled: 29.0000 (26.8949)  loss_ce_0_unscaled: 1.3026 (1.2786)  loss_bbox_0_unscaled: 0.7121 (0.5403)  loss_giou_0_unscaled: 0.7530 (0.7710)  cardinality_error_0_unscaled: 79.0000 (78.4256)  loss_ce_1_unscaled: 1.2167 (1.2347)  loss_bbox_1_unscaled: 0.8036 (0.5819)  loss_giou_1_unscaled: 0.8884 (0.8081)  cardinality_error_1_unscaled: 49.0000 (48.4693)  time: 0.1113  data: 0.0068  max mem: 594\n",
      "Test:  [ 970/4410]  eta: 0:06:11  class_error: 100.00  loss: 8.6919 (7.6135)  loss_ce: 2.8565 (2.8191)  loss_bbox: 3.9667 (3.0963)  loss_giou: 1.7414 (1.6981)  loss_ce_unscaled: 1.4282 (1.4096)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7933 (0.6193)  loss_giou_unscaled: 0.8707 (0.8491)  cardinality_error_unscaled: 29.0000 (26.9166)  loss_ce_0_unscaled: 1.2960 (1.2788)  loss_bbox_0_unscaled: 0.6303 (0.5410)  loss_giou_0_unscaled: 0.7556 (0.7709)  cardinality_error_0_unscaled: 79.0000 (78.4305)  loss_ce_1_unscaled: 1.1988 (1.2341)  loss_bbox_1_unscaled: 0.7933 (0.5838)  loss_giou_1_unscaled: 0.8573 (0.8082)  cardinality_error_1_unscaled: 49.0000 (48.4748)  time: 0.1125  data: 0.0067  max mem: 594\n",
      "Test:  [ 980/4410]  eta: 0:06:10  class_error: 100.00  loss: 9.0478 (7.6270)  loss_ce: 2.8558 (2.8193)  loss_bbox: 4.4226 (3.1092)  loss_giou: 1.7041 (1.6985)  loss_ce_unscaled: 1.4279 (1.4096)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8845 (0.6218)  loss_giou_unscaled: 0.8520 (0.8493)  cardinality_error_unscaled: 29.0000 (26.9378)  loss_ce_0_unscaled: 1.3046 (1.2791)  loss_bbox_0_unscaled: 0.6303 (0.5422)  loss_giou_0_unscaled: 0.6736 (0.7695)  cardinality_error_0_unscaled: 79.0000 (78.4353)  loss_ce_1_unscaled: 1.1920 (1.2337)  loss_bbox_1_unscaled: 0.8369 (0.5863)  loss_giou_1_unscaled: 0.8520 (0.8084)  cardinality_error_1_unscaled: 49.0000 (48.4801)  time: 0.1124  data: 0.0065  max mem: 594\n",
      "Test:  [ 990/4410]  eta: 0:06:10  class_error: 100.00  loss: 9.1709 (7.6376)  loss_ce: 2.8365 (2.8197)  loss_bbox: 4.7047 (3.1207)  loss_giou: 1.6706 (1.6971)  loss_ce_unscaled: 1.4182 (1.4099)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.9409 (0.6241)  loss_giou_unscaled: 0.8353 (0.8486)  cardinality_error_unscaled: 29.0000 (26.9586)  loss_ce_0_unscaled: 1.2873 (1.2791)  loss_bbox_0_unscaled: 0.6320 (0.5430)  loss_giou_0_unscaled: 0.6741 (0.7693)  cardinality_error_0_unscaled: 79.0000 (78.4410)  loss_ce_1_unscaled: 1.1826 (1.2331)  loss_bbox_1_unscaled: 0.8241 (0.5888)  loss_giou_1_unscaled: 0.8461 (0.8085)  cardinality_error_1_unscaled: 49.0000 (48.4854)  time: 0.1142  data: 0.0065  max mem: 594\n",
      "Test:  [1000/4410]  eta: 0:06:09  class_error: 100.00  loss: 8.0655 (7.6335)  loss_ce: 2.8568 (2.8201)  loss_bbox: 3.6686 (3.1199)  loss_giou: 1.4571 (1.6936)  loss_ce_unscaled: 1.4284 (1.4100)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7337 (0.6240)  loss_giou_unscaled: 0.7285 (0.8468)  cardinality_error_unscaled: 29.0000 (26.9780)  loss_ce_0_unscaled: 1.3048 (1.2796)  loss_bbox_0_unscaled: 0.5537 (0.5426)  loss_giou_0_unscaled: 0.6786 (0.7678)  cardinality_error_0_unscaled: 79.0000 (78.4466)  loss_ce_1_unscaled: 1.2183 (1.2334)  loss_bbox_1_unscaled: 0.6054 (0.5880)  loss_giou_1_unscaled: 0.6792 (0.8065)  cardinality_error_1_unscaled: 49.0000 (48.4905)  time: 0.1138  data: 0.0067  max mem: 594\n",
      "Test:  [1010/4410]  eta: 0:06:08  class_error: 100.00  loss: 7.2810 (7.6294)  loss_ce: 2.8593 (2.8203)  loss_bbox: 3.0260 (3.1187)  loss_giou: 1.3370 (1.6904)  loss_ce_unscaled: 1.4296 (1.4102)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6052 (0.6237)  loss_giou_unscaled: 0.6685 (0.8452)  cardinality_error_unscaled: 29.0000 (26.9980)  loss_ce_0_unscaled: 1.3292 (1.2802)  loss_bbox_0_unscaled: 0.4903 (0.5420)  loss_giou_0_unscaled: 0.6317 (0.7662)  cardinality_error_0_unscaled: 79.0000 (78.4520)  loss_ce_1_unscaled: 1.2610 (1.2338)  loss_bbox_1_unscaled: 0.5042 (0.5872)  loss_giou_1_unscaled: 0.6317 (0.8047)  cardinality_error_1_unscaled: 49.0000 (48.4955)  time: 0.1116  data: 0.0066  max mem: 594\n",
      "Test:  [1020/4410]  eta: 0:06:07  class_error: 100.00  loss: 7.6442 (7.6274)  loss_ce: 2.8593 (2.8206)  loss_bbox: 3.2796 (3.1187)  loss_giou: 1.4015 (1.6881)  loss_ce_unscaled: 1.4296 (1.4103)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6559 (0.6237)  loss_giou_unscaled: 0.7007 (0.8441)  cardinality_error_unscaled: 29.0000 (27.0167)  loss_ce_0_unscaled: 1.3307 (1.2807)  loss_bbox_0_unscaled: 0.4594 (0.5414)  loss_giou_0_unscaled: 0.6297 (0.7650)  cardinality_error_0_unscaled: 79.0000 (78.4564)  loss_ce_1_unscaled: 1.2803 (1.2343)  loss_bbox_1_unscaled: 0.4804 (0.5862)  loss_giou_1_unscaled: 0.6317 (0.8032)  cardinality_error_1_unscaled: 49.0000 (48.4995)  time: 0.1135  data: 0.0064  max mem: 594\n",
      "Test:  [1030/4410]  eta: 0:06:06  class_error: 100.00  loss: 7.2802 (7.6234)  loss_ce: 2.7760 (2.8199)  loss_bbox: 2.5179 (3.1116)  loss_giou: 1.8871 (1.6919)  loss_ce_unscaled: 1.3880 (1.4100)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5036 (0.6223)  loss_giou_unscaled: 0.9435 (0.8459)  cardinality_error_unscaled: 28.0000 (27.0204)  loss_ce_0_unscaled: 1.2930 (1.2805)  loss_bbox_0_unscaled: 0.4334 (0.5403)  loss_giou_0_unscaled: 0.9181 (0.7675)  cardinality_error_0_unscaled: 78.0000 (78.4510)  loss_ce_1_unscaled: 1.2578 (1.2344)  loss_bbox_1_unscaled: 0.4334 (0.5847)  loss_giou_1_unscaled: 0.9181 (0.8052)  cardinality_error_1_unscaled: 48.0000 (48.4947)  time: 0.1160  data: 0.0065  max mem: 594\n",
      "Test:  [1040/4410]  eta: 0:06:05  class_error: 100.00  loss: 7.0242 (7.6157)  loss_ce: 2.7535 (2.8193)  loss_bbox: 2.1668 (3.1016)  loss_giou: 2.0375 (1.6947)  loss_ce_unscaled: 1.3767 (1.4097)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4334 (0.6203)  loss_giou_unscaled: 1.0188 (0.8474)  cardinality_error_unscaled: 27.0000 (27.0125)  loss_ce_0_unscaled: 1.2598 (1.2803)  loss_bbox_0_unscaled: 0.3995 (0.5387)  loss_giou_0_unscaled: 0.9915 (0.7695)  cardinality_error_0_unscaled: 78.0000 (78.4467)  loss_ce_1_unscaled: 1.2496 (1.2346)  loss_bbox_1_unscaled: 0.4148 (0.5828)  loss_giou_1_unscaled: 1.0056 (0.8070)  cardinality_error_1_unscaled: 48.0000 (48.4899)  time: 0.1138  data: 0.0065  max mem: 594\n",
      "Test:  [1050/4410]  eta: 0:06:04  class_error: 100.00  loss: 6.9844 (7.6101)  loss_ce: 2.7507 (2.8187)  loss_bbox: 2.1534 (3.0939)  loss_giou: 2.0393 (1.6975)  loss_ce_unscaled: 1.3753 (1.4094)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4307 (0.6188)  loss_giou_unscaled: 1.0197 (0.8488)  cardinality_error_unscaled: 27.0000 (27.0143)  loss_ce_0_unscaled: 1.2625 (1.2802)  loss_bbox_0_unscaled: 0.3859 (0.5375)  loss_giou_0_unscaled: 0.9823 (0.7716)  cardinality_error_0_unscaled: 78.0000 (78.4434)  loss_ce_1_unscaled: 1.2557 (1.2347)  loss_bbox_1_unscaled: 0.3908 (0.5813)  loss_giou_1_unscaled: 1.0056 (0.8087)  cardinality_error_1_unscaled: 48.0000 (48.4862)  time: 0.1122  data: 0.0066  max mem: 594\n",
      "Test:  [1060/4410]  eta: 0:06:03  class_error: 100.00  loss: 7.1176 (7.6062)  loss_ce: 2.7683 (2.8185)  loss_bbox: 2.4040 (3.0928)  loss_giou: 1.6798 (1.6949)  loss_ce_unscaled: 1.3842 (1.4093)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4808 (0.6186)  loss_giou_unscaled: 0.8399 (0.8474)  cardinality_error_unscaled: 28.0000 (27.0273)  loss_ce_0_unscaled: 1.2738 (1.2803)  loss_bbox_0_unscaled: 0.4403 (0.5371)  loss_giou_0_unscaled: 0.7293 (0.7706)  cardinality_error_0_unscaled: 78.0000 (78.4477)  loss_ce_1_unscaled: 1.2612 (1.2352)  loss_bbox_1_unscaled: 0.4525 (0.5807)  loss_giou_1_unscaled: 0.7636 (0.8074)  cardinality_error_1_unscaled: 49.0000 (48.4910)  time: 0.1132  data: 0.0067  max mem: 594\n",
      "Test:  [1070/4410]  eta: 0:06:02  class_error: 100.00  loss: 7.2148 (7.6018)  loss_ce: 2.8102 (2.8186)  loss_bbox: 2.8690 (3.0914)  loss_giou: 1.4303 (1.6918)  loss_ce_unscaled: 1.4051 (1.4093)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5738 (0.6183)  loss_giou_unscaled: 0.7151 (0.8459)  cardinality_error_unscaled: 29.0000 (27.0420)  loss_ce_0_unscaled: 1.3032 (1.2806)  loss_bbox_0_unscaled: 0.4444 (0.5364)  loss_giou_0_unscaled: 0.6577 (0.7691)  cardinality_error_0_unscaled: 79.0000 (78.4519)  loss_ce_1_unscaled: 1.2858 (1.2357)  loss_bbox_1_unscaled: 0.4456 (0.5796)  loss_giou_1_unscaled: 0.6577 (0.8056)  cardinality_error_1_unscaled: 49.0000 (48.4958)  time: 0.1108  data: 0.0065  max mem: 594\n",
      "Test:  [1080/4410]  eta: 0:06:01  class_error: 100.00  loss: 7.1921 (7.5962)  loss_ce: 2.8148 (2.8185)  loss_bbox: 2.8606 (3.0882)  loss_giou: 1.3900 (1.6895)  loss_ce_unscaled: 1.4074 (1.4092)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5721 (0.6176)  loss_giou_unscaled: 0.6950 (0.8447)  cardinality_error_unscaled: 29.0000 (27.0546)  loss_ce_0_unscaled: 1.2955 (1.2806)  loss_bbox_0_unscaled: 0.4639 (0.5360)  loss_giou_0_unscaled: 0.6577 (0.7684)  cardinality_error_0_unscaled: 79.0000 (78.4561)  loss_ce_1_unscaled: 1.2975 (1.2363)  loss_bbox_1_unscaled: 0.4639 (0.5789)  loss_giou_1_unscaled: 0.6577 (0.8044)  cardinality_error_1_unscaled: 49.0000 (48.4995)  time: 0.1116  data: 0.0064  max mem: 594\n",
      "Test:  [1090/4410]  eta: 0:06:00  class_error: 100.00  loss: 6.8325 (7.5874)  loss_ce: 2.8221 (2.8187)  loss_bbox: 2.2534 (3.0780)  loss_giou: 1.5909 (1.6907)  loss_ce_unscaled: 1.4110 (1.4094)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4507 (0.6156)  loss_giou_unscaled: 0.7955 (0.8454)  cardinality_error_unscaled: 28.0000 (27.0614)  loss_ce_0_unscaled: 1.2551 (1.2803)  loss_bbox_0_unscaled: 0.4303 (0.5345)  loss_giou_0_unscaled: 0.7821 (0.7694)  cardinality_error_0_unscaled: 78.0000 (78.4510)  loss_ce_1_unscaled: 1.2939 (1.2368)  loss_bbox_1_unscaled: 0.4303 (0.5771)  loss_giou_1_unscaled: 0.7821 (0.8051)  cardinality_error_1_unscaled: 48.0000 (48.4950)  time: 0.1123  data: 0.0066  max mem: 594\n",
      "Test:  [1100/4410]  eta: 0:05:59  class_error: 100.00  loss: 6.5357 (7.5777)  loss_ce: 2.8551 (2.8190)  loss_bbox: 1.8864 (3.0670)  loss_giou: 1.8374 (1.6917)  loss_ce_unscaled: 1.4275 (1.4095)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3773 (0.6134)  loss_giou_unscaled: 0.9187 (0.8458)  cardinality_error_unscaled: 28.0000 (27.0672)  loss_ce_0_unscaled: 1.2424 (1.2801)  loss_bbox_0_unscaled: 0.3717 (0.5329)  loss_giou_0_unscaled: 0.8830 (0.7703)  cardinality_error_0_unscaled: 78.0000 (78.4469)  loss_ce_1_unscaled: 1.2896 (1.2374)  loss_bbox_1_unscaled: 0.3717 (0.5751)  loss_giou_1_unscaled: 0.8830 (0.8057)  cardinality_error_1_unscaled: 48.0000 (48.4905)  time: 0.1102  data: 0.0066  max mem: 594\n",
      "Test:  [1110/4410]  eta: 0:05:58  class_error: 100.00  loss: 6.5251 (7.5684)  loss_ce: 2.8433 (2.8192)  loss_bbox: 1.8104 (3.0566)  loss_giou: 1.8237 (1.6926)  loss_ce_unscaled: 1.4217 (1.4096)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3621 (0.6113)  loss_giou_unscaled: 0.9119 (0.8463)  cardinality_error_unscaled: 28.0000 (27.0765)  loss_ce_0_unscaled: 1.2532 (1.2799)  loss_bbox_0_unscaled: 0.3307 (0.5315)  loss_giou_0_unscaled: 0.8956 (0.7713)  cardinality_error_0_unscaled: 78.0000 (78.4437)  loss_ce_1_unscaled: 1.2960 (1.2379)  loss_bbox_1_unscaled: 0.3307 (0.5732)  loss_giou_1_unscaled: 0.8956 (0.8063)  cardinality_error_1_unscaled: 48.0000 (48.4869)  time: 0.1116  data: 0.0064  max mem: 594\n",
      "Test:  [1120/4410]  eta: 0:05:57  class_error: 100.00  loss: 7.3161 (7.5718)  loss_ce: 2.8308 (2.8192)  loss_bbox: 2.8494 (3.0600)  loss_giou: 1.7911 (1.6926)  loss_ce_unscaled: 1.4154 (1.4096)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5699 (0.6120)  loss_giou_unscaled: 0.8956 (0.8463)  cardinality_error_unscaled: 28.0000 (27.0919)  loss_ce_0_unscaled: 1.2778 (1.2803)  loss_bbox_0_unscaled: 0.4390 (0.5324)  loss_giou_0_unscaled: 0.8158 (0.7707)  cardinality_error_0_unscaled: 79.0000 (78.4487)  loss_ce_1_unscaled: 1.2599 (1.2377)  loss_bbox_1_unscaled: 0.5699 (0.5742)  loss_giou_1_unscaled: 0.8350 (0.8064)  cardinality_error_1_unscaled: 49.0000 (48.4915)  time: 0.1133  data: 0.0065  max mem: 594\n",
      "Test:  [1130/4410]  eta: 0:05:56  class_error: 100.00  loss: 8.0947 (7.5810)  loss_ce: 2.7914 (2.8189)  loss_bbox: 3.4887 (3.0677)  loss_giou: 1.8058 (1.6944)  loss_ce_unscaled: 1.3957 (1.4094)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6977 (0.6135)  loss_giou_unscaled: 0.9029 (0.8472)  cardinality_error_unscaled: 29.0000 (27.1088)  loss_ce_0_unscaled: 1.3244 (1.2806)  loss_bbox_0_unscaled: 0.6615 (0.5332)  loss_giou_0_unscaled: 0.6947 (0.7704)  cardinality_error_0_unscaled: 79.0000 (78.4536)  loss_ce_1_unscaled: 1.2348 (1.2376)  loss_bbox_1_unscaled: 0.6734 (0.5752)  loss_giou_1_unscaled: 0.8368 (0.8068)  cardinality_error_1_unscaled: 49.0000 (48.4960)  time: 0.1135  data: 0.0068  max mem: 594\n",
      "Test:  [1140/4410]  eta: 0:05:55  class_error: 100.00  loss: 8.3696 (7.5846)  loss_ce: 2.7868 (2.8189)  loss_bbox: 3.6578 (3.0715)  loss_giou: 1.8248 (1.6943)  loss_ce_unscaled: 1.3934 (1.4094)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7316 (0.6143)  loss_giou_unscaled: 0.9124 (0.8471)  cardinality_error_unscaled: 29.0000 (27.1227)  loss_ce_0_unscaled: 1.3200 (1.2809)  loss_bbox_0_unscaled: 0.6199 (0.5338)  loss_giou_0_unscaled: 0.6947 (0.7702)  cardinality_error_0_unscaled: 79.0000 (78.4566)  loss_ce_1_unscaled: 1.2407 (1.2377)  loss_bbox_1_unscaled: 0.6601 (0.5759)  loss_giou_1_unscaled: 0.7939 (0.8066)  cardinality_error_1_unscaled: 49.0000 (48.4996)  time: 0.1129  data: 0.0069  max mem: 594\n",
      "Test:  [1150/4410]  eta: 0:05:54  class_error: 100.00  loss: 6.6714 (7.5738)  loss_ce: 2.8368 (2.8193)  loss_bbox: 2.3350 (3.0627)  loss_giou: 1.4374 (1.6918)  loss_ce_unscaled: 1.4184 (1.4096)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4670 (0.6125)  loss_giou_unscaled: 0.7187 (0.8459)  cardinality_error_unscaled: 28.0000 (27.1295)  loss_ce_0_unscaled: 1.3068 (1.2811)  loss_bbox_0_unscaled: 0.4670 (0.5326)  loss_giou_0_unscaled: 0.7004 (0.7696)  cardinality_error_0_unscaled: 78.0000 (78.4466)  loss_ce_1_unscaled: 1.2870 (1.2385)  loss_bbox_1_unscaled: 0.4670 (0.5743)  loss_giou_1_unscaled: 0.7059 (0.8057)  cardinality_error_1_unscaled: 48.0000 (48.4952)  time: 0.1124  data: 0.0065  max mem: 594\n",
      "Test:  [1160/4410]  eta: 0:05:53  class_error: 100.00  loss: 6.2806 (7.5633)  loss_ce: 2.8572 (2.8197)  loss_bbox: 2.0155 (3.0543)  loss_giou: 1.4048 (1.6894)  loss_ce_unscaled: 1.4286 (1.4098)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4031 (0.6109)  loss_giou_unscaled: 0.7024 (0.8447)  cardinality_error_unscaled: 28.0000 (27.1370)  loss_ce_0_unscaled: 1.3008 (1.2813)  loss_bbox_0_unscaled: 0.3664 (0.5315)  loss_giou_0_unscaled: 0.7024 (0.7691)  cardinality_error_0_unscaled: 77.0000 (78.4358)  loss_ce_1_unscaled: 1.3148 (1.2391)  loss_bbox_1_unscaled: 0.3756 (0.5730)  loss_giou_1_unscaled: 0.7024 (0.8048)  cardinality_error_1_unscaled: 48.0000 (48.4910)  time: 0.1103  data: 0.0063  max mem: 594\n",
      "Test:  [1170/4410]  eta: 0:05:52  class_error: 100.00  loss: 6.2852 (7.5534)  loss_ce: 2.8572 (2.8200)  loss_bbox: 2.0216 (3.0463)  loss_giou: 1.3922 (1.6871)  loss_ce_unscaled: 1.4286 (1.4100)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4043 (0.6093)  loss_giou_unscaled: 0.6961 (0.8436)  cardinality_error_unscaled: 28.0000 (27.1443)  loss_ce_0_unscaled: 1.2887 (1.2814)  loss_bbox_0_unscaled: 0.3974 (0.5305)  loss_giou_0_unscaled: 0.7065 (0.7686)  cardinality_error_0_unscaled: 77.0000 (78.4295)  loss_ce_1_unscaled: 1.3148 (1.2398)  loss_bbox_1_unscaled: 0.3964 (0.5716)  loss_giou_1_unscaled: 0.7042 (0.8041)  cardinality_error_1_unscaled: 48.0000 (48.4876)  time: 0.1093  data: 0.0065  max mem: 594\n",
      "Test:  [1180/4410]  eta: 0:05:51  class_error: 100.00  loss: 6.7755 (7.5488)  loss_ce: 2.8753 (2.8204)  loss_bbox: 2.3308 (3.0417)  loss_giou: 1.5937 (1.6866)  loss_ce_unscaled: 1.4377 (1.4102)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4662 (0.6083)  loss_giou_unscaled: 0.7969 (0.8433)  cardinality_error_unscaled: 28.0000 (27.1566)  loss_ce_0_unscaled: 1.2966 (1.2818)  loss_bbox_0_unscaled: 0.4216 (0.5300)  loss_giou_0_unscaled: 0.7740 (0.7690)  cardinality_error_0_unscaled: 78.0000 (78.4335)  loss_ce_1_unscaled: 1.2845 (1.2398)  loss_bbox_1_unscaled: 0.4687 (0.5711)  loss_giou_1_unscaled: 0.7969 (0.8042)  cardinality_error_1_unscaled: 49.0000 (48.4920)  time: 0.1091  data: 0.0065  max mem: 594\n",
      "Test:  [1190/4410]  eta: 0:05:50  class_error: 100.00  loss: 7.0924 (7.5443)  loss_ce: 2.8768 (2.8208)  loss_bbox: 2.5704 (3.0374)  loss_giou: 1.6328 (1.6861)  loss_ce_unscaled: 1.4384 (1.4104)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5141 (0.6075)  loss_giou_unscaled: 0.8164 (0.8431)  cardinality_error_unscaled: 29.0000 (27.1587)  loss_ce_0_unscaled: 1.3575 (1.2822)  loss_bbox_0_unscaled: 0.4751 (0.5295)  loss_giou_0_unscaled: 0.8076 (0.7692)  cardinality_error_0_unscaled: 79.0000 (78.4358)  loss_ce_1_unscaled: 1.2270 (1.2397)  loss_bbox_1_unscaled: 0.5141 (0.5705)  loss_giou_1_unscaled: 0.8164 (0.8043)  cardinality_error_1_unscaled: 49.0000 (48.4962)  time: 0.1121  data: 0.0067  max mem: 594\n",
      "Test:  [1200/4410]  eta: 0:05:49  class_error: 100.00  loss: 7.0167 (7.5393)  loss_ce: 2.8768 (2.8212)  loss_bbox: 2.5359 (3.0328)  loss_giou: 1.6264 (1.6853)  loss_ce_unscaled: 1.4384 (1.4106)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5072 (0.6066)  loss_giou_unscaled: 0.8132 (0.8427)  cardinality_error_unscaled: 28.0000 (27.1657)  loss_ce_0_unscaled: 1.2997 (1.2824)  loss_bbox_0_unscaled: 0.4599 (0.5290)  loss_giou_0_unscaled: 0.8076 (0.7694)  cardinality_error_0_unscaled: 79.0000 (78.4388)  loss_ce_1_unscaled: 1.2272 (1.2398)  loss_bbox_1_unscaled: 0.5094 (0.5699)  loss_giou_1_unscaled: 0.8132 (0.8042)  cardinality_error_1_unscaled: 49.0000 (48.4988)  time: 0.1136  data: 0.0070  max mem: 594\n",
      "Test:  [1210/4410]  eta: 0:05:48  class_error: 100.00  loss: 6.4860 (7.5296)  loss_ce: 2.8547 (2.8213)  loss_bbox: 2.2461 (3.0257)  loss_giou: 1.4788 (1.6826)  loss_ce_unscaled: 1.4273 (1.4106)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4492 (0.6051)  loss_giou_unscaled: 0.7394 (0.8413)  cardinality_error_unscaled: 27.0000 (27.1643)  loss_ce_0_unscaled: 1.3073 (1.2827)  loss_bbox_0_unscaled: 0.4091 (0.5277)  loss_giou_0_unscaled: 0.6914 (0.7683)  cardinality_error_0_unscaled: 77.0000 (78.4236)  loss_ce_1_unscaled: 1.2628 (1.2400)  loss_bbox_1_unscaled: 0.4492 (0.5688)  loss_giou_1_unscaled: 0.7394 (0.8031)  cardinality_error_1_unscaled: 47.0000 (48.4864)  time: 0.1132  data: 0.0068  max mem: 594\n",
      "Test:  [1220/4410]  eta: 0:05:47  class_error: 100.00  loss: 6.2815 (7.5202)  loss_ce: 2.8117 (2.8214)  loss_bbox: 2.0667 (3.0189)  loss_giou: 1.3041 (1.6800)  loss_ce_unscaled: 1.4058 (1.4107)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4133 (0.6038)  loss_giou_unscaled: 0.6520 (0.8400)  cardinality_error_unscaled: 27.0000 (27.1630)  loss_ce_0_unscaled: 1.3182 (1.2830)  loss_bbox_0_unscaled: 0.3675 (0.5264)  loss_giou_0_unscaled: 0.6396 (0.7673)  cardinality_error_0_unscaled: 77.0000 (78.4038)  loss_ce_1_unscaled: 1.2664 (1.2402)  loss_bbox_1_unscaled: 0.4016 (0.5676)  loss_giou_1_unscaled: 0.6520 (0.8019)  cardinality_error_1_unscaled: 47.0000 (48.4742)  time: 0.1127  data: 0.0070  max mem: 594\n",
      "Test:  [1230/4410]  eta: 0:05:46  class_error: 100.00  loss: 6.3686 (7.5136)  loss_ce: 2.8566 (2.8217)  loss_bbox: 2.1850 (3.0146)  loss_giou: 1.3293 (1.6772)  loss_ce_unscaled: 1.4283 (1.4109)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4370 (0.6029)  loss_giou_unscaled: 0.6647 (0.8386)  cardinality_error_unscaled: 27.0000 (27.1625)  loss_ce_0_unscaled: 1.3190 (1.2833)  loss_bbox_0_unscaled: 0.3675 (0.5254)  loss_giou_0_unscaled: 0.6396 (0.7662)  cardinality_error_0_unscaled: 77.0000 (78.3883)  loss_ce_1_unscaled: 1.2772 (1.2405)  loss_bbox_1_unscaled: 0.3801 (0.5666)  loss_giou_1_unscaled: 0.6424 (0.8007)  cardinality_error_1_unscaled: 47.0000 (48.4630)  time: 0.1112  data: 0.0073  max mem: 594\n",
      "Test:  [1240/4410]  eta: 0:05:45  class_error: 100.00  loss: 6.7941 (7.5144)  loss_ce: 2.8355 (2.8215)  loss_bbox: 2.5592 (3.0153)  loss_giou: 1.3895 (1.6776)  loss_ce_unscaled: 1.4177 (1.4108)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5118 (0.6031)  loss_giou_unscaled: 0.6948 (0.8388)  cardinality_error_unscaled: 28.0000 (27.1692)  loss_ce_0_unscaled: 1.3254 (1.2837)  loss_bbox_0_unscaled: 0.4058 (0.5247)  loss_giou_0_unscaled: 0.6250 (0.7650)  cardinality_error_0_unscaled: 76.0000 (78.3683)  loss_ce_1_unscaled: 1.2893 (1.2409)  loss_bbox_1_unscaled: 0.4203 (0.5662)  loss_giou_1_unscaled: 0.6424 (0.7999)  cardinality_error_1_unscaled: 48.0000 (48.4593)  time: 0.1133  data: 0.0072  max mem: 594\n",
      "Test:  [1250/4410]  eta: 0:05:44  class_error: 100.00  loss: 7.6739 (7.5142)  loss_ce: 2.7948 (2.8214)  loss_bbox: 3.1036 (3.0153)  loss_giou: 1.7338 (1.6775)  loss_ce_unscaled: 1.3974 (1.4107)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6207 (0.6031)  loss_giou_unscaled: 0.8669 (0.8387)  cardinality_error_unscaled: 28.0000 (27.1759)  loss_ce_0_unscaled: 1.3415 (1.2842)  loss_bbox_0_unscaled: 0.4235 (0.5242)  loss_giou_0_unscaled: 0.6017 (0.7638)  cardinality_error_0_unscaled: 76.0000 (78.3501)  loss_ce_1_unscaled: 1.2931 (1.2413)  loss_bbox_1_unscaled: 0.5190 (0.5658)  loss_giou_1_unscaled: 0.7211 (0.7996)  cardinality_error_1_unscaled: 48.0000 (48.4556)  time: 0.1152  data: 0.0074  max mem: 594\n",
      "Test:  [1260/4410]  eta: 0:05:43  class_error: 100.00  loss: 7.3129 (7.5101)  loss_ce: 2.8223 (2.8215)  loss_bbox: 2.8786 (3.0124)  loss_giou: 1.6052 (1.6762)  loss_ce_unscaled: 1.4111 (1.4107)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5757 (0.6025)  loss_giou_unscaled: 0.8026 (0.8381)  cardinality_error_unscaled: 28.0000 (27.1816)  loss_ce_0_unscaled: 1.3400 (1.2845)  loss_bbox_0_unscaled: 0.4132 (0.5232)  loss_giou_0_unscaled: 0.6185 (0.7626)  cardinality_error_0_unscaled: 78.0000 (78.3434)  loss_ce_1_unscaled: 1.2979 (1.2417)  loss_bbox_1_unscaled: 0.5190 (0.5654)  loss_giou_1_unscaled: 0.7187 (0.7987)  cardinality_error_1_unscaled: 48.0000 (48.4528)  time: 0.1151  data: 0.0073  max mem: 594\n",
      "Test:  [1270/4410]  eta: 0:05:42  class_error: 100.00  loss: 6.6955 (7.5019)  loss_ce: 2.8435 (2.8217)  loss_bbox: 2.6716 (3.0084)  loss_giou: 1.2138 (1.6718)  loss_ce_unscaled: 1.4218 (1.4109)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5343 (0.6017)  loss_giou_unscaled: 0.6069 (0.8359)  cardinality_error_unscaled: 28.0000 (27.1865)  loss_ce_0_unscaled: 1.3088 (1.2846)  loss_bbox_0_unscaled: 0.4092 (0.5226)  loss_giou_0_unscaled: 0.5631 (0.7606)  cardinality_error_0_unscaled: 78.0000 (78.3462)  loss_ce_1_unscaled: 1.2540 (1.2417)  loss_bbox_1_unscaled: 0.4717 (0.5644)  loss_giou_1_unscaled: 0.6150 (0.7965)  cardinality_error_1_unscaled: 49.0000 (48.4571)  time: 0.1153  data: 0.0070  max mem: 594\n",
      "Test:  [1280/4410]  eta: 0:05:41  class_error: 100.00  loss: 6.6295 (7.4924)  loss_ce: 2.8453 (2.8220)  loss_bbox: 2.4807 (3.0029)  loss_giou: 1.1492 (1.6674)  loss_ce_unscaled: 1.4227 (1.4110)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4961 (0.6006)  loss_giou_unscaled: 0.5746 (0.8337)  cardinality_error_unscaled: 29.0000 (27.1881)  loss_ce_0_unscaled: 1.2873 (1.2847)  loss_bbox_0_unscaled: 0.4202 (0.5214)  loss_giou_0_unscaled: 0.4796 (0.7583)  cardinality_error_0_unscaled: 79.0000 (78.3505)  loss_ce_1_unscaled: 1.2430 (1.2418)  loss_bbox_1_unscaled: 0.4230 (0.5630)  loss_giou_1_unscaled: 0.4984 (0.7941)  cardinality_error_1_unscaled: 49.0000 (48.4614)  time: 0.1145  data: 0.0071  max mem: 594\n",
      "Test:  [1290/4410]  eta: 0:05:40  class_error: 100.00  loss: 6.7000 (7.4858)  loss_ce: 2.8306 (2.8220)  loss_bbox: 2.5402 (2.9999)  loss_giou: 1.1226 (1.6638)  loss_ce_unscaled: 1.4153 (1.4110)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5080 (0.6000)  loss_giou_unscaled: 0.5613 (0.8319)  cardinality_error_unscaled: 28.0000 (27.1944)  loss_ce_0_unscaled: 1.2873 (1.2846)  loss_bbox_0_unscaled: 0.4525 (0.5212)  loss_giou_0_unscaled: 0.4962 (0.7567)  cardinality_error_0_unscaled: 79.0000 (78.3478)  loss_ce_1_unscaled: 1.2430 (1.2418)  loss_bbox_1_unscaled: 0.4653 (0.5625)  loss_giou_1_unscaled: 0.4987 (0.7923)  cardinality_error_1_unscaled: 49.0000 (48.4640)  time: 0.1123  data: 0.0069  max mem: 594\n",
      "Test:  [1300/4410]  eta: 0:05:39  class_error: 100.00  loss: 6.7270 (7.4804)  loss_ce: 2.8235 (2.8223)  loss_bbox: 2.4316 (2.9939)  loss_giou: 1.6052 (1.6642)  loss_ce_unscaled: 1.4118 (1.4112)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4863 (0.5988)  loss_giou_unscaled: 0.8026 (0.8321)  cardinality_error_unscaled: 27.0000 (27.1768)  loss_ce_0_unscaled: 1.2702 (1.2845)  loss_bbox_0_unscaled: 0.4542 (0.5203)  loss_giou_0_unscaled: 0.7650 (0.7573)  cardinality_error_0_unscaled: 77.0000 (78.3374)  loss_ce_1_unscaled: 1.2433 (1.2419)  loss_bbox_1_unscaled: 0.4542 (0.5613)  loss_giou_1_unscaled: 0.7650 (0.7926)  cardinality_error_1_unscaled: 47.0000 (48.4527)  time: 0.1134  data: 0.0067  max mem: 594\n",
      "Test:  [1310/4410]  eta: 0:05:38  class_error: 100.00  loss: 6.6952 (7.4738)  loss_ce: 2.8740 (2.8227)  loss_bbox: 2.1879 (2.9870)  loss_giou: 1.6722 (1.6640)  loss_ce_unscaled: 1.4370 (1.4114)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4376 (0.5974)  loss_giou_unscaled: 0.8361 (0.8320)  cardinality_error_unscaled: 25.0000 (27.1571)  loss_ce_0_unscaled: 1.2705 (1.2845)  loss_bbox_0_unscaled: 0.4201 (0.5194)  loss_giou_0_unscaled: 0.8122 (0.7576)  cardinality_error_0_unscaled: 77.0000 (78.3272)  loss_ce_1_unscaled: 1.2620 (1.2421)  loss_bbox_1_unscaled: 0.4201 (0.5600)  loss_giou_1_unscaled: 0.8122 (0.7926)  cardinality_error_1_unscaled: 47.0000 (48.4416)  time: 0.1130  data: 0.0068  max mem: 594\n",
      "Test:  [1320/4410]  eta: 0:05:37  class_error: 100.00  loss: 6.5482 (7.4664)  loss_ce: 2.8740 (2.8231)  loss_bbox: 1.9655 (2.9796)  loss_giou: 1.6278 (1.6637)  loss_ce_unscaled: 1.4370 (1.4115)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3931 (0.5959)  loss_giou_unscaled: 0.8139 (0.8319)  cardinality_error_unscaled: 25.0000 (27.1416)  loss_ce_0_unscaled: 1.2820 (1.2845)  loss_bbox_0_unscaled: 0.3794 (0.5183)  loss_giou_0_unscaled: 0.8125 (0.7579)  cardinality_error_0_unscaled: 77.0000 (78.3179)  loss_ce_1_unscaled: 1.2670 (1.2423)  loss_bbox_1_unscaled: 0.3794 (0.5587)  loss_giou_1_unscaled: 0.8125 (0.7927)  cardinality_error_1_unscaled: 47.0000 (48.4315)  time: 0.1137  data: 0.0067  max mem: 594\n",
      "Test:  [1330/4410]  eta: 0:05:36  class_error: 100.00  loss: 6.5482 (7.4624)  loss_ce: 2.8428 (2.8231)  loss_bbox: 2.2260 (2.9772)  loss_giou: 1.5144 (1.6622)  loss_ce_unscaled: 1.4214 (1.4115)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4452 (0.5954)  loss_giou_unscaled: 0.7572 (0.8311)  cardinality_error_unscaled: 26.0000 (27.1270)  loss_ce_0_unscaled: 1.2926 (1.2847)  loss_bbox_0_unscaled: 0.4117 (0.5181)  loss_giou_0_unscaled: 0.7480 (0.7575)  cardinality_error_0_unscaled: 77.0000 (78.3140)  loss_ce_1_unscaled: 1.2804 (1.2425)  loss_bbox_1_unscaled: 0.4117 (0.5581)  loss_giou_1_unscaled: 0.7572 (0.7922)  cardinality_error_1_unscaled: 48.0000 (48.4282)  time: 0.1159  data: 0.0066  max mem: 594\n",
      "Test:  [1340/4410]  eta: 0:05:35  class_error: 100.00  loss: 6.6091 (7.4566)  loss_ce: 2.8146 (2.8230)  loss_bbox: 2.4764 (2.9732)  loss_giou: 1.3972 (1.6604)  loss_ce_unscaled: 1.4073 (1.4115)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4953 (0.5946)  loss_giou_unscaled: 0.6986 (0.8302)  cardinality_error_unscaled: 27.0000 (27.1208)  loss_ce_0_unscaled: 1.2988 (1.2847)  loss_bbox_0_unscaled: 0.4498 (0.5178)  loss_giou_0_unscaled: 0.7017 (0.7572)  cardinality_error_0_unscaled: 78.0000 (78.3050)  loss_ce_1_unscaled: 1.2680 (1.2427)  loss_bbox_1_unscaled: 0.4498 (0.5576)  loss_giou_1_unscaled: 0.7203 (0.7917)  cardinality_error_1_unscaled: 48.0000 (48.4251)  time: 0.1132  data: 0.0066  max mem: 594\n",
      "Test:  [1350/4410]  eta: 0:05:34  class_error: 100.00  loss: 6.4744 (7.4512)  loss_ce: 2.8123 (2.8230)  loss_bbox: 2.3687 (2.9696)  loss_giou: 1.3972 (1.6585)  loss_ce_unscaled: 1.4062 (1.4115)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4737 (0.5939)  loss_giou_unscaled: 0.6986 (0.8293)  cardinality_error_unscaled: 27.0000 (27.1140)  loss_ce_0_unscaled: 1.2910 (1.2848)  loss_bbox_0_unscaled: 0.4230 (0.5171)  loss_giou_0_unscaled: 0.6939 (0.7567)  cardinality_error_0_unscaled: 78.0000 (78.3005)  loss_ce_1_unscaled: 1.2619 (1.2428)  loss_bbox_1_unscaled: 0.4230 (0.5566)  loss_giou_1_unscaled: 0.6991 (0.7910)  cardinality_error_1_unscaled: 48.0000 (48.4226)  time: 0.1132  data: 0.0070  max mem: 594\n",
      "Test:  [1360/4410]  eta: 0:05:33  class_error: 100.00  loss: 6.8749 (7.4487)  loss_ce: 2.8987 (2.8241)  loss_bbox: 2.5051 (2.9682)  loss_giou: 1.3534 (1.6564)  loss_ce_unscaled: 1.4493 (1.4121)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5010 (0.5936)  loss_giou_unscaled: 0.6767 (0.8282)  cardinality_error_unscaled: 29.0000 (27.1278)  loss_ce_0_unscaled: 1.2907 (1.2848)  loss_bbox_0_unscaled: 0.4820 (0.5173)  loss_giou_0_unscaled: 0.6991 (0.7563)  cardinality_error_0_unscaled: 78.0000 (78.3049)  loss_ce_1_unscaled: 1.2491 (1.2426)  loss_bbox_1_unscaled: 0.4820 (0.5564)  loss_giou_1_unscaled: 0.6991 (0.7903)  cardinality_error_1_unscaled: 49.0000 (48.4269)  time: 0.1148  data: 0.0070  max mem: 594\n",
      "Test:  [1370/4410]  eta: 0:05:32  class_error: 100.00  loss: 7.2692 (7.4495)  loss_ce: 2.9586 (2.8251)  loss_bbox: 2.8516 (2.9707)  loss_giou: 1.2760 (1.6537)  loss_ce_unscaled: 1.4793 (1.4125)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5703 (0.5941)  loss_giou_unscaled: 0.6380 (0.8268)  cardinality_error_unscaled: 29.0000 (27.1415)  loss_ce_0_unscaled: 1.2866 (1.2848)  loss_bbox_0_unscaled: 0.5703 (0.5180)  loss_giou_0_unscaled: 0.6708 (0.7556)  cardinality_error_0_unscaled: 79.0000 (78.3100)  loss_ce_1_unscaled: 1.1751 (1.2419)  loss_bbox_1_unscaled: 0.5703 (0.5570)  loss_giou_1_unscaled: 0.6708 (0.7893)  cardinality_error_1_unscaled: 49.0000 (48.4311)  time: 0.1144  data: 0.0066  max mem: 594\n",
      "Test:  [1380/4410]  eta: 0:05:31  class_error: 100.00  loss: 7.7711 (7.4507)  loss_ce: 2.9410 (2.8257)  loss_bbox: 3.5703 (2.9735)  loss_giou: 1.2760 (1.6514)  loss_ce_unscaled: 1.4705 (1.4129)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7141 (0.5947)  loss_giou_unscaled: 0.6380 (0.8257)  cardinality_error_unscaled: 29.0000 (27.1550)  loss_ce_0_unscaled: 1.2866 (1.2847)  loss_bbox_0_unscaled: 0.5787 (0.5183)  loss_giou_0_unscaled: 0.6579 (0.7551)  cardinality_error_0_unscaled: 79.0000 (78.3143)  loss_ce_1_unscaled: 1.1648 (1.2415)  loss_bbox_1_unscaled: 0.6045 (0.5574)  loss_giou_1_unscaled: 0.6579 (0.7885)  cardinality_error_1_unscaled: 49.0000 (48.4352)  time: 0.1147  data: 0.0065  max mem: 594\n",
      "Test:  [1390/4410]  eta: 0:05:30  class_error: 100.00  loss: 6.7893 (7.4399)  loss_ce: 2.8717 (2.8261)  loss_bbox: 2.5514 (2.9662)  loss_giou: 1.3044 (1.6476)  loss_ce_unscaled: 1.4358 (1.4130)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5103 (0.5932)  loss_giou_unscaled: 0.6522 (0.8238)  cardinality_error_unscaled: 29.0000 (27.1589)  loss_ce_0_unscaled: 1.2312 (1.2843)  loss_bbox_0_unscaled: 0.4735 (0.5174)  loss_giou_0_unscaled: 0.6629 (0.7537)  cardinality_error_0_unscaled: 79.0000 (78.3192)  loss_ce_1_unscaled: 1.2062 (1.2412)  loss_bbox_1_unscaled: 0.5103 (0.5563)  loss_giou_1_unscaled: 0.6629 (0.7869)  cardinality_error_1_unscaled: 49.0000 (48.4393)  time: 0.1142  data: 0.0065  max mem: 594\n",
      "Test:  [1400/4410]  eta: 0:05:29  class_error: 100.00  loss: 6.4229 (7.4331)  loss_ce: 2.8689 (2.8264)  loss_bbox: 2.2689 (2.9618)  loss_giou: 1.2922 (1.6448)  loss_ce_unscaled: 1.4344 (1.4132)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4538 (0.5924)  loss_giou_unscaled: 0.6461 (0.8224)  cardinality_error_unscaled: 29.0000 (27.1699)  loss_ce_0_unscaled: 1.2185 (1.2839)  loss_bbox_0_unscaled: 0.4538 (0.5171)  loss_giou_0_unscaled: 0.6461 (0.7528)  cardinality_error_0_unscaled: 79.0000 (78.3241)  loss_ce_1_unscaled: 1.2123 (1.2411)  loss_bbox_1_unscaled: 0.4538 (0.5556)  loss_giou_1_unscaled: 0.6461 (0.7858)  cardinality_error_1_unscaled: 49.0000 (48.4433)  time: 0.1147  data: 0.0067  max mem: 594\n",
      "Test:  [1410/4410]  eta: 0:05:29  class_error: 100.00  loss: 6.4229 (7.4267)  loss_ce: 2.8635 (2.8268)  loss_bbox: 2.2689 (2.9573)  loss_giou: 1.2922 (1.6426)  loss_ce_unscaled: 1.4317 (1.4134)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4538 (0.5915)  loss_giou_unscaled: 0.6461 (0.8213)  cardinality_error_unscaled: 29.0000 (27.1765)  loss_ce_0_unscaled: 1.2206 (1.2835)  loss_bbox_0_unscaled: 0.4538 (0.5167)  loss_giou_0_unscaled: 0.6461 (0.7522)  cardinality_error_0_unscaled: 79.0000 (78.3288)  loss_ce_1_unscaled: 1.2134 (1.2408)  loss_bbox_1_unscaled: 0.4538 (0.5550)  loss_giou_1_unscaled: 0.6461 (0.7849)  cardinality_error_1_unscaled: 49.0000 (48.4472)  time: 0.1167  data: 0.0068  max mem: 594\n",
      "Test:  [1420/4410]  eta: 0:05:28  class_error: 100.00  loss: 7.1245 (7.4298)  loss_ce: 2.8802 (2.8271)  loss_bbox: 2.8503 (2.9603)  loss_giou: 1.4722 (1.6424)  loss_ce_unscaled: 1.4401 (1.4135)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5701 (0.5921)  loss_giou_unscaled: 0.7361 (0.8212)  cardinality_error_unscaled: 29.0000 (27.1872)  loss_ce_0_unscaled: 1.2380 (1.2833)  loss_bbox_0_unscaled: 0.5701 (0.5177)  loss_giou_0_unscaled: 0.7361 (0.7526)  cardinality_error_0_unscaled: 79.0000 (78.3336)  loss_ce_1_unscaled: 1.2262 (1.2410)  loss_bbox_1_unscaled: 0.5701 (0.5557)  loss_giou_1_unscaled: 0.7361 (0.7851)  cardinality_error_1_unscaled: 49.0000 (48.4511)  time: 0.1179  data: 0.0070  max mem: 594\n",
      "Test:  [1430/4410]  eta: 0:05:27  class_error: 100.00  loss: 8.0398 (7.4336)  loss_ce: 2.8865 (2.8274)  loss_bbox: 3.4095 (2.9629)  loss_giou: 1.7215 (1.6433)  loss_ce_unscaled: 1.4432 (1.4137)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6819 (0.5926)  loss_giou_unscaled: 0.8607 (0.8217)  cardinality_error_unscaled: 29.0000 (27.1985)  loss_ce_0_unscaled: 1.2445 (1.2829)  loss_bbox_0_unscaled: 0.6753 (0.5186)  loss_giou_0_unscaled: 0.8227 (0.7533)  cardinality_error_0_unscaled: 79.0000 (78.3382)  loss_ce_1_unscaled: 1.2569 (1.2411)  loss_bbox_1_unscaled: 0.6793 (0.5565)  loss_giou_1_unscaled: 0.8388 (0.7856)  cardinality_error_1_unscaled: 49.0000 (48.4549)  time: 0.1164  data: 0.0069  max mem: 594\n",
      "Test:  [1440/4410]  eta: 0:05:26  class_error: 100.00  loss: 8.0726 (7.4376)  loss_ce: 2.8654 (2.8277)  loss_bbox: 3.4824 (2.9665)  loss_giou: 1.7026 (1.6433)  loss_ce_unscaled: 1.4327 (1.4138)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6965 (0.5933)  loss_giou_unscaled: 0.8513 (0.8217)  cardinality_error_unscaled: 29.0000 (27.2096)  loss_ce_0_unscaled: 1.2299 (1.2826)  loss_bbox_0_unscaled: 0.6748 (0.5197)  loss_giou_0_unscaled: 0.8278 (0.7538)  cardinality_error_0_unscaled: 79.0000 (78.3428)  loss_ce_1_unscaled: 1.2532 (1.2411)  loss_bbox_1_unscaled: 0.6819 (0.5573)  loss_giou_1_unscaled: 0.8459 (0.7859)  cardinality_error_1_unscaled: 49.0000 (48.4587)  time: 0.1140  data: 0.0068  max mem: 594\n",
      "Test:  [1450/4410]  eta: 0:05:25  class_error: 100.00  loss: 8.2373 (7.4431)  loss_ce: 2.8658 (2.8281)  loss_bbox: 3.6297 (2.9715)  loss_giou: 1.6854 (1.6435)  loss_ce_unscaled: 1.4329 (1.4141)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7259 (0.5943)  loss_giou_unscaled: 0.8427 (0.8217)  cardinality_error_unscaled: 29.0000 (27.2130)  loss_ce_0_unscaled: 1.2356 (1.2824)  loss_bbox_0_unscaled: 0.6858 (0.5208)  loss_giou_0_unscaled: 0.8278 (0.7543)  cardinality_error_0_unscaled: 79.0000 (78.3467)  loss_ce_1_unscaled: 1.2532 (1.2413)  loss_bbox_1_unscaled: 0.6882 (0.5584)  loss_giou_1_unscaled: 0.8342 (0.7862)  cardinality_error_1_unscaled: 49.0000 (48.4624)  time: 0.1137  data: 0.0067  max mem: 594\n",
      "Test:  [1460/4410]  eta: 0:05:24  class_error: 100.00  loss: 8.3645 (7.4494)  loss_ce: 2.8747 (2.8284)  loss_bbox: 3.5963 (2.9764)  loss_giou: 1.7619 (1.6446)  loss_ce_unscaled: 1.4373 (1.4142)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7193 (0.5953)  loss_giou_unscaled: 0.8809 (0.8223)  cardinality_error_unscaled: 28.0000 (27.2074)  loss_ce_0_unscaled: 1.2475 (1.2823)  loss_bbox_0_unscaled: 0.6858 (0.5220)  loss_giou_0_unscaled: 0.8331 (0.7550)  cardinality_error_0_unscaled: 79.0000 (78.3511)  loss_ce_1_unscaled: 1.2798 (1.2415)  loss_bbox_1_unscaled: 0.7085 (0.5596)  loss_giou_1_unscaled: 0.8342 (0.7867)  cardinality_error_1_unscaled: 49.0000 (48.4661)  time: 0.1200  data: 0.0068  max mem: 594\n",
      "Test:  [1470/4410]  eta: 0:05:23  class_error: 100.00  loss: 8.2897 (7.4547)  loss_ce: 2.8747 (2.8287)  loss_bbox: 3.6033 (2.9809)  loss_giou: 1.7467 (1.6451)  loss_ce_unscaled: 1.4373 (1.4143)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7207 (0.5962)  loss_giou_unscaled: 0.8733 (0.8226)  cardinality_error_unscaled: 29.0000 (27.1999)  loss_ce_0_unscaled: 1.2539 (1.2821)  loss_bbox_0_unscaled: 0.6848 (0.5231)  loss_giou_0_unscaled: 0.7964 (0.7553)  cardinality_error_0_unscaled: 79.0000 (78.3555)  loss_ce_1_unscaled: 1.2797 (1.2417)  loss_bbox_1_unscaled: 0.7112 (0.5607)  loss_giou_1_unscaled: 0.8357 (0.7869)  cardinality_error_1_unscaled: 49.0000 (48.4697)  time: 0.1188  data: 0.0070  max mem: 594\n",
      "Test:  [1480/4410]  eta: 0:05:22  class_error: 100.00  loss: 8.3431 (7.4609)  loss_ce: 2.8439 (2.8286)  loss_bbox: 3.8452 (2.9866)  loss_giou: 1.7403 (1.6456)  loss_ce_unscaled: 1.4219 (1.4143)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7690 (0.5973)  loss_giou_unscaled: 0.8701 (0.8228)  cardinality_error_unscaled: 29.0000 (27.2053)  loss_ce_0_unscaled: 1.2598 (1.2820)  loss_bbox_0_unscaled: 0.7390 (0.5246)  loss_giou_0_unscaled: 0.8343 (0.7559)  cardinality_error_0_unscaled: 79.0000 (78.3599)  loss_ce_1_unscaled: 1.2758 (1.2420)  loss_bbox_1_unscaled: 0.7718 (0.5621)  loss_giou_1_unscaled: 0.8385 (0.7874)  cardinality_error_1_unscaled: 49.0000 (48.4733)  time: 0.1115  data: 0.0070  max mem: 594\n",
      "Test:  [1490/4410]  eta: 0:05:21  class_error: 100.00  loss: 8.3748 (7.4659)  loss_ce: 2.8291 (2.8287)  loss_bbox: 3.8452 (2.9910)  loss_giou: 1.7442 (1.6462)  loss_ce_unscaled: 1.4146 (1.4144)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7690 (0.5982)  loss_giou_unscaled: 0.8721 (0.8231)  cardinality_error_unscaled: 29.0000 (27.2086)  loss_ce_0_unscaled: 1.2741 (1.2819)  loss_bbox_0_unscaled: 0.7690 (0.5260)  loss_giou_0_unscaled: 0.8450 (0.7564)  cardinality_error_0_unscaled: 79.0000 (78.3642)  loss_ce_1_unscaled: 1.2775 (1.2423)  loss_bbox_1_unscaled: 0.7690 (0.5632)  loss_giou_1_unscaled: 0.8483 (0.7878)  cardinality_error_1_unscaled: 49.0000 (48.4769)  time: 0.1133  data: 0.0068  max mem: 594\n",
      "Test:  [1500/4410]  eta: 0:05:20  class_error: 100.00  loss: 8.3585 (7.4724)  loss_ce: 2.8296 (2.8288)  loss_bbox: 3.7954 (2.9970)  loss_giou: 1.7605 (1.6466)  loss_ce_unscaled: 1.4148 (1.4144)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7591 (0.5994)  loss_giou_unscaled: 0.8803 (0.8233)  cardinality_error_unscaled: 28.0000 (27.2112)  loss_ce_0_unscaled: 1.2752 (1.2818)  loss_bbox_0_unscaled: 0.7431 (0.5275)  loss_giou_0_unscaled: 0.8786 (0.7570)  cardinality_error_0_unscaled: 79.0000 (78.3684)  loss_ce_1_unscaled: 1.2775 (1.2425)  loss_bbox_1_unscaled: 0.7431 (0.5644)  loss_giou_1_unscaled: 0.8803 (0.7884)  cardinality_error_1_unscaled: 49.0000 (48.4803)  time: 0.1159  data: 0.0069  max mem: 594\n",
      "Test:  [1510/4410]  eta: 0:05:19  class_error: 100.00  loss: 8.2847 (7.4758)  loss_ce: 2.8660 (2.8292)  loss_bbox: 3.7020 (3.0005)  loss_giou: 1.6734 (1.6462)  loss_ce_unscaled: 1.4330 (1.4146)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7404 (0.6001)  loss_giou_unscaled: 0.8367 (0.8231)  cardinality_error_unscaled: 29.0000 (27.2204)  loss_ce_0_unscaled: 1.2672 (1.2816)  loss_bbox_0_unscaled: 0.7392 (0.5287)  loss_giou_0_unscaled: 0.7911 (0.7571)  cardinality_error_0_unscaled: 79.0000 (78.3726)  loss_ce_1_unscaled: 1.2536 (1.2425)  loss_bbox_1_unscaled: 0.7352 (0.5653)  loss_giou_1_unscaled: 0.8367 (0.7884)  cardinality_error_1_unscaled: 49.0000 (48.4838)  time: 0.1136  data: 0.0067  max mem: 594\n",
      "Test:  [1520/4410]  eta: 0:05:18  class_error: 100.00  loss: 8.0241 (7.4785)  loss_ce: 2.8696 (2.8293)  loss_bbox: 3.5463 (3.0034)  loss_giou: 1.5766 (1.6458)  loss_ce_unscaled: 1.4348 (1.4146)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7093 (0.6007)  loss_giou_unscaled: 0.7883 (0.8229)  cardinality_error_unscaled: 29.0000 (27.2275)  loss_ce_0_unscaled: 1.2334 (1.2813)  loss_bbox_0_unscaled: 0.7093 (0.5296)  loss_giou_0_unscaled: 0.7809 (0.7572)  cardinality_error_0_unscaled: 79.0000 (78.3767)  loss_ce_1_unscaled: 1.2349 (1.2423)  loss_bbox_1_unscaled: 0.7072 (0.5660)  loss_giou_1_unscaled: 0.7883 (0.7884)  cardinality_error_1_unscaled: 49.0000 (48.4872)  time: 0.1128  data: 0.0066  max mem: 594\n",
      "Test:  [1530/4410]  eta: 0:05:17  class_error: 100.00  loss: 8.0241 (7.4819)  loss_ce: 2.8718 (2.8297)  loss_bbox: 3.5204 (3.0063)  loss_giou: 1.5821 (1.6459)  loss_ce_unscaled: 1.4359 (1.4149)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7041 (0.6013)  loss_giou_unscaled: 0.7910 (0.8229)  cardinality_error_unscaled: 29.0000 (27.2378)  loss_ce_0_unscaled: 1.2212 (1.2809)  loss_bbox_0_unscaled: 0.7041 (0.5306)  loss_giou_0_unscaled: 0.7760 (0.7575)  cardinality_error_0_unscaled: 79.0000 (78.3808)  loss_ce_1_unscaled: 1.2349 (1.2424)  loss_bbox_1_unscaled: 0.6719 (0.5667)  loss_giou_1_unscaled: 0.7809 (0.7885)  cardinality_error_1_unscaled: 49.0000 (48.4905)  time: 0.1125  data: 0.0067  max mem: 594\n",
      "Test:  [1540/4410]  eta: 0:05:15  class_error: 100.00  loss: 8.0209 (7.4846)  loss_ce: 2.8977 (2.8300)  loss_bbox: 3.4997 (3.0095)  loss_giou: 1.5724 (1.6452)  loss_ce_unscaled: 1.4489 (1.4150)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6999 (0.6019)  loss_giou_unscaled: 0.7862 (0.8226)  cardinality_error_unscaled: 29.0000 (27.2453)  loss_ce_0_unscaled: 1.2212 (1.2807)  loss_bbox_0_unscaled: 0.7084 (0.5318)  loss_giou_0_unscaled: 0.7567 (0.7573)  cardinality_error_0_unscaled: 79.0000 (78.3848)  loss_ce_1_unscaled: 1.2511 (1.2425)  loss_bbox_1_unscaled: 0.6999 (0.5675)  loss_giou_1_unscaled: 0.7760 (0.7884)  cardinality_error_1_unscaled: 49.0000 (48.4938)  time: 0.1106  data: 0.0067  max mem: 594\n",
      "Test:  [1550/4410]  eta: 0:05:15  class_error: 100.00  loss: 7.8106 (7.4849)  loss_ce: 2.8816 (2.8303)  loss_bbox: 3.4282 (3.0105)  loss_giou: 1.5110 (1.6440)  loss_ce_unscaled: 1.4408 (1.4152)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6856 (0.6021)  loss_giou_unscaled: 0.7555 (0.8220)  cardinality_error_unscaled: 29.0000 (27.2553)  loss_ce_0_unscaled: 1.2129 (1.2802)  loss_bbox_0_unscaled: 0.6913 (0.5325)  loss_giou_0_unscaled: 0.7400 (0.7571)  cardinality_error_0_unscaled: 79.0000 (78.3888)  loss_ce_1_unscaled: 1.2503 (1.2425)  loss_bbox_1_unscaled: 0.6856 (0.5679)  loss_giou_1_unscaled: 0.7555 (0.7880)  cardinality_error_1_unscaled: 49.0000 (48.4971)  time: 0.1173  data: 0.0072  max mem: 594\n",
      "Test:  [1560/4410]  eta: 0:05:14  class_error: 100.00  loss: 7.7782 (7.4869)  loss_ce: 2.8640 (2.8305)  loss_bbox: 3.2787 (3.0126)  loss_giou: 1.5154 (1.6438)  loss_ce_unscaled: 1.4320 (1.4152)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6557 (0.6025)  loss_giou_unscaled: 0.7577 (0.8219)  cardinality_error_unscaled: 29.0000 (27.2607)  loss_ce_0_unscaled: 1.2159 (1.2799)  loss_bbox_0_unscaled: 0.6557 (0.5334)  loss_giou_0_unscaled: 0.7577 (0.7574)  cardinality_error_0_unscaled: 79.0000 (78.3927)  loss_ce_1_unscaled: 1.2543 (1.2427)  loss_bbox_1_unscaled: 0.6557 (0.5686)  loss_giou_1_unscaled: 0.7577 (0.7882)  cardinality_error_1_unscaled: 49.0000 (48.5003)  time: 0.1206  data: 0.0076  max mem: 594\n",
      "Test:  [1570/4410]  eta: 0:05:13  class_error: 100.00  loss: 8.2747 (7.4934)  loss_ce: 2.8544 (2.8307)  loss_bbox: 3.7033 (3.0187)  loss_giou: 1.6361 (1.6440)  loss_ce_unscaled: 1.4272 (1.4154)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7407 (0.6037)  loss_giou_unscaled: 0.8180 (0.8220)  cardinality_error_unscaled: 29.0000 (27.2718)  loss_ce_0_unscaled: 1.2540 (1.2798)  loss_bbox_0_unscaled: 0.7321 (0.5349)  loss_giou_0_unscaled: 0.8136 (0.7576)  cardinality_error_0_unscaled: 79.0000 (78.3966)  loss_ce_1_unscaled: 1.2575 (1.2427)  loss_bbox_1_unscaled: 0.7330 (0.5700)  loss_giou_1_unscaled: 0.8169 (0.7883)  cardinality_error_1_unscaled: 49.0000 (48.5035)  time: 0.1205  data: 0.0076  max mem: 594\n",
      "Test:  [1580/4410]  eta: 0:05:12  class_error: 100.00  loss: 8.3659 (7.4982)  loss_ce: 2.8499 (2.8308)  loss_bbox: 3.9203 (3.0235)  loss_giou: 1.6361 (1.6439)  loss_ce_unscaled: 1.4249 (1.4154)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7841 (0.6047)  loss_giou_unscaled: 0.8180 (0.8220)  cardinality_error_unscaled: 29.0000 (27.2827)  loss_ce_0_unscaled: 1.2267 (1.2794)  loss_bbox_0_unscaled: 0.7666 (0.5364)  loss_giou_0_unscaled: 0.7967 (0.7579)  cardinality_error_0_unscaled: 79.0000 (78.4004)  loss_ce_1_unscaled: 1.2434 (1.2427)  loss_bbox_1_unscaled: 0.7731 (0.5712)  loss_giou_1_unscaled: 0.8136 (0.7884)  cardinality_error_1_unscaled: 49.0000 (48.5066)  time: 0.1201  data: 0.0076  max mem: 594\n",
      "Test:  [1590/4410]  eta: 0:05:11  class_error: 100.00  loss: 8.2731 (7.5022)  loss_ce: 2.8507 (2.8311)  loss_bbox: 3.7952 (3.0274)  loss_giou: 1.6367 (1.6438)  loss_ce_unscaled: 1.4253 (1.4155)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7590 (0.6055)  loss_giou_unscaled: 0.8183 (0.8219)  cardinality_error_unscaled: 29.0000 (27.2929)  loss_ce_0_unscaled: 1.2313 (1.2793)  loss_bbox_0_unscaled: 0.7581 (0.5374)  loss_giou_0_unscaled: 0.7808 (0.7582)  cardinality_error_0_unscaled: 79.0000 (78.4041)  loss_ce_1_unscaled: 1.2479 (1.2429)  loss_bbox_1_unscaled: 0.7590 (0.5721)  loss_giou_1_unscaled: 0.7808 (0.7885)  cardinality_error_1_unscaled: 49.0000 (48.5097)  time: 0.1155  data: 0.0073  max mem: 594\n",
      "Test:  [1600/4410]  eta: 0:05:10  class_error: 100.00  loss: 7.3980 (7.4985)  loss_ce: 2.8866 (2.8314)  loss_bbox: 3.1133 (3.0247)  loss_giou: 1.5472 (1.6424)  loss_ce_unscaled: 1.4433 (1.4157)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6227 (0.6049)  loss_giou_unscaled: 0.7736 (0.8212)  cardinality_error_unscaled: 29.0000 (27.3036)  loss_ce_0_unscaled: 1.2347 (1.2790)  loss_bbox_0_unscaled: 0.5422 (0.5372)  loss_giou_0_unscaled: 0.7629 (0.7579)  cardinality_error_0_unscaled: 79.0000 (78.4079)  loss_ce_1_unscaled: 1.2532 (1.2429)  loss_bbox_1_unscaled: 0.6227 (0.5718)  loss_giou_1_unscaled: 0.7629 (0.7880)  cardinality_error_1_unscaled: 49.0000 (48.5128)  time: 0.1149  data: 0.0072  max mem: 594\n",
      "Test:  [1610/4410]  eta: 0:05:09  class_error: 100.00  loss: 6.8150 (7.4923)  loss_ce: 2.8850 (2.8318)  loss_bbox: 2.4487 (3.0200)  loss_giou: 1.3334 (1.6405)  loss_ce_unscaled: 1.4425 (1.4159)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4897 (0.6040)  loss_giou_unscaled: 0.6667 (0.8202)  cardinality_error_unscaled: 29.0000 (27.3135)  loss_ce_0_unscaled: 1.2147 (1.2787)  loss_bbox_0_unscaled: 0.4897 (0.5367)  loss_giou_0_unscaled: 0.6663 (0.7573)  cardinality_error_0_unscaled: 79.0000 (78.4115)  loss_ce_1_unscaled: 1.2462 (1.2430)  loss_bbox_1_unscaled: 0.4897 (0.5711)  loss_giou_1_unscaled: 0.6663 (0.7872)  cardinality_error_1_unscaled: 49.0000 (48.5158)  time: 0.1149  data: 0.0073  max mem: 594\n",
      "Test:  [1620/4410]  eta: 0:05:08  class_error: 100.00  loss: 6.5715 (7.4878)  loss_ce: 2.8732 (2.8320)  loss_bbox: 2.3266 (3.0166)  loss_giou: 1.3438 (1.6392)  loss_ce_unscaled: 1.4366 (1.4160)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4653 (0.6033)  loss_giou_unscaled: 0.6719 (0.8196)  cardinality_error_unscaled: 29.0000 (27.3239)  loss_ce_0_unscaled: 1.2253 (1.2784)  loss_bbox_0_unscaled: 0.4653 (0.5365)  loss_giou_0_unscaled: 0.6444 (0.7570)  cardinality_error_0_unscaled: 79.0000 (78.4152)  loss_ce_1_unscaled: 1.2437 (1.2429)  loss_bbox_1_unscaled: 0.4653 (0.5706)  loss_giou_1_unscaled: 0.6719 (0.7868)  cardinality_error_1_unscaled: 49.0000 (48.5188)  time: 0.1136  data: 0.0070  max mem: 594\n",
      "Test:  [1630/4410]  eta: 0:05:07  class_error: 100.00  loss: 7.0471 (7.4883)  loss_ce: 2.8538 (2.8322)  loss_bbox: 2.6768 (3.0171)  loss_giou: 1.5165 (1.6390)  loss_ce_unscaled: 1.4269 (1.4161)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5354 (0.6034)  loss_giou_unscaled: 0.7583 (0.8195)  cardinality_error_unscaled: 29.0000 (27.3342)  loss_ce_0_unscaled: 1.2159 (1.2780)  loss_bbox_0_unscaled: 0.5356 (0.5370)  loss_giou_0_unscaled: 0.7583 (0.7572)  cardinality_error_0_unscaled: 79.0000 (78.4188)  loss_ce_1_unscaled: 1.2481 (1.2431)  loss_bbox_1_unscaled: 0.5354 (0.5709)  loss_giou_1_unscaled: 0.7583 (0.7869)  cardinality_error_1_unscaled: 49.0000 (48.5218)  time: 0.1122  data: 0.0069  max mem: 594\n",
      "Test:  [1640/4410]  eta: 0:05:05  class_error: 100.00  loss: 7.5643 (7.4903)  loss_ce: 2.8535 (2.8324)  loss_bbox: 3.1802 (3.0191)  loss_giou: 1.5526 (1.6388)  loss_ce_unscaled: 1.4268 (1.4162)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6360 (0.6038)  loss_giou_unscaled: 0.7763 (0.8194)  cardinality_error_unscaled: 29.0000 (27.3443)  loss_ce_0_unscaled: 1.2159 (1.2777)  loss_bbox_0_unscaled: 0.6293 (0.5378)  loss_giou_0_unscaled: 0.7763 (0.7574)  cardinality_error_0_unscaled: 79.0000 (78.4223)  loss_ce_1_unscaled: 1.2667 (1.2432)  loss_bbox_1_unscaled: 0.6293 (0.5714)  loss_giou_1_unscaled: 0.7812 (0.7870)  cardinality_error_1_unscaled: 49.0000 (48.5247)  time: 0.1108  data: 0.0067  max mem: 594\n",
      "Test:  [1650/4410]  eta: 0:05:04  class_error: 100.00  loss: 7.7923 (7.4917)  loss_ce: 2.8590 (2.8326)  loss_bbox: 3.1508 (3.0193)  loss_giou: 1.5959 (1.6398)  loss_ce_unscaled: 1.4295 (1.4163)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6302 (0.6039)  loss_giou_unscaled: 0.7980 (0.8199)  cardinality_error_unscaled: 29.0000 (27.3537)  loss_ce_0_unscaled: 1.2274 (1.2775)  loss_bbox_0_unscaled: 0.6241 (0.5381)  loss_giou_0_unscaled: 0.7980 (0.7582)  cardinality_error_0_unscaled: 79.0000 (78.4258)  loss_ce_1_unscaled: 1.2664 (1.2434)  loss_bbox_1_unscaled: 0.6241 (0.5715)  loss_giou_1_unscaled: 0.8196 (0.7876)  cardinality_error_1_unscaled: 49.0000 (48.5276)  time: 0.1127  data: 0.0066  max mem: 594\n",
      "Test:  [1660/4410]  eta: 0:05:03  class_error: 100.00  loss: 7.6153 (7.4911)  loss_ce: 2.8716 (2.8330)  loss_bbox: 3.0838 (3.0196)  loss_giou: 1.5536 (1.6386)  loss_ce_unscaled: 1.4358 (1.4165)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6168 (0.6039)  loss_giou_unscaled: 0.7768 (0.8193)  cardinality_error_unscaled: 29.0000 (27.3636)  loss_ce_0_unscaled: 1.2737 (1.2777)  loss_bbox_0_unscaled: 0.5779 (0.5382)  loss_giou_0_unscaled: 0.7056 (0.7574)  cardinality_error_0_unscaled: 79.0000 (78.4293)  loss_ce_1_unscaled: 1.2641 (1.2435)  loss_bbox_1_unscaled: 0.5783 (0.5716)  loss_giou_1_unscaled: 0.7768 (0.7870)  cardinality_error_1_unscaled: 49.0000 (48.5304)  time: 0.1143  data: 0.0070  max mem: 594\n",
      "Test:  [1670/4410]  eta: 0:05:02  class_error: 100.00  loss: 7.2127 (7.4881)  loss_ce: 2.8979 (2.8334)  loss_bbox: 2.9007 (3.0181)  loss_giou: 1.3698 (1.6367)  loss_ce_unscaled: 1.4490 (1.4167)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5801 (0.6036)  loss_giou_unscaled: 0.6849 (0.8183)  cardinality_error_unscaled: 29.0000 (27.3728)  loss_ce_0_unscaled: 1.2744 (1.2776)  loss_bbox_0_unscaled: 0.5133 (0.5381)  loss_giou_0_unscaled: 0.6323 (0.7566)  cardinality_error_0_unscaled: 79.0000 (78.4327)  loss_ce_1_unscaled: 1.2617 (1.2436)  loss_bbox_1_unscaled: 0.5475 (0.5715)  loss_giou_1_unscaled: 0.6659 (0.7861)  cardinality_error_1_unscaled: 49.0000 (48.5332)  time: 0.1137  data: 0.0072  max mem: 594\n",
      "Test:  [1680/4410]  eta: 0:05:01  class_error: 100.00  loss: 7.0038 (7.4842)  loss_ce: 2.9016 (2.8339)  loss_bbox: 2.7658 (3.0162)  loss_giou: 1.3234 (1.6342)  loss_ce_unscaled: 1.4508 (1.4169)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5532 (0.6032)  loss_giou_unscaled: 0.6617 (0.8171)  cardinality_error_unscaled: 29.0000 (27.3825)  loss_ce_0_unscaled: 1.2471 (1.2776)  loss_bbox_0_unscaled: 0.5015 (0.5378)  loss_giou_0_unscaled: 0.6526 (0.7558)  cardinality_error_0_unscaled: 79.0000 (78.4360)  loss_ce_1_unscaled: 1.2617 (1.2438)  loss_bbox_1_unscaled: 0.5066 (0.5710)  loss_giou_1_unscaled: 0.6585 (0.7851)  cardinality_error_1_unscaled: 49.0000 (48.5360)  time: 0.1125  data: 0.0076  max mem: 594\n",
      "Test:  [1690/4410]  eta: 0:05:00  class_error: 100.00  loss: 6.3894 (7.4730)  loss_ce: 2.8857 (2.8341)  loss_bbox: 2.1437 (3.0085)  loss_giou: 1.1447 (1.6304)  loss_ce_unscaled: 1.4428 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4287 (0.6017)  loss_giou_unscaled: 0.5723 (0.8152)  cardinality_error_unscaled: 29.0000 (27.3921)  loss_ce_0_unscaled: 1.2579 (1.2776)  loss_bbox_0_unscaled: 0.3627 (0.5365)  loss_giou_0_unscaled: 0.5041 (0.7538)  cardinality_error_0_unscaled: 79.0000 (78.4382)  loss_ce_1_unscaled: 1.2406 (1.2436)  loss_bbox_1_unscaled: 0.3627 (0.5695)  loss_giou_1_unscaled: 0.5025 (0.7829)  cardinality_error_1_unscaled: 49.0000 (48.5387)  time: 0.1125  data: 0.0072  max mem: 594\n",
      "Test:  [1700/4410]  eta: 0:04:59  class_error: 100.00  loss: 5.3595 (7.4624)  loss_ce: 2.8728 (2.8342)  loss_bbox: 1.5919 (3.0013)  loss_giou: 0.9864 (1.6268)  loss_ce_unscaled: 1.4364 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3184 (0.6003)  loss_giou_unscaled: 0.4932 (0.8134)  cardinality_error_unscaled: 29.0000 (27.4015)  loss_ce_0_unscaled: 1.2579 (1.2775)  loss_bbox_0_unscaled: 0.3034 (0.5352)  loss_giou_0_unscaled: 0.4235 (0.7521)  cardinality_error_0_unscaled: 79.0000 (78.4415)  loss_ce_1_unscaled: 1.2255 (1.2435)  loss_bbox_1_unscaled: 0.3034 (0.5681)  loss_giou_1_unscaled: 0.4287 (0.7810)  cardinality_error_1_unscaled: 49.0000 (48.5414)  time: 0.1138  data: 0.0067  max mem: 594\n",
      "Test:  [1710/4410]  eta: 0:04:58  class_error: 100.00  loss: 5.4334 (7.4528)  loss_ce: 2.8666 (2.8344)  loss_bbox: 1.6006 (2.9948)  loss_giou: 0.9952 (1.6236)  loss_ce_unscaled: 1.4333 (1.4172)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3201 (0.5990)  loss_giou_unscaled: 0.4976 (0.8118)  cardinality_error_unscaled: 29.0000 (27.4109)  loss_ce_0_unscaled: 1.2604 (1.2774)  loss_bbox_0_unscaled: 0.3131 (0.5342)  loss_giou_0_unscaled: 0.4538 (0.7504)  cardinality_error_0_unscaled: 79.0000 (78.4448)  loss_ce_1_unscaled: 1.2266 (1.2434)  loss_bbox_1_unscaled: 0.3131 (0.5669)  loss_giou_1_unscaled: 0.4538 (0.7793)  cardinality_error_1_unscaled: 49.0000 (48.5441)  time: 0.1144  data: 0.0069  max mem: 594\n",
      "Test:  [1720/4410]  eta: 0:04:57  class_error: 100.00  loss: 6.2055 (7.4488)  loss_ce: 2.8537 (2.8345)  loss_bbox: 2.1863 (2.9922)  loss_giou: 1.2761 (1.6221)  loss_ce_unscaled: 1.4268 (1.4173)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4373 (0.5984)  loss_giou_unscaled: 0.6381 (0.8111)  cardinality_error_unscaled: 29.0000 (27.4201)  loss_ce_0_unscaled: 1.2604 (1.2773)  loss_bbox_0_unscaled: 0.3591 (0.5336)  loss_giou_0_unscaled: 0.5220 (0.7497)  cardinality_error_0_unscaled: 79.0000 (78.4468)  loss_ce_1_unscaled: 1.2227 (1.2432)  loss_bbox_1_unscaled: 0.4373 (0.5664)  loss_giou_1_unscaled: 0.5220 (0.7786)  cardinality_error_1_unscaled: 49.0000 (48.5468)  time: 0.1163  data: 0.0073  max mem: 594\n",
      "Test:  [1730/4410]  eta: 0:04:56  class_error: 100.00  loss: 6.8449 (7.4455)  loss_ce: 2.8522 (2.8347)  loss_bbox: 2.6558 (2.9907)  loss_giou: 1.2803 (1.6202)  loss_ce_unscaled: 1.4261 (1.4173)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5312 (0.5981)  loss_giou_unscaled: 0.6401 (0.8101)  cardinality_error_unscaled: 29.0000 (27.4287)  loss_ce_0_unscaled: 1.2549 (1.2772)  loss_bbox_0_unscaled: 0.4297 (0.5331)  loss_giou_0_unscaled: 0.5796 (0.7488)  cardinality_error_0_unscaled: 79.0000 (78.4500)  loss_ce_1_unscaled: 1.2131 (1.2428)  loss_bbox_1_unscaled: 0.5117 (0.5659)  loss_giou_1_unscaled: 0.6401 (0.7777)  cardinality_error_1_unscaled: 49.0000 (48.5494)  time: 0.1172  data: 0.0073  max mem: 594\n",
      "Test:  [1740/4410]  eta: 0:04:55  class_error: 100.00  loss: 7.4043 (7.4460)  loss_ce: 2.8522 (2.8346)  loss_bbox: 3.0022 (2.9915)  loss_giou: 1.5080 (1.6199)  loss_ce_unscaled: 1.4261 (1.4173)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6004 (0.5983)  loss_giou_unscaled: 0.7540 (0.8099)  cardinality_error_unscaled: 29.0000 (27.4377)  loss_ce_0_unscaled: 1.2529 (1.2771)  loss_bbox_0_unscaled: 0.4607 (0.5329)  loss_giou_0_unscaled: 0.5584 (0.7481)  cardinality_error_0_unscaled: 79.0000 (78.4520)  loss_ce_1_unscaled: 1.1813 (1.2425)  loss_bbox_1_unscaled: 0.5217 (0.5658)  loss_giou_1_unscaled: 0.6611 (0.7773)  cardinality_error_1_unscaled: 49.0000 (48.5520)  time: 0.1162  data: 0.0071  max mem: 594\n",
      "Test:  [1750/4410]  eta: 0:04:54  class_error: 100.00  loss: 7.7798 (7.4486)  loss_ce: 2.7999 (2.8344)  loss_bbox: 3.4495 (2.9942)  loss_giou: 1.6302 (1.6200)  loss_ce_unscaled: 1.4000 (1.4172)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6899 (0.5988)  loss_giou_unscaled: 0.8151 (0.8100)  cardinality_error_unscaled: 29.0000 (27.4466)  loss_ce_0_unscaled: 1.2525 (1.2769)  loss_bbox_0_unscaled: 0.4607 (0.5323)  loss_giou_0_unscaled: 0.5114 (0.7468)  cardinality_error_0_unscaled: 79.0000 (78.4523)  loss_ce_1_unscaled: 1.1449 (1.2419)  loss_bbox_1_unscaled: 0.5985 (0.5661)  loss_giou_1_unscaled: 0.7676 (0.7771)  cardinality_error_1_unscaled: 49.0000 (48.5545)  time: 0.1174  data: 0.0072  max mem: 594\n",
      "Test:  [1760/4410]  eta: 0:04:53  class_error: 100.00  loss: 7.7492 (7.4495)  loss_ce: 2.7854 (2.8343)  loss_bbox: 3.5054 (2.9957)  loss_giou: 1.5799 (1.6195)  loss_ce_unscaled: 1.3927 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7011 (0.5991)  loss_giou_unscaled: 0.7899 (0.8098)  cardinality_error_unscaled: 29.0000 (27.4543)  loss_ce_0_unscaled: 1.2472 (1.2767)  loss_bbox_0_unscaled: 0.3254 (0.5316)  loss_giou_0_unscaled: 0.5149 (0.7456)  cardinality_error_0_unscaled: 79.0000 (78.4532)  loss_ce_1_unscaled: 1.1284 (1.2414)  loss_bbox_1_unscaled: 0.5928 (0.5659)  loss_giou_1_unscaled: 0.7622 (0.7765)  cardinality_error_1_unscaled: 49.0000 (48.5571)  time: 0.1154  data: 0.0070  max mem: 594\n",
      "Test:  [1770/4410]  eta: 0:04:52  class_error: 100.00  loss: 7.9591 (7.4531)  loss_ce: 2.8084 (2.8342)  loss_bbox: 3.5988 (2.9992)  loss_giou: 1.5799 (1.6198)  loss_ce_unscaled: 1.4042 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7198 (0.5998)  loss_giou_unscaled: 0.7899 (0.8099)  cardinality_error_unscaled: 29.0000 (27.4630)  loss_ce_0_unscaled: 1.2472 (1.2766)  loss_bbox_0_unscaled: 0.4193 (0.5309)  loss_giou_0_unscaled: 0.5000 (0.7442)  cardinality_error_0_unscaled: 79.0000 (78.4534)  loss_ce_1_unscaled: 1.1388 (1.2409)  loss_bbox_1_unscaled: 0.5518 (0.5658)  loss_giou_1_unscaled: 0.6751 (0.7759)  cardinality_error_1_unscaled: 49.0000 (48.5596)  time: 0.1127  data: 0.0069  max mem: 594\n",
      "Test:  [1780/4410]  eta: 0:04:51  class_error: 100.00  loss: 7.5094 (7.4524)  loss_ce: 2.8443 (2.8343)  loss_bbox: 3.3639 (2.9994)  loss_giou: 1.4813 (1.6187)  loss_ce_unscaled: 1.4221 (1.4172)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6728 (0.5999)  loss_giou_unscaled: 0.7406 (0.8094)  cardinality_error_unscaled: 29.0000 (27.4716)  loss_ce_0_unscaled: 1.2756 (1.2767)  loss_bbox_0_unscaled: 0.4238 (0.5307)  loss_giou_0_unscaled: 0.5599 (0.7436)  cardinality_error_0_unscaled: 79.0000 (78.4559)  loss_ce_1_unscaled: 1.2053 (1.2411)  loss_bbox_1_unscaled: 0.5689 (0.5657)  loss_giou_1_unscaled: 0.7191 (0.7753)  cardinality_error_1_unscaled: 49.0000 (48.5620)  time: 0.1140  data: 0.0072  max mem: 594\n",
      "Test:  [1790/4410]  eta: 0:04:50  class_error: 100.00  loss: 7.2201 (7.4511)  loss_ce: 2.8487 (2.8344)  loss_bbox: 2.9242 (2.9992)  loss_giou: 1.4382 (1.6175)  loss_ce_unscaled: 1.4244 (1.4172)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5848 (0.5998)  loss_giou_unscaled: 0.7191 (0.8087)  cardinality_error_unscaled: 29.0000 (27.4802)  loss_ce_0_unscaled: 1.2818 (1.2767)  loss_bbox_0_unscaled: 0.5168 (0.5304)  loss_giou_0_unscaled: 0.5956 (0.7428)  cardinality_error_0_unscaled: 79.0000 (78.4567)  loss_ce_1_unscaled: 1.2807 (1.2413)  loss_bbox_1_unscaled: 0.5306 (0.5656)  loss_giou_1_unscaled: 0.7049 (0.7746)  cardinality_error_1_unscaled: 49.0000 (48.5645)  time: 0.1142  data: 0.0071  max mem: 594\n",
      "Test:  [1800/4410]  eta: 0:04:49  class_error: 100.00  loss: 7.1617 (7.4499)  loss_ce: 2.8487 (2.8345)  loss_bbox: 2.9100 (2.9990)  loss_giou: 1.4097 (1.6163)  loss_ce_unscaled: 1.4244 (1.4172)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5820 (0.5998)  loss_giou_unscaled: 0.7049 (0.8082)  cardinality_error_unscaled: 29.0000 (27.4886)  loss_ce_0_unscaled: 1.2790 (1.2768)  loss_bbox_0_unscaled: 0.5306 (0.5305)  loss_giou_0_unscaled: 0.6539 (0.7421)  cardinality_error_0_unscaled: 79.0000 (78.4586)  loss_ce_1_unscaled: 1.2745 (1.2414)  loss_bbox_1_unscaled: 0.5636 (0.5655)  loss_giou_1_unscaled: 0.6789 (0.7739)  cardinality_error_1_unscaled: 49.0000 (48.5669)  time: 0.1146  data: 0.0067  max mem: 594\n",
      "Test:  [1810/4410]  eta: 0:04:48  class_error: 100.00  loss: 7.2847 (7.4499)  loss_ce: 2.8134 (2.8343)  loss_bbox: 3.0551 (3.0000)  loss_giou: 1.4355 (1.6156)  loss_ce_unscaled: 1.4067 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6110 (0.6000)  loss_giou_unscaled: 0.7178 (0.8078)  cardinality_error_unscaled: 29.0000 (27.4970)  loss_ce_0_unscaled: 1.2847 (1.2768)  loss_bbox_0_unscaled: 0.5453 (0.5301)  loss_giou_0_unscaled: 0.5811 (0.7411)  cardinality_error_0_unscaled: 79.0000 (78.4578)  loss_ce_1_unscaled: 1.2348 (1.2414)  loss_bbox_1_unscaled: 0.5773 (0.5658)  loss_giou_1_unscaled: 0.6789 (0.7733)  cardinality_error_1_unscaled: 49.0000 (48.5693)  time: 0.1141  data: 0.0067  max mem: 594\n",
      "Test:  [1820/4410]  eta: 0:04:47  class_error: 100.00  loss: 7.0459 (7.4456)  loss_ce: 2.8119 (2.8343)  loss_bbox: 2.7071 (2.9978)  loss_giou: 1.4005 (1.6135)  loss_ce_unscaled: 1.4059 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5414 (0.5996)  loss_giou_unscaled: 0.7002 (0.8068)  cardinality_error_unscaled: 29.0000 (27.5052)  loss_ce_0_unscaled: 1.2870 (1.2769)  loss_bbox_0_unscaled: 0.4367 (0.5297)  loss_giou_0_unscaled: 0.5438 (0.7400)  cardinality_error_0_unscaled: 78.0000 (78.4552)  loss_ce_1_unscaled: 1.2546 (1.2415)  loss_bbox_1_unscaled: 0.5495 (0.5655)  loss_giou_1_unscaled: 0.6296 (0.7723)  cardinality_error_1_unscaled: 49.0000 (48.5717)  time: 0.1131  data: 0.0069  max mem: 594\n",
      "Test:  [1830/4410]  eta: 0:04:46  class_error: 100.00  loss: 6.8564 (7.4428)  loss_ce: 2.8259 (2.8343)  loss_bbox: 2.7050 (2.9969)  loss_giou: 1.3330 (1.6116)  loss_ce_unscaled: 1.4130 (1.4171)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5410 (0.5994)  loss_giou_unscaled: 0.6665 (0.8058)  cardinality_error_unscaled: 29.0000 (27.5134)  loss_ce_0_unscaled: 1.2876 (1.2769)  loss_bbox_0_unscaled: 0.4850 (0.5297)  loss_giou_0_unscaled: 0.5438 (0.7393)  cardinality_error_0_unscaled: 78.0000 (78.4533)  loss_ce_1_unscaled: 1.2594 (1.2416)  loss_bbox_1_unscaled: 0.5410 (0.5655)  loss_giou_1_unscaled: 0.5899 (0.7715)  cardinality_error_1_unscaled: 49.0000 (48.5740)  time: 0.1147  data: 0.0070  max mem: 594\n",
      "Test:  [1840/4410]  eta: 0:04:44  class_error: 100.00  loss: 6.8676 (7.4397)  loss_ce: 2.9533 (2.8350)  loss_bbox: 2.6526 (2.9946)  loss_giou: 1.3485 (1.6101)  loss_ce_unscaled: 1.4767 (1.4175)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5305 (0.5989)  loss_giou_unscaled: 0.6743 (0.8050)  cardinality_error_unscaled: 29.0000 (27.5215)  loss_ce_0_unscaled: 1.2288 (1.2766)  loss_bbox_0_unscaled: 0.5290 (0.5296)  loss_giou_0_unscaled: 0.6492 (0.7389)  cardinality_error_0_unscaled: 79.0000 (78.4563)  loss_ce_1_unscaled: 1.2417 (1.2415)  loss_bbox_1_unscaled: 0.5305 (0.5652)  loss_giou_1_unscaled: 0.6743 (0.7709)  cardinality_error_1_unscaled: 49.0000 (48.5763)  time: 0.1151  data: 0.0070  max mem: 594\n",
      "Test:  [1850/4410]  eta: 0:04:43  class_error: 100.00  loss: 6.9973 (7.4398)  loss_ce: 2.9682 (2.8357)  loss_bbox: 2.7072 (2.9952)  loss_giou: 1.3723 (1.6089)  loss_ce_unscaled: 1.4841 (1.4179)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5414 (0.5990)  loss_giou_unscaled: 0.6862 (0.8045)  cardinality_error_unscaled: 29.0000 (27.5294)  loss_ce_0_unscaled: 1.2137 (1.2762)  loss_bbox_0_unscaled: 0.5414 (0.5301)  loss_giou_0_unscaled: 0.6862 (0.7387)  cardinality_error_0_unscaled: 79.0000 (78.4592)  loss_ce_1_unscaled: 1.2295 (1.2415)  loss_bbox_1_unscaled: 0.5414 (0.5655)  loss_giou_1_unscaled: 0.6862 (0.7705)  cardinality_error_1_unscaled: 49.0000 (48.5786)  time: 0.1126  data: 0.0069  max mem: 594\n",
      "Test:  [1860/4410]  eta: 0:04:42  class_error: 100.00  loss: 7.2565 (7.4377)  loss_ce: 2.9631 (2.8363)  loss_bbox: 2.9008 (2.9938)  loss_giou: 1.3220 (1.6075)  loss_ce_unscaled: 1.4815 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5802 (0.5988)  loss_giou_unscaled: 0.6610 (0.8038)  cardinality_error_unscaled: 29.0000 (27.5368)  loss_ce_0_unscaled: 1.2133 (1.2759)  loss_bbox_0_unscaled: 0.5787 (0.5302)  loss_giou_0_unscaled: 0.6610 (0.7383)  cardinality_error_0_unscaled: 79.0000 (78.4600)  loss_ce_1_unscaled: 1.2355 (1.2415)  loss_bbox_1_unscaled: 0.5802 (0.5654)  loss_giou_1_unscaled: 0.6610 (0.7700)  cardinality_error_1_unscaled: 49.0000 (48.5803)  time: 0.1137  data: 0.0068  max mem: 594\n",
      "Test:  [1870/4410]  eta: 0:04:41  class_error: 100.00  loss: 7.0900 (7.4359)  loss_ce: 2.9068 (2.8363)  loss_bbox: 2.7654 (2.9928)  loss_giou: 1.3876 (1.6068)  loss_ce_unscaled: 1.4534 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5531 (0.5986)  loss_giou_unscaled: 0.6938 (0.8034)  cardinality_error_unscaled: 28.0000 (27.5393)  loss_ce_0_unscaled: 1.2380 (1.2759)  loss_bbox_0_unscaled: 0.5380 (0.5302)  loss_giou_0_unscaled: 0.6528 (0.7380)  cardinality_error_0_unscaled: 78.0000 (78.4532)  loss_ce_1_unscaled: 1.2573 (1.2418)  loss_bbox_1_unscaled: 0.5380 (0.5653)  loss_giou_1_unscaled: 0.6519 (0.7695)  cardinality_error_1_unscaled: 48.0000 (48.5772)  time: 0.1147  data: 0.0070  max mem: 594\n",
      "Test:  [1880/4410]  eta: 0:04:40  class_error: 100.00  loss: 7.1734 (7.4342)  loss_ce: 2.8363 (2.8364)  loss_bbox: 2.7698 (2.9917)  loss_giou: 1.4980 (1.6061)  loss_ce_unscaled: 1.4181 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5540 (0.5983)  loss_giou_unscaled: 0.7490 (0.8030)  cardinality_error_unscaled: 28.0000 (27.5417)  loss_ce_0_unscaled: 1.2671 (1.2759)  loss_bbox_0_unscaled: 0.5685 (0.5304)  loss_giou_0_unscaled: 0.6891 (0.7379)  cardinality_error_0_unscaled: 78.0000 (78.4508)  loss_ce_1_unscaled: 1.2930 (1.2420)  loss_bbox_1_unscaled: 0.5540 (0.5653)  loss_giou_1_unscaled: 0.6890 (0.7693)  cardinality_error_1_unscaled: 48.0000 (48.5742)  time: 0.1125  data: 0.0069  max mem: 594\n",
      "Test:  [1890/4410]  eta: 0:04:39  class_error: 100.00  loss: 7.1749 (7.4324)  loss_ce: 2.8255 (2.8364)  loss_bbox: 2.8294 (2.9909)  loss_giou: 1.5154 (1.6051)  loss_ce_unscaled: 1.4128 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5659 (0.5982)  loss_giou_unscaled: 0.7577 (0.8026)  cardinality_error_unscaled: 28.0000 (27.5442)  loss_ce_0_unscaled: 1.2619 (1.2758)  loss_bbox_0_unscaled: 0.5659 (0.5305)  loss_giou_0_unscaled: 0.6751 (0.7375)  cardinality_error_0_unscaled: 78.0000 (78.4474)  loss_ce_1_unscaled: 1.2893 (1.2422)  loss_bbox_1_unscaled: 0.5619 (0.5652)  loss_giou_1_unscaled: 0.6709 (0.7688)  cardinality_error_1_unscaled: 48.0000 (48.5717)  time: 0.1138  data: 0.0069  max mem: 594\n",
      "Test:  [1900/4410]  eta: 0:04:38  class_error: 100.00  loss: 7.2536 (7.4308)  loss_ce: 2.8083 (2.8364)  loss_bbox: 2.9422 (2.9908)  loss_giou: 1.3561 (1.6036)  loss_ce_unscaled: 1.4041 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5884 (0.5982)  loss_giou_unscaled: 0.6781 (0.8018)  cardinality_error_unscaled: 29.0000 (27.5518)  loss_ce_0_unscaled: 1.2717 (1.2758)  loss_bbox_0_unscaled: 0.5474 (0.5302)  loss_giou_0_unscaled: 0.6033 (0.7367)  cardinality_error_0_unscaled: 79.0000 (78.4503)  loss_ce_1_unscaled: 1.2363 (1.2420)  loss_bbox_1_unscaled: 0.5482 (0.5650)  loss_giou_1_unscaled: 0.6289 (0.7679)  cardinality_error_1_unscaled: 49.0000 (48.5739)  time: 0.1164  data: 0.0073  max mem: 594\n",
      "Test:  [1910/4410]  eta: 0:04:37  class_error: 100.00  loss: 7.3726 (7.4298)  loss_ce: 2.8219 (2.8364)  loss_bbox: 2.9621 (2.9907)  loss_giou: 1.3589 (1.6026)  loss_ce_unscaled: 1.4109 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5924 (0.5981)  loss_giou_unscaled: 0.6795 (0.8013)  cardinality_error_unscaled: 29.0000 (27.5594)  loss_ce_0_unscaled: 1.2717 (1.2758)  loss_bbox_0_unscaled: 0.4370 (0.5296)  loss_giou_0_unscaled: 0.5603 (0.7357)  cardinality_error_0_unscaled: 79.0000 (78.4521)  loss_ce_1_unscaled: 1.2244 (1.2419)  loss_bbox_1_unscaled: 0.5169 (0.5649)  loss_giou_1_unscaled: 0.6034 (0.7673)  cardinality_error_1_unscaled: 49.0000 (48.5761)  time: 0.1140  data: 0.0072  max mem: 594\n",
      "Test:  [1920/4410]  eta: 0:04:36  class_error: 100.00  loss: 7.3468 (7.4290)  loss_ce: 2.8219 (2.8364)  loss_bbox: 3.0608 (2.9908)  loss_giou: 1.4878 (1.6018)  loss_ce_unscaled: 1.4109 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6122 (0.5982)  loss_giou_unscaled: 0.7439 (0.8009)  cardinality_error_unscaled: 29.0000 (27.5664)  loss_ce_0_unscaled: 1.2660 (1.2758)  loss_bbox_0_unscaled: 0.4436 (0.5293)  loss_giou_0_unscaled: 0.5813 (0.7351)  cardinality_error_0_unscaled: 79.0000 (78.4529)  loss_ce_1_unscaled: 1.2367 (1.2419)  loss_bbox_1_unscaled: 0.5265 (0.5647)  loss_giou_1_unscaled: 0.6577 (0.7667)  cardinality_error_1_unscaled: 49.0000 (48.5778)  time: 0.1134  data: 0.0068  max mem: 594\n",
      "Test:  [1930/4410]  eta: 0:04:35  class_error: 100.00  loss: 7.3441 (7.4291)  loss_ce: 2.8022 (2.8362)  loss_bbox: 3.0608 (2.9909)  loss_giou: 1.5731 (1.6020)  loss_ce_unscaled: 1.4011 (1.4181)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6122 (0.5982)  loss_giou_unscaled: 0.7865 (0.8010)  cardinality_error_unscaled: 28.0000 (27.5686)  loss_ce_0_unscaled: 1.2686 (1.2758)  loss_bbox_0_unscaled: 0.5265 (0.5295)  loss_giou_0_unscaled: 0.6730 (0.7350)  cardinality_error_0_unscaled: 78.0000 (78.4505)  loss_ce_1_unscaled: 1.2453 (1.2420)  loss_bbox_1_unscaled: 0.5506 (0.5646)  loss_giou_1_unscaled: 0.7019 (0.7665)  cardinality_error_1_unscaled: 48.0000 (48.5748)  time: 0.1136  data: 0.0069  max mem: 594\n",
      "Test:  [1940/4410]  eta: 0:04:34  class_error: 100.00  loss: 7.3441 (7.4288)  loss_ce: 2.7930 (2.8360)  loss_bbox: 3.0456 (2.9910)  loss_giou: 1.6385 (1.6019)  loss_ce_unscaled: 1.3965 (1.4180)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6091 (0.5982)  loss_giou_unscaled: 0.8192 (0.8009)  cardinality_error_unscaled: 28.0000 (27.5708)  loss_ce_0_unscaled: 1.2643 (1.2757)  loss_bbox_0_unscaled: 0.5506 (0.5296)  loss_giou_0_unscaled: 0.7051 (0.7349)  cardinality_error_0_unscaled: 78.0000 (78.4467)  loss_ce_1_unscaled: 1.2537 (1.2420)  loss_bbox_1_unscaled: 0.5616 (0.5647)  loss_giou_1_unscaled: 0.7106 (0.7664)  cardinality_error_1_unscaled: 48.0000 (48.5719)  time: 0.1122  data: 0.0070  max mem: 594\n",
      "Test:  [1950/4410]  eta: 0:04:33  class_error: 100.00  loss: 6.9990 (7.4255)  loss_ce: 2.7960 (2.8358)  loss_bbox: 2.6816 (2.9886)  loss_giou: 1.5214 (1.6011)  loss_ce_unscaled: 1.3980 (1.4179)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5363 (0.5977)  loss_giou_unscaled: 0.7607 (0.8005)  cardinality_error_unscaled: 28.0000 (27.5736)  loss_ce_0_unscaled: 1.2611 (1.2757)  loss_bbox_0_unscaled: 0.5345 (0.5294)  loss_giou_0_unscaled: 0.7042 (0.7347)  cardinality_error_0_unscaled: 78.0000 (78.4439)  loss_ce_1_unscaled: 1.2499 (1.2421)  loss_bbox_1_unscaled: 0.5237 (0.5644)  loss_giou_1_unscaled: 0.7042 (0.7660)  cardinality_error_1_unscaled: 48.0000 (48.5695)  time: 0.1116  data: 0.0071  max mem: 594\n",
      "Test:  [1960/4410]  eta: 0:04:31  class_error: 100.00  loss: 6.4558 (7.4202)  loss_ce: 2.8141 (2.8359)  loss_bbox: 2.3166 (2.9846)  loss_giou: 1.3330 (1.5997)  loss_ce_unscaled: 1.4070 (1.4180)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4633 (0.5969)  loss_giou_unscaled: 0.6665 (0.7998)  cardinality_error_unscaled: 29.0000 (27.5808)  loss_ce_0_unscaled: 1.2682 (1.2758)  loss_bbox_0_unscaled: 0.4633 (0.5289)  loss_giou_0_unscaled: 0.6659 (0.7342)  cardinality_error_0_unscaled: 79.0000 (78.4467)  loss_ce_1_unscaled: 1.2841 (1.2424)  loss_bbox_1_unscaled: 0.4633 (0.5637)  loss_giou_1_unscaled: 0.6659 (0.7654)  cardinality_error_1_unscaled: 49.0000 (48.5716)  time: 0.1090  data: 0.0072  max mem: 594\n",
      "Test:  [1970/4410]  eta: 0:04:30  class_error: 100.00  loss: 6.5042 (7.4173)  loss_ce: 2.8438 (2.8360)  loss_bbox: 2.4486 (2.9828)  loss_giou: 1.3263 (1.5986)  loss_ce_unscaled: 1.4219 (1.4180)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4897 (0.5966)  loss_giou_unscaled: 0.6631 (0.7993)  cardinality_error_unscaled: 29.0000 (27.5880)  loss_ce_0_unscaled: 1.2882 (1.2759)  loss_bbox_0_unscaled: 0.4897 (0.5287)  loss_giou_0_unscaled: 0.6443 (0.7341)  cardinality_error_0_unscaled: 79.0000 (78.4475)  loss_ce_1_unscaled: 1.3077 (1.2427)  loss_bbox_1_unscaled: 0.4897 (0.5634)  loss_giou_1_unscaled: 0.6435 (0.7651)  cardinality_error_1_unscaled: 49.0000 (48.5738)  time: 0.1098  data: 0.0071  max mem: 594\n",
      "Test:  [1980/4410]  eta: 0:04:29  class_error: 100.00  loss: 6.8190 (7.4145)  loss_ce: 2.8366 (2.8360)  loss_bbox: 2.5176 (2.9809)  loss_giou: 1.4245 (1.5977)  loss_ce_unscaled: 1.4183 (1.4180)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5035 (0.5962)  loss_giou_unscaled: 0.7123 (0.7988)  cardinality_error_unscaled: 29.0000 (27.5946)  loss_ce_0_unscaled: 1.2873 (1.2760)  loss_bbox_0_unscaled: 0.4925 (0.5286)  loss_giou_0_unscaled: 0.6767 (0.7339)  cardinality_error_0_unscaled: 79.0000 (78.4493)  loss_ce_1_unscaled: 1.3104 (1.2431)  loss_bbox_1_unscaled: 0.4925 (0.5631)  loss_giou_1_unscaled: 0.6767 (0.7647)  cardinality_error_1_unscaled: 49.0000 (48.5755)  time: 0.1117  data: 0.0072  max mem: 594\n",
      "Test:  [1990/4410]  eta: 0:04:28  class_error: 100.00  loss: 7.0849 (7.4134)  loss_ce: 2.8210 (2.8359)  loss_bbox: 2.7375 (2.9804)  loss_giou: 1.4477 (1.5971)  loss_ce_unscaled: 1.4105 (1.4179)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5475 (0.5961)  loss_giou_unscaled: 0.7239 (0.7985)  cardinality_error_unscaled: 28.0000 (27.5967)  loss_ce_0_unscaled: 1.2737 (1.2760)  loss_bbox_0_unscaled: 0.5466 (0.5288)  loss_giou_0_unscaled: 0.6833 (0.7337)  cardinality_error_0_unscaled: 78.0000 (78.4455)  loss_ce_1_unscaled: 1.2932 (1.2432)  loss_bbox_1_unscaled: 0.5466 (0.5632)  loss_giou_1_unscaled: 0.6833 (0.7644)  cardinality_error_1_unscaled: 48.0000 (48.5726)  time: 0.1100  data: 0.0070  max mem: 594\n",
      "Test:  [2000/4410]  eta: 0:04:27  class_error: 100.00  loss: 7.0178 (7.4121)  loss_ce: 2.8068 (2.8357)  loss_bbox: 2.7918 (2.9798)  loss_giou: 1.4477 (1.5965)  loss_ce_unscaled: 1.4034 (1.4179)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5584 (0.5960)  loss_giou_unscaled: 0.7239 (0.7983)  cardinality_error_unscaled: 28.0000 (27.5987)  loss_ce_0_unscaled: 1.2673 (1.2759)  loss_bbox_0_unscaled: 0.5600 (0.5289)  loss_giou_0_unscaled: 0.6833 (0.7335)  cardinality_error_0_unscaled: 78.0000 (78.4428)  loss_ce_1_unscaled: 1.2811 (1.2434)  loss_bbox_1_unscaled: 0.5589 (0.5632)  loss_giou_1_unscaled: 0.6833 (0.7641)  cardinality_error_1_unscaled: 48.0000 (48.5697)  time: 0.1091  data: 0.0067  max mem: 594\n",
      "Test:  [2010/4410]  eta: 0:04:26  class_error: 100.00  loss: 7.4652 (7.4139)  loss_ce: 2.8135 (2.8356)  loss_bbox: 3.0633 (2.9815)  loss_giou: 1.5813 (1.5968)  loss_ce_unscaled: 1.4067 (1.4178)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6127 (0.5963)  loss_giou_unscaled: 0.7906 (0.7984)  cardinality_error_unscaled: 28.0000 (27.6007)  loss_ce_0_unscaled: 1.2614 (1.2759)  loss_bbox_0_unscaled: 0.5990 (0.5294)  loss_giou_0_unscaled: 0.6963 (0.7334)  cardinality_error_0_unscaled: 78.0000 (78.4391)  loss_ce_1_unscaled: 1.2709 (1.2435)  loss_bbox_1_unscaled: 0.5990 (0.5636)  loss_giou_1_unscaled: 0.7061 (0.7640)  cardinality_error_1_unscaled: 48.0000 (48.5669)  time: 0.1106  data: 0.0067  max mem: 594\n",
      "Test:  [2020/4410]  eta: 0:04:25  class_error: 100.00  loss: 7.4947 (7.4133)  loss_ce: 2.8180 (2.8356)  loss_bbox: 3.0633 (2.9808)  loss_giou: 1.5813 (1.5968)  loss_ce_unscaled: 1.4090 (1.4178)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6127 (0.5962)  loss_giou_unscaled: 0.7906 (0.7984)  cardinality_error_unscaled: 28.0000 (27.6027)  loss_ce_0_unscaled: 1.2696 (1.2759)  loss_bbox_0_unscaled: 0.5053 (0.5290)  loss_giou_0_unscaled: 0.6725 (0.7328)  cardinality_error_0_unscaled: 78.0000 (78.4344)  loss_ce_1_unscaled: 1.2552 (1.2435)  loss_bbox_1_unscaled: 0.6080 (0.5635)  loss_giou_1_unscaled: 0.7182 (0.7637)  cardinality_error_1_unscaled: 48.0000 (48.5641)  time: 0.1113  data: 0.0066  max mem: 594\n",
      "Test:  [2030/4410]  eta: 0:04:24  class_error: 100.00  loss: 7.4176 (7.4147)  loss_ce: 2.8651 (2.8359)  loss_bbox: 2.9481 (2.9816)  loss_giou: 1.6120 (1.5973)  loss_ce_unscaled: 1.4326 (1.4179)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5896 (0.5963)  loss_giou_unscaled: 0.8060 (0.7986)  cardinality_error_unscaled: 28.0000 (27.6041)  loss_ce_0_unscaled: 1.2716 (1.2758)  loss_bbox_0_unscaled: 0.4885 (0.5288)  loss_giou_0_unscaled: 0.6256 (0.7324)  cardinality_error_0_unscaled: 78.0000 (78.4313)  loss_ce_1_unscaled: 1.2407 (1.2435)  loss_bbox_1_unscaled: 0.5385 (0.5637)  loss_giou_1_unscaled: 0.7358 (0.7637)  cardinality_error_1_unscaled: 48.0000 (48.5613)  time: 0.1097  data: 0.0066  max mem: 594\n",
      "Test:  [2040/4410]  eta: 0:04:23  class_error: 100.00  loss: 7.3205 (7.4148)  loss_ce: 2.8672 (2.8359)  loss_bbox: 2.9515 (2.9817)  loss_giou: 1.6783 (1.5972)  loss_ce_unscaled: 1.4336 (1.4180)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5903 (0.5963)  loss_giou_unscaled: 0.8391 (0.7986)  cardinality_error_unscaled: 28.0000 (27.6056)  loss_ce_0_unscaled: 1.2704 (1.2759)  loss_bbox_0_unscaled: 0.5160 (0.5287)  loss_giou_0_unscaled: 0.6256 (0.7319)  cardinality_error_0_unscaled: 78.0000 (78.4263)  loss_ce_1_unscaled: 1.2452 (1.2436)  loss_bbox_1_unscaled: 0.5944 (0.5638)  loss_giou_1_unscaled: 0.7507 (0.7635)  cardinality_error_1_unscaled: 48.0000 (48.5590)  time: 0.1092  data: 0.0066  max mem: 594\n",
      "Test:  [2050/4410]  eta: 0:04:21  class_error: 100.00  loss: 7.2389 (7.4128)  loss_ce: 2.9082 (2.8364)  loss_bbox: 2.9139 (2.9815)  loss_giou: 1.2044 (1.5949)  loss_ce_unscaled: 1.4541 (1.4182)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5828 (0.5963)  loss_giou_unscaled: 0.6022 (0.7975)  cardinality_error_unscaled: 29.0000 (27.6124)  loss_ce_0_unscaled: 1.3025 (1.2761)  loss_bbox_0_unscaled: 0.5516 (0.5290)  loss_giou_0_unscaled: 0.5943 (0.7310)  cardinality_error_0_unscaled: 79.0000 (78.4291)  loss_ce_1_unscaled: 1.2412 (1.2435)  loss_bbox_1_unscaled: 0.5828 (0.5639)  loss_giou_1_unscaled: 0.6011 (0.7625)  cardinality_error_1_unscaled: 49.0000 (48.5612)  time: 0.1093  data: 0.0066  max mem: 594\n",
      "Test:  [2060/4410]  eta: 0:04:20  class_error: 100.00  loss: 7.1753 (7.4111)  loss_ce: 2.9221 (2.8368)  loss_bbox: 2.9776 (2.9815)  loss_giou: 1.1022 (1.5929)  loss_ce_unscaled: 1.4610 (1.4184)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5955 (0.5963)  loss_giou_unscaled: 0.5511 (0.7964)  cardinality_error_unscaled: 29.0000 (27.6191)  loss_ce_0_unscaled: 1.3127 (1.2763)  loss_bbox_0_unscaled: 0.5813 (0.5292)  loss_giou_0_unscaled: 0.5406 (0.7303)  cardinality_error_0_unscaled: 79.0000 (78.4318)  loss_ce_1_unscaled: 1.2271 (1.2434)  loss_bbox_1_unscaled: 0.5931 (0.5641)  loss_giou_1_unscaled: 0.5511 (0.7617)  cardinality_error_1_unscaled: 49.0000 (48.5633)  time: 0.1095  data: 0.0066  max mem: 594\n",
      "Test:  [2070/4410]  eta: 0:04:19  class_error: 100.00  loss: 7.1542 (7.4093)  loss_ce: 2.9009 (2.8371)  loss_bbox: 2.9657 (2.9808)  loss_giou: 1.2255 (1.5914)  loss_ce_unscaled: 1.4505 (1.4185)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5931 (0.5962)  loss_giou_unscaled: 0.6128 (0.7957)  cardinality_error_unscaled: 29.0000 (27.6258)  loss_ce_0_unscaled: 1.3085 (1.2765)  loss_bbox_0_unscaled: 0.5783 (0.5294)  loss_giou_0_unscaled: 0.5511 (0.7295)  cardinality_error_0_unscaled: 79.0000 (78.4346)  loss_ce_1_unscaled: 1.2332 (1.2435)  loss_bbox_1_unscaled: 0.5931 (0.5641)  loss_giou_1_unscaled: 0.5591 (0.7608)  cardinality_error_1_unscaled: 49.0000 (48.5654)  time: 0.1107  data: 0.0067  max mem: 594\n",
      "Test:  [2080/4410]  eta: 0:04:18  class_error: 100.00  loss: 7.0403 (7.4082)  loss_ce: 2.8683 (2.8372)  loss_bbox: 2.9024 (2.9805)  loss_giou: 1.3537 (1.5905)  loss_ce_unscaled: 1.4342 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5805 (0.5961)  loss_giou_unscaled: 0.6768 (0.7952)  cardinality_error_unscaled: 29.0000 (27.6295)  loss_ce_0_unscaled: 1.2982 (1.2765)  loss_bbox_0_unscaled: 0.5805 (0.5297)  loss_giou_0_unscaled: 0.5509 (0.7288)  cardinality_error_0_unscaled: 79.0000 (78.4363)  loss_ce_1_unscaled: 1.2680 (1.2438)  loss_bbox_1_unscaled: 0.5805 (0.5642)  loss_giou_1_unscaled: 0.5892 (0.7602)  cardinality_error_1_unscaled: 49.0000 (48.5675)  time: 0.1117  data: 0.0069  max mem: 594\n",
      "Test:  [2090/4410]  eta: 0:04:17  class_error: 100.00  loss: 7.1753 (7.4067)  loss_ce: 2.8582 (2.8373)  loss_bbox: 2.9024 (2.9799)  loss_giou: 1.3689 (1.5895)  loss_ce_unscaled: 1.4291 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5805 (0.5960)  loss_giou_unscaled: 0.6845 (0.7948)  cardinality_error_unscaled: 29.0000 (27.6332)  loss_ce_0_unscaled: 1.2799 (1.2765)  loss_bbox_0_unscaled: 0.5574 (0.5296)  loss_giou_0_unscaled: 0.5961 (0.7282)  cardinality_error_0_unscaled: 79.0000 (78.4390)  loss_ce_1_unscaled: 1.2931 (1.2440)  loss_bbox_1_unscaled: 0.5740 (0.5640)  loss_giou_1_unscaled: 0.6237 (0.7595)  cardinality_error_1_unscaled: 49.0000 (48.5696)  time: 0.1134  data: 0.0070  max mem: 594\n",
      "Test:  [2100/4410]  eta: 0:04:16  class_error: 100.00  loss: 7.1907 (7.4053)  loss_ce: 2.8477 (2.8373)  loss_bbox: 2.9127 (2.9793)  loss_giou: 1.4323 (1.5887)  loss_ce_unscaled: 1.4239 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5825 (0.5959)  loss_giou_unscaled: 0.7161 (0.7944)  cardinality_error_unscaled: 29.0000 (27.6392)  loss_ce_0_unscaled: 1.2907 (1.2766)  loss_bbox_0_unscaled: 0.5072 (0.5295)  loss_giou_0_unscaled: 0.5681 (0.7275)  cardinality_error_0_unscaled: 79.0000 (78.4398)  loss_ce_1_unscaled: 1.2878 (1.2442)  loss_bbox_1_unscaled: 0.5590 (0.5639)  loss_giou_1_unscaled: 0.6237 (0.7592)  cardinality_error_1_unscaled: 49.0000 (48.5712)  time: 0.1119  data: 0.0070  max mem: 594\n",
      "Test:  [2110/4410]  eta: 0:04:15  class_error: 100.00  loss: 7.3926 (7.4049)  loss_ce: 2.8586 (2.8374)  loss_bbox: 3.0072 (2.9790)  loss_giou: 1.5310 (1.5886)  loss_ce_unscaled: 1.4293 (1.4187)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6014 (0.5958)  loss_giou_unscaled: 0.7655 (0.7943)  cardinality_error_unscaled: 28.0000 (27.6409)  loss_ce_0_unscaled: 1.2866 (1.2767)  loss_bbox_0_unscaled: 0.5741 (0.5297)  loss_giou_0_unscaled: 0.7085 (0.7276)  cardinality_error_0_unscaled: 78.0000 (78.4377)  loss_ce_1_unscaled: 1.3244 (1.2446)  loss_bbox_1_unscaled: 0.6014 (0.5640)  loss_giou_1_unscaled: 0.7531 (0.7592)  cardinality_error_1_unscaled: 48.0000 (48.5685)  time: 0.1103  data: 0.0067  max mem: 594\n",
      "Test:  [2120/4410]  eta: 0:04:14  class_error: 100.00  loss: 7.4328 (7.4048)  loss_ce: 2.8586 (2.8375)  loss_bbox: 3.0072 (2.9784)  loss_giou: 1.5587 (1.5888)  loss_ce_unscaled: 1.4293 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6014 (0.5957)  loss_giou_unscaled: 0.7793 (0.7944)  cardinality_error_unscaled: 28.0000 (27.6426)  loss_ce_0_unscaled: 1.2846 (1.2767)  loss_bbox_0_unscaled: 0.6014 (0.5298)  loss_giou_0_unscaled: 0.7647 (0.7277)  cardinality_error_0_unscaled: 78.0000 (78.4356)  loss_ce_1_unscaled: 1.3287 (1.2450)  loss_bbox_1_unscaled: 0.5741 (0.5639)  loss_giou_1_unscaled: 0.7560 (0.7592)  cardinality_error_1_unscaled: 48.0000 (48.5658)  time: 0.1115  data: 0.0068  max mem: 594\n",
      "Test:  [2130/4410]  eta: 0:04:13  class_error: 100.00  loss: 7.4510 (7.4036)  loss_ce: 2.8684 (2.8377)  loss_bbox: 3.0152 (2.9776)  loss_giou: 1.5608 (1.5883)  loss_ce_unscaled: 1.4342 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6030 (0.5955)  loss_giou_unscaled: 0.7804 (0.7941)  cardinality_error_unscaled: 28.0000 (27.6443)  loss_ce_0_unscaled: 1.2883 (1.2767)  loss_bbox_0_unscaled: 0.5236 (0.5298)  loss_giou_0_unscaled: 0.7532 (0.7277)  cardinality_error_0_unscaled: 78.0000 (78.4336)  loss_ce_1_unscaled: 1.3160 (1.2453)  loss_bbox_1_unscaled: 0.5236 (0.5638)  loss_giou_1_unscaled: 0.7501 (0.7591)  cardinality_error_1_unscaled: 48.0000 (48.5631)  time: 0.1146  data: 0.0071  max mem: 594\n",
      "Test:  [2140/4410]  eta: 0:04:12  class_error: 100.00  loss: 7.4510 (7.4032)  loss_ce: 2.8593 (2.8378)  loss_bbox: 3.0317 (2.9777)  loss_giou: 1.5261 (1.5877)  loss_ce_unscaled: 1.4296 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6063 (0.5955)  loss_giou_unscaled: 0.7631 (0.7939)  cardinality_error_unscaled: 28.0000 (27.6460)  loss_ce_0_unscaled: 1.2768 (1.2767)  loss_bbox_0_unscaled: 0.5236 (0.5299)  loss_giou_0_unscaled: 0.7037 (0.7272)  cardinality_error_0_unscaled: 78.0000 (78.4316)  loss_ce_1_unscaled: 1.2979 (1.2456)  loss_bbox_1_unscaled: 0.5236 (0.5637)  loss_giou_1_unscaled: 0.7158 (0.7587)  cardinality_error_1_unscaled: 48.0000 (48.5605)  time: 0.1169  data: 0.0074  max mem: 594\n",
      "Test:  [2150/4410]  eta: 0:04:11  class_error: 100.00  loss: 7.3817 (7.4019)  loss_ce: 2.8592 (2.8379)  loss_bbox: 2.9916 (2.9772)  loss_giou: 1.3799 (1.5868)  loss_ce_unscaled: 1.4296 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5983 (0.5954)  loss_giou_unscaled: 0.6899 (0.7934)  cardinality_error_unscaled: 28.0000 (27.6476)  loss_ce_0_unscaled: 1.2756 (1.2767)  loss_bbox_0_unscaled: 0.5204 (0.5299)  loss_giou_0_unscaled: 0.6231 (0.7268)  cardinality_error_0_unscaled: 78.0000 (78.4296)  loss_ce_1_unscaled: 1.2961 (1.2458)  loss_bbox_1_unscaled: 0.5219 (0.5636)  loss_giou_1_unscaled: 0.6642 (0.7581)  cardinality_error_1_unscaled: 48.0000 (48.5579)  time: 0.1161  data: 0.0075  max mem: 594\n",
      "Test:  [2160/4410]  eta: 0:04:09  class_error: 100.00  loss: 6.9491 (7.3996)  loss_ce: 2.8697 (2.8380)  loss_bbox: 2.5992 (2.9755)  loss_giou: 1.3563 (1.5861)  loss_ce_unscaled: 1.4348 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5198 (0.5951)  loss_giou_unscaled: 0.6781 (0.7931)  cardinality_error_unscaled: 28.0000 (27.6492)  loss_ce_0_unscaled: 1.2806 (1.2767)  loss_bbox_0_unscaled: 0.5115 (0.5299)  loss_giou_0_unscaled: 0.6490 (0.7265)  cardinality_error_0_unscaled: 78.0000 (78.4276)  loss_ce_1_unscaled: 1.2997 (1.2461)  loss_bbox_1_unscaled: 0.5059 (0.5635)  loss_giou_1_unscaled: 0.6499 (0.7577)  cardinality_error_1_unscaled: 48.0000 (48.5553)  time: 0.1170  data: 0.0074  max mem: 594\n",
      "Test:  [2170/4410]  eta: 0:04:08  class_error: 100.00  loss: 7.1428 (7.3988)  loss_ce: 2.8460 (2.8380)  loss_bbox: 2.8106 (2.9750)  loss_giou: 1.5103 (1.5859)  loss_ce_unscaled: 1.4230 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5621 (0.5950)  loss_giou_unscaled: 0.7551 (0.7929)  cardinality_error_unscaled: 28.0000 (27.6509)  loss_ce_0_unscaled: 1.2780 (1.2767)  loss_bbox_0_unscaled: 0.5079 (0.5298)  loss_giou_0_unscaled: 0.6673 (0.7262)  cardinality_error_0_unscaled: 78.0000 (78.4251)  loss_ce_1_unscaled: 1.2949 (1.2462)  loss_bbox_1_unscaled: 0.5659 (0.5635)  loss_giou_1_unscaled: 0.6699 (0.7574)  cardinality_error_1_unscaled: 48.0000 (48.5527)  time: 0.1129  data: 0.0071  max mem: 594\n",
      "Test:  [2180/4410]  eta: 0:04:07  class_error: 100.00  loss: 7.0783 (7.3960)  loss_ce: 2.8139 (2.8379)  loss_bbox: 2.7956 (2.9730)  loss_giou: 1.4933 (1.5852)  loss_ce_unscaled: 1.4069 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5591 (0.5946)  loss_giou_unscaled: 0.7466 (0.7926)  cardinality_error_unscaled: 28.0000 (27.6520)  loss_ce_0_unscaled: 1.2721 (1.2767)  loss_bbox_0_unscaled: 0.5027 (0.5296)  loss_giou_0_unscaled: 0.6695 (0.7259)  cardinality_error_0_unscaled: 78.0000 (78.4227)  loss_ce_1_unscaled: 1.2611 (1.2463)  loss_bbox_1_unscaled: 0.5376 (0.5632)  loss_giou_1_unscaled: 0.6946 (0.7571)  cardinality_error_1_unscaled: 48.0000 (48.5502)  time: 0.1084  data: 0.0065  max mem: 594\n",
      "Test:  [2190/4410]  eta: 0:04:06  class_error: 100.00  loss: 7.0547 (7.3950)  loss_ce: 2.8272 (2.8378)  loss_bbox: 2.7956 (2.9724)  loss_giou: 1.4579 (1.5848)  loss_ce_unscaled: 1.4136 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5591 (0.5945)  loss_giou_unscaled: 0.7289 (0.7924)  cardinality_error_unscaled: 28.0000 (27.6531)  loss_ce_0_unscaled: 1.2699 (1.2767)  loss_bbox_0_unscaled: 0.4889 (0.5295)  loss_giou_0_unscaled: 0.6497 (0.7256)  cardinality_error_0_unscaled: 78.0000 (78.4208)  loss_ce_1_unscaled: 1.2611 (1.2463)  loss_bbox_1_unscaled: 0.5027 (0.5630)  loss_giou_1_unscaled: 0.7004 (0.7568)  cardinality_error_1_unscaled: 48.0000 (48.5477)  time: 0.1101  data: 0.0065  max mem: 594\n",
      "Test:  [2200/4410]  eta: 0:04:05  class_error: 100.00  loss: 7.2022 (7.3937)  loss_ce: 2.8142 (2.8376)  loss_bbox: 2.9799 (2.9719)  loss_giou: 1.4617 (1.5842)  loss_ce_unscaled: 1.4071 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5960 (0.5944)  loss_giou_unscaled: 0.7308 (0.7921)  cardinality_error_unscaled: 28.0000 (27.6542)  loss_ce_0_unscaled: 1.2626 (1.2766)  loss_bbox_0_unscaled: 0.4981 (0.5294)  loss_giou_0_unscaled: 0.6526 (0.7255)  cardinality_error_0_unscaled: 78.0000 (78.4184)  loss_ce_1_unscaled: 1.2686 (1.2464)  loss_bbox_1_unscaled: 0.5336 (0.5630)  loss_giou_1_unscaled: 0.7304 (0.7567)  cardinality_error_1_unscaled: 48.0000 (48.5452)  time: 0.1121  data: 0.0068  max mem: 594\n",
      "Test:  [2210/4410]  eta: 0:04:04  class_error: 100.00  loss: 7.2392 (7.3935)  loss_ce: 2.7901 (2.8374)  loss_bbox: 2.8925 (2.9718)  loss_giou: 1.5364 (1.5843)  loss_ce_unscaled: 1.3951 (1.4187)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5785 (0.5944)  loss_giou_unscaled: 0.7682 (0.7921)  cardinality_error_unscaled: 28.0000 (27.6558)  loss_ce_0_unscaled: 1.2281 (1.2764)  loss_bbox_0_unscaled: 0.5293 (0.5295)  loss_giou_0_unscaled: 0.7578 (0.7256)  cardinality_error_0_unscaled: 78.0000 (78.4156)  loss_ce_1_unscaled: 1.2472 (1.2464)  loss_bbox_1_unscaled: 0.5594 (0.5629)  loss_giou_1_unscaled: 0.7728 (0.7568)  cardinality_error_1_unscaled: 48.0000 (48.5427)  time: 0.1132  data: 0.0072  max mem: 594\n",
      "Test:  [2220/4410]  eta: 0:04:03  class_error: 100.00  loss: 7.1359 (7.3934)  loss_ce: 2.7901 (2.8372)  loss_bbox: 2.8902 (2.9722)  loss_giou: 1.5457 (1.5840)  loss_ce_unscaled: 1.3951 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5780 (0.5944)  loss_giou_unscaled: 0.7728 (0.7920)  cardinality_error_unscaled: 28.0000 (27.6578)  loss_ce_0_unscaled: 1.2264 (1.2762)  loss_bbox_0_unscaled: 0.5530 (0.5296)  loss_giou_0_unscaled: 0.7625 (0.7255)  cardinality_error_0_unscaled: 78.0000 (78.4133)  loss_ce_1_unscaled: 1.2447 (1.2464)  loss_bbox_1_unscaled: 0.5696 (0.5631)  loss_giou_1_unscaled: 0.7751 (0.7568)  cardinality_error_1_unscaled: 48.0000 (48.5407)  time: 0.1128  data: 0.0072  max mem: 594\n",
      "Test:  [2230/4410]  eta: 0:04:02  class_error: 100.00  loss: 8.1544 (7.4010)  loss_ce: 2.7962 (2.8372)  loss_bbox: 3.4094 (2.9774)  loss_giou: 1.8292 (1.5863)  loss_ce_unscaled: 1.3981 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6819 (0.5955)  loss_giou_unscaled: 0.9146 (0.7931)  cardinality_error_unscaled: 29.0000 (27.6638)  loss_ce_0_unscaled: 1.2609 (1.2763)  loss_bbox_0_unscaled: 0.6023 (0.5300)  loss_giou_0_unscaled: 0.7767 (0.7262)  cardinality_error_0_unscaled: 79.0000 (78.4160)  loss_ce_1_unscaled: 1.2270 (1.2462)  loss_bbox_1_unscaled: 0.6602 (0.5639)  loss_giou_1_unscaled: 0.8831 (0.7581)  cardinality_error_1_unscaled: 49.0000 (48.5428)  time: 0.1128  data: 0.0069  max mem: 594\n",
      "Test:  [2240/4410]  eta: 0:04:01  class_error: 100.00  loss: 8.7952 (7.4071)  loss_ce: 2.8218 (2.8372)  loss_bbox: 3.9909 (2.9813)  loss_giou: 1.9980 (1.5886)  loss_ce_unscaled: 1.4109 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7982 (0.5963)  loss_giou_unscaled: 0.9990 (0.7943)  cardinality_error_unscaled: 29.0000 (27.6698)  loss_ce_0_unscaled: 1.2816 (1.2763)  loss_bbox_0_unscaled: 0.6023 (0.5301)  loss_giou_0_unscaled: 0.8437 (0.7267)  cardinality_error_0_unscaled: 79.0000 (78.4181)  loss_ce_1_unscaled: 1.1957 (1.2459)  loss_bbox_1_unscaled: 0.7090 (0.5646)  loss_giou_1_unscaled: 1.0542 (0.7594)  cardinality_error_1_unscaled: 49.0000 (48.5448)  time: 0.1106  data: 0.0068  max mem: 594\n",
      "Test:  [2250/4410]  eta: 0:03:59  class_error: 100.00  loss: 8.8252 (7.4123)  loss_ce: 2.8406 (2.8372)  loss_bbox: 3.7535 (2.9848)  loss_giou: 1.9980 (1.5903)  loss_ce_unscaled: 1.4203 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7507 (0.5970)  loss_giou_unscaled: 0.9990 (0.7952)  cardinality_error_unscaled: 29.0000 (27.6753)  loss_ce_0_unscaled: 1.2828 (1.2764)  loss_bbox_0_unscaled: 0.6183 (0.5304)  loss_giou_0_unscaled: 0.8330 (0.7270)  cardinality_error_0_unscaled: 79.0000 (78.4185)  loss_ce_1_unscaled: 1.1957 (1.2457)  loss_bbox_1_unscaled: 0.6866 (0.5652)  loss_giou_1_unscaled: 0.9711 (0.7605)  cardinality_error_1_unscaled: 49.0000 (48.5464)  time: 0.1100  data: 0.0067  max mem: 594\n",
      "Test:  [2260/4410]  eta: 0:03:58  class_error: 100.00  loss: 7.9158 (7.4123)  loss_ce: 2.8328 (2.8372)  loss_bbox: 3.0558 (2.9846)  loss_giou: 1.6812 (1.5905)  loss_ce_unscaled: 1.4164 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6112 (0.5969)  loss_giou_unscaled: 0.8406 (0.7953)  cardinality_error_unscaled: 28.0000 (27.6767)  loss_ce_0_unscaled: 1.2863 (1.2765)  loss_bbox_0_unscaled: 0.4783 (0.5301)  loss_giou_0_unscaled: 0.7498 (0.7268)  cardinality_error_0_unscaled: 78.0000 (78.4109)  loss_ce_1_unscaled: 1.2318 (1.2457)  loss_bbox_1_unscaled: 0.5669 (0.5650)  loss_giou_1_unscaled: 0.8120 (0.7604)  cardinality_error_1_unscaled: 48.0000 (48.5440)  time: 0.1128  data: 0.0068  max mem: 594\n",
      "Test:  [2270/4410]  eta: 0:03:57  class_error: 100.00  loss: 7.3514 (7.4123)  loss_ce: 2.8303 (2.8371)  loss_bbox: 2.9245 (2.9846)  loss_giou: 1.6191 (1.5905)  loss_ce_unscaled: 1.4151 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5849 (0.5969)  loss_giou_unscaled: 0.8096 (0.7953)  cardinality_error_unscaled: 28.0000 (27.6772)  loss_ce_0_unscaled: 1.3041 (1.2766)  loss_bbox_0_unscaled: 0.4783 (0.5298)  loss_giou_0_unscaled: 0.7132 (0.7267)  cardinality_error_0_unscaled: 77.0000 (78.4025)  loss_ce_1_unscaled: 1.2488 (1.2458)  loss_bbox_1_unscaled: 0.5650 (0.5651)  loss_giou_1_unscaled: 0.7552 (0.7604)  cardinality_error_1_unscaled: 48.0000 (48.5416)  time: 0.1134  data: 0.0068  max mem: 594\n",
      "Test:  [2280/4410]  eta: 0:03:56  class_error: 100.00  loss: 7.3713 (7.4116)  loss_ce: 2.8457 (2.8371)  loss_bbox: 2.9256 (2.9838)  loss_giou: 1.6019 (1.5906)  loss_ce_unscaled: 1.4228 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5851 (0.5968)  loss_giou_unscaled: 0.8010 (0.7953)  cardinality_error_unscaled: 28.0000 (27.6791)  loss_ce_0_unscaled: 1.3045 (1.2767)  loss_bbox_0_unscaled: 0.4873 (0.5295)  loss_giou_0_unscaled: 0.7085 (0.7267)  cardinality_error_0_unscaled: 77.0000 (78.3937)  loss_ce_1_unscaled: 1.2488 (1.2458)  loss_bbox_1_unscaled: 0.5647 (0.5649)  loss_giou_1_unscaled: 0.7552 (0.7604)  cardinality_error_1_unscaled: 48.0000 (48.5397)  time: 0.1134  data: 0.0067  max mem: 594\n",
      "Test:  [2290/4410]  eta: 0:03:55  class_error: 100.00  loss: 7.0632 (7.4080)  loss_ce: 2.8246 (2.8371)  loss_bbox: 2.5240 (2.9802)  loss_giou: 1.6351 (1.5908)  loss_ce_unscaled: 1.4123 (1.4185)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5048 (0.5960)  loss_giou_unscaled: 0.8176 (0.7954)  cardinality_error_unscaled: 29.0000 (27.6849)  loss_ce_0_unscaled: 1.2864 (1.2768)  loss_bbox_0_unscaled: 0.4205 (0.5290)  loss_giou_0_unscaled: 0.7084 (0.7266)  cardinality_error_0_unscaled: 77.0000 (78.3920)  loss_ce_1_unscaled: 1.2564 (1.2460)  loss_bbox_1_unscaled: 0.4936 (0.5643)  loss_giou_1_unscaled: 0.7587 (0.7605)  cardinality_error_1_unscaled: 49.0000 (48.5417)  time: 0.1143  data: 0.0070  max mem: 594\n",
      "Test:  [2300/4410]  eta: 0:03:54  class_error: 100.00  loss: 6.8471 (7.4058)  loss_ce: 2.8091 (2.8370)  loss_bbox: 2.3211 (2.9775)  loss_giou: 1.7149 (1.5913)  loss_ce_unscaled: 1.4045 (1.4185)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4642 (0.5955)  loss_giou_unscaled: 0.8575 (0.7957)  cardinality_error_unscaled: 29.0000 (27.6906)  loss_ce_0_unscaled: 1.3268 (1.2770)  loss_bbox_0_unscaled: 0.3922 (0.5284)  loss_giou_0_unscaled: 0.6481 (0.7264)  cardinality_error_0_unscaled: 78.0000 (78.3872)  loss_ce_1_unscaled: 1.2986 (1.2462)  loss_bbox_1_unscaled: 0.4642 (0.5640)  loss_giou_1_unscaled: 0.8149 (0.7609)  cardinality_error_1_unscaled: 49.0000 (48.5437)  time: 0.1141  data: 0.0068  max mem: 594\n",
      "Test:  [2310/4410]  eta: 0:03:53  class_error: 100.00  loss: 6.6015 (7.4008)  loss_ce: 2.8001 (2.8368)  loss_bbox: 2.2185 (2.9736)  loss_giou: 1.6139 (1.5904)  loss_ce_unscaled: 1.4000 (1.4184)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4437 (0.5947)  loss_giou_unscaled: 0.8070 (0.7952)  cardinality_error_unscaled: 29.0000 (27.6958)  loss_ce_0_unscaled: 1.3158 (1.2771)  loss_bbox_0_unscaled: 0.3912 (0.5277)  loss_giou_0_unscaled: 0.6621 (0.7261)  cardinality_error_0_unscaled: 77.0000 (78.3817)  loss_ce_1_unscaled: 1.2920 (1.2464)  loss_bbox_1_unscaled: 0.4437 (0.5633)  loss_giou_1_unscaled: 0.7682 (0.7606)  cardinality_error_1_unscaled: 49.0000 (48.5452)  time: 0.1124  data: 0.0064  max mem: 594\n",
      "Test:  [2320/4410]  eta: 0:03:52  class_error: 100.00  loss: 6.5215 (7.3986)  loss_ce: 2.8267 (2.8369)  loss_bbox: 2.3944 (2.9721)  loss_giou: 1.3624 (1.5897)  loss_ce_unscaled: 1.4134 (1.4184)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4789 (0.5944)  loss_giou_unscaled: 0.6812 (0.7948)  cardinality_error_unscaled: 28.0000 (27.6971)  loss_ce_0_unscaled: 1.2900 (1.2771)  loss_bbox_0_unscaled: 0.4421 (0.5277)  loss_giou_0_unscaled: 0.6621 (0.7260)  cardinality_error_0_unscaled: 78.0000 (78.3800)  loss_ce_1_unscaled: 1.3049 (1.2467)  loss_bbox_1_unscaled: 0.4789 (0.5632)  loss_giou_1_unscaled: 0.6812 (0.7603)  cardinality_error_1_unscaled: 48.0000 (48.5429)  time: 0.1116  data: 0.0066  max mem: 594\n",
      "Test:  [2330/4410]  eta: 0:03:51  class_error: 100.00  loss: 6.8706 (7.3964)  loss_ce: 2.8722 (2.8370)  loss_bbox: 2.6165 (2.9705)  loss_giou: 1.4082 (1.5889)  loss_ce_unscaled: 1.4361 (1.4185)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5233 (0.5941)  loss_giou_unscaled: 0.7041 (0.7944)  cardinality_error_unscaled: 28.0000 (27.6984)  loss_ce_0_unscaled: 1.2826 (1.2771)  loss_bbox_0_unscaled: 0.5233 (0.5277)  loss_giou_0_unscaled: 0.7041 (0.7259)  cardinality_error_0_unscaled: 78.0000 (78.3784)  loss_ce_1_unscaled: 1.3312 (1.2471)  loss_bbox_1_unscaled: 0.5233 (0.5630)  loss_giou_1_unscaled: 0.6942 (0.7600)  cardinality_error_1_unscaled: 48.0000 (48.5405)  time: 0.1118  data: 0.0067  max mem: 594\n",
      "Test:  [2340/4410]  eta: 0:03:50  class_error: 100.00  loss: 6.7459 (7.3938)  loss_ce: 2.8684 (2.8372)  loss_bbox: 2.5539 (2.9685)  loss_giou: 1.3862 (1.5882)  loss_ce_unscaled: 1.4342 (1.4186)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5108 (0.5937)  loss_giou_unscaled: 0.6931 (0.7941)  cardinality_error_unscaled: 28.0000 (27.6997)  loss_ce_0_unscaled: 1.2780 (1.2771)  loss_bbox_0_unscaled: 0.5108 (0.5276)  loss_giou_0_unscaled: 0.6931 (0.7259)  cardinality_error_0_unscaled: 78.0000 (78.3759)  loss_ce_1_unscaled: 1.3272 (1.2474)  loss_bbox_1_unscaled: 0.5108 (0.5627)  loss_giou_1_unscaled: 0.6927 (0.7598)  cardinality_error_1_unscaled: 48.0000 (48.5382)  time: 0.1121  data: 0.0068  max mem: 594\n",
      "Test:  [2350/4410]  eta: 0:03:49  class_error: 100.00  loss: 7.6690 (7.3965)  loss_ce: 2.8698 (2.8374)  loss_bbox: 3.0156 (2.9697)  loss_giou: 1.7440 (1.5894)  loss_ce_unscaled: 1.4349 (1.4187)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6031 (0.5939)  loss_giou_unscaled: 0.8720 (0.7947)  cardinality_error_unscaled: 28.0000 (27.7006)  loss_ce_0_unscaled: 1.2887 (1.2772)  loss_bbox_0_unscaled: 0.5286 (0.5277)  loss_giou_0_unscaled: 0.8352 (0.7265)  cardinality_error_0_unscaled: 78.0000 (78.3743)  loss_ce_1_unscaled: 1.2326 (1.2472)  loss_bbox_1_unscaled: 0.5509 (0.5629)  loss_giou_1_unscaled: 0.8402 (0.7604)  cardinality_error_1_unscaled: 48.0000 (48.5359)  time: 0.1120  data: 0.0067  max mem: 594\n",
      "Test:  [2360/4410]  eta: 0:03:47  class_error: 100.00  loss: 7.9612 (7.3994)  loss_ce: 2.8848 (2.8376)  loss_bbox: 3.2223 (2.9709)  loss_giou: 1.8508 (1.5908)  loss_ce_unscaled: 1.4424 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6445 (0.5942)  loss_giou_unscaled: 0.9254 (0.7954)  cardinality_error_unscaled: 28.0000 (27.7014)  loss_ce_0_unscaled: 1.2957 (1.2773)  loss_bbox_0_unscaled: 0.5676 (0.5279)  loss_giou_0_unscaled: 0.8770 (0.7271)  cardinality_error_0_unscaled: 78.0000 (78.3727)  loss_ce_1_unscaled: 1.2201 (1.2470)  loss_bbox_1_unscaled: 0.6219 (0.5632)  loss_giou_1_unscaled: 0.9119 (0.7611)  cardinality_error_1_unscaled: 48.0000 (48.5337)  time: 0.1131  data: 0.0065  max mem: 594\n",
      "Test:  [2370/4410]  eta: 0:03:46  class_error: 100.00  loss: 8.2768 (7.4030)  loss_ce: 2.8848 (2.8378)  loss_bbox: 3.3749 (2.9732)  loss_giou: 1.8391 (1.5920)  loss_ce_unscaled: 1.4424 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6750 (0.5946)  loss_giou_unscaled: 0.9195 (0.7960)  cardinality_error_unscaled: 28.0000 (27.7010)  loss_ce_0_unscaled: 1.2997 (1.2773)  loss_bbox_0_unscaled: 0.5727 (0.5281)  loss_giou_0_unscaled: 0.8642 (0.7277)  cardinality_error_0_unscaled: 78.0000 (78.3712)  loss_ce_1_unscaled: 1.2174 (1.2468)  loss_bbox_1_unscaled: 0.6081 (0.5635)  loss_giou_1_unscaled: 0.8964 (0.7617)  cardinality_error_1_unscaled: 48.0000 (48.5314)  time: 0.1132  data: 0.0065  max mem: 594\n",
      "Test:  [2380/4410]  eta: 0:03:45  class_error: 100.00  loss: 7.5467 (7.4022)  loss_ce: 2.8786 (2.8379)  loss_bbox: 2.9969 (2.9729)  loss_giou: 1.7300 (1.5914)  loss_ce_unscaled: 1.4393 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5994 (0.5946)  loss_giou_unscaled: 0.8650 (0.7957)  cardinality_error_unscaled: 28.0000 (27.7014)  loss_ce_0_unscaled: 1.2873 (1.2774)  loss_bbox_0_unscaled: 0.5829 (0.5284)  loss_giou_0_unscaled: 0.8092 (0.7277)  cardinality_error_0_unscaled: 78.0000 (78.3696)  loss_ce_1_unscaled: 1.2402 (1.2468)  loss_bbox_1_unscaled: 0.5908 (0.5636)  loss_giou_1_unscaled: 0.8445 (0.7616)  cardinality_error_1_unscaled: 48.0000 (48.5292)  time: 0.1124  data: 0.0070  max mem: 594\n",
      "Test:  [2390/4410]  eta: 0:03:44  class_error: 100.00  loss: 7.4714 (7.4027)  loss_ce: 2.8385 (2.8379)  loss_bbox: 2.9806 (2.9733)  loss_giou: 1.5871 (1.5915)  loss_ce_unscaled: 1.4193 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5961 (0.5947)  loss_giou_unscaled: 0.7936 (0.7957)  cardinality_error_unscaled: 28.0000 (27.7005)  loss_ce_0_unscaled: 1.2878 (1.2774)  loss_bbox_0_unscaled: 0.5961 (0.5287)  loss_giou_0_unscaled: 0.7936 (0.7280)  cardinality_error_0_unscaled: 78.0000 (78.3680)  loss_ce_1_unscaled: 1.2448 (1.2468)  loss_bbox_1_unscaled: 0.5961 (0.5638)  loss_giou_1_unscaled: 0.7936 (0.7617)  cardinality_error_1_unscaled: 48.0000 (48.5270)  time: 0.1123  data: 0.0072  max mem: 594\n",
      "Test:  [2400/4410]  eta: 0:03:43  class_error: 100.00  loss: 7.5304 (7.4028)  loss_ce: 2.8314 (2.8379)  loss_bbox: 2.9806 (2.9734)  loss_giou: 1.6081 (1.5914)  loss_ce_unscaled: 1.4157 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5961 (0.5947)  loss_giou_unscaled: 0.8041 (0.7957)  cardinality_error_unscaled: 28.0000 (27.7018)  loss_ce_0_unscaled: 1.2895 (1.2775)  loss_bbox_0_unscaled: 0.5961 (0.5290)  loss_giou_0_unscaled: 0.8041 (0.7282)  cardinality_error_0_unscaled: 78.0000 (78.3665)  loss_ce_1_unscaled: 1.2462 (1.2468)  loss_bbox_1_unscaled: 0.5961 (0.5639)  loss_giou_1_unscaled: 0.8041 (0.7618)  cardinality_error_1_unscaled: 48.0000 (48.5248)  time: 0.1130  data: 0.0069  max mem: 594\n",
      "Test:  [2410/4410]  eta: 0:03:42  class_error: 100.00  loss: 7.8254 (7.4052)  loss_ce: 2.8419 (2.8379)  loss_bbox: 3.2179 (2.9754)  loss_giou: 1.6284 (1.5919)  loss_ce_unscaled: 1.4210 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6436 (0.5951)  loss_giou_unscaled: 0.8142 (0.7959)  cardinality_error_unscaled: 28.0000 (27.7026)  loss_ce_0_unscaled: 1.2898 (1.2775)  loss_bbox_0_unscaled: 0.6436 (0.5297)  loss_giou_0_unscaled: 0.8086 (0.7287)  cardinality_error_0_unscaled: 78.0000 (78.3650)  loss_ce_1_unscaled: 1.2408 (1.2468)  loss_bbox_1_unscaled: 0.6436 (0.5644)  loss_giou_1_unscaled: 0.8086 (0.7622)  cardinality_error_1_unscaled: 48.0000 (48.5226)  time: 0.1132  data: 0.0069  max mem: 594\n",
      "Test:  [2420/4410]  eta: 0:03:41  class_error: 100.00  loss: 7.9216 (7.4075)  loss_ce: 2.8239 (2.8378)  loss_bbox: 3.4796 (2.9772)  loss_giou: 1.6556 (1.5925)  loss_ce_unscaled: 1.4120 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6959 (0.5954)  loss_giou_unscaled: 0.8278 (0.7962)  cardinality_error_unscaled: 28.0000 (27.7026)  loss_ce_0_unscaled: 1.2898 (1.2776)  loss_bbox_0_unscaled: 0.6959 (0.5303)  loss_giou_0_unscaled: 0.8278 (0.7293)  cardinality_error_0_unscaled: 78.0000 (78.3635)  loss_ce_1_unscaled: 1.2408 (1.2468)  loss_bbox_1_unscaled: 0.6959 (0.5649)  loss_giou_1_unscaled: 0.8229 (0.7626)  cardinality_error_1_unscaled: 48.0000 (48.5204)  time: 0.1109  data: 0.0069  max mem: 594\n",
      "Test:  [2430/4410]  eta: 0:03:40  class_error: 100.00  loss: 7.9745 (7.4101)  loss_ce: 2.8215 (2.8378)  loss_bbox: 3.4921 (2.9793)  loss_giou: 1.6628 (1.5930)  loss_ce_unscaled: 1.4108 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6984 (0.5959)  loss_giou_unscaled: 0.8314 (0.7965)  cardinality_error_unscaled: 28.0000 (27.7026)  loss_ce_0_unscaled: 1.2865 (1.2776)  loss_bbox_0_unscaled: 0.7003 (0.5310)  loss_giou_0_unscaled: 0.8296 (0.7298)  cardinality_error_0_unscaled: 78.0000 (78.3620)  loss_ce_1_unscaled: 1.2447 (1.2467)  loss_bbox_1_unscaled: 0.6984 (0.5654)  loss_giou_1_unscaled: 0.8285 (0.7630)  cardinality_error_1_unscaled: 48.0000 (48.5183)  time: 0.1100  data: 0.0067  max mem: 594\n",
      "Test:  [2440/4410]  eta: 0:03:39  class_error: 100.00  loss: 7.8980 (7.4116)  loss_ce: 2.8221 (2.8377)  loss_bbox: 3.4286 (2.9804)  loss_giou: 1.6796 (1.5934)  loss_ce_unscaled: 1.4111 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6857 (0.5961)  loss_giou_unscaled: 0.8398 (0.7967)  cardinality_error_unscaled: 27.0000 (27.6911)  loss_ce_0_unscaled: 1.2865 (1.2777)  loss_bbox_0_unscaled: 0.6857 (0.5315)  loss_giou_0_unscaled: 0.8337 (0.7303)  cardinality_error_0_unscaled: 78.0000 (78.3605)  loss_ce_1_unscaled: 1.2429 (1.2467)  loss_bbox_1_unscaled: 0.6857 (0.5658)  loss_giou_1_unscaled: 0.8337 (0.7633)  cardinality_error_1_unscaled: 48.0000 (48.5162)  time: 0.1103  data: 0.0065  max mem: 594\n",
      "Test:  [2450/4410]  eta: 0:03:38  class_error: 100.00  loss: 7.8929 (7.4141)  loss_ce: 2.8291 (2.8377)  loss_bbox: 3.3843 (2.9825)  loss_giou: 1.6805 (1.5939)  loss_ce_unscaled: 1.4145 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6769 (0.5965)  loss_giou_unscaled: 0.8402 (0.7969)  cardinality_error_unscaled: 26.0000 (27.6842)  loss_ce_0_unscaled: 1.2881 (1.2777)  loss_bbox_0_unscaled: 0.6769 (0.5321)  loss_giou_0_unscaled: 0.8402 (0.7307)  cardinality_error_0_unscaled: 78.0000 (78.3590)  loss_ce_1_unscaled: 1.2421 (1.2467)  loss_bbox_1_unscaled: 0.6769 (0.5663)  loss_giou_1_unscaled: 0.8402 (0.7637)  cardinality_error_1_unscaled: 48.0000 (48.5141)  time: 0.1158  data: 0.0066  max mem: 594\n",
      "Test:  [2460/4410]  eta: 0:03:36  class_error: 100.00  loss: 7.9642 (7.4160)  loss_ce: 2.8244 (2.8377)  loss_bbox: 3.4355 (2.9843)  loss_giou: 1.6571 (1.5941)  loss_ce_unscaled: 1.4122 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6871 (0.5969)  loss_giou_unscaled: 0.8285 (0.7970)  cardinality_error_unscaled: 25.0000 (27.6725)  loss_ce_0_unscaled: 1.2870 (1.2778)  loss_bbox_0_unscaled: 0.6871 (0.5328)  loss_giou_0_unscaled: 0.8233 (0.7311)  cardinality_error_0_unscaled: 78.0000 (78.3580)  loss_ce_1_unscaled: 1.2456 (1.2467)  loss_bbox_1_unscaled: 0.6871 (0.5668)  loss_giou_1_unscaled: 0.8233 (0.7639)  cardinality_error_1_unscaled: 48.0000 (48.5124)  time: 0.1144  data: 0.0066  max mem: 594\n",
      "Test:  [2470/4410]  eta: 0:03:35  class_error: 100.00  loss: 8.0119 (7.4187)  loss_ce: 2.8184 (2.8376)  loss_bbox: 3.4054 (2.9862)  loss_giou: 1.6571 (1.5950)  loss_ce_unscaled: 1.4092 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6811 (0.5972)  loss_giou_unscaled: 0.8285 (0.7975)  cardinality_error_unscaled: 25.0000 (27.6665)  loss_ce_0_unscaled: 1.2844 (1.2777)  loss_bbox_0_unscaled: 0.7028 (0.5335)  loss_giou_0_unscaled: 0.8201 (0.7315)  cardinality_error_0_unscaled: 79.0000 (78.3606)  loss_ce_1_unscaled: 1.2335 (1.2466)  loss_bbox_1_unscaled: 0.6871 (0.5673)  loss_giou_1_unscaled: 0.8233 (0.7643)  cardinality_error_1_unscaled: 49.0000 (48.5144)  time: 0.1086  data: 0.0068  max mem: 594\n",
      "Test:  [2480/4410]  eta: 0:03:34  class_error: 100.00  loss: 8.1290 (7.4215)  loss_ce: 2.8269 (2.8376)  loss_bbox: 3.4223 (2.9880)  loss_giou: 1.8551 (1.5958)  loss_ce_unscaled: 1.4135 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6845 (0.5976)  loss_giou_unscaled: 0.9276 (0.7979)  cardinality_error_unscaled: 28.0000 (27.6663)  loss_ce_0_unscaled: 1.2795 (1.2778)  loss_bbox_0_unscaled: 0.7202 (0.5342)  loss_giou_0_unscaled: 0.8366 (0.7319)  cardinality_error_0_unscaled: 79.0000 (78.3632)  loss_ce_1_unscaled: 1.2304 (1.2466)  loss_bbox_1_unscaled: 0.6965 (0.5679)  loss_giou_1_unscaled: 0.8525 (0.7647)  cardinality_error_1_unscaled: 49.0000 (48.5163)  time: 0.1096  data: 0.0069  max mem: 594\n",
      "Test:  [2490/4410]  eta: 0:03:33  class_error: 100.00  loss: 8.0364 (7.4241)  loss_ce: 2.8477 (2.8377)  loss_bbox: 3.4640 (2.9900)  loss_giou: 1.8506 (1.5965)  loss_ce_unscaled: 1.4239 (1.4188)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6928 (0.5980)  loss_giou_unscaled: 0.9253 (0.7982)  cardinality_error_unscaled: 28.0000 (27.6676)  loss_ce_0_unscaled: 1.2883 (1.2778)  loss_bbox_0_unscaled: 0.7188 (0.5349)  loss_giou_0_unscaled: 0.8540 (0.7324)  cardinality_error_0_unscaled: 79.0000 (78.3657)  loss_ce_1_unscaled: 1.2340 (1.2465)  loss_bbox_1_unscaled: 0.6965 (0.5684)  loss_giou_1_unscaled: 0.8566 (0.7651)  cardinality_error_1_unscaled: 49.0000 (48.5183)  time: 0.1082  data: 0.0066  max mem: 594\n",
      "Test:  [2500/4410]  eta: 0:03:32  class_error: 100.00  loss: 7.9071 (7.4255)  loss_ce: 2.8545 (2.8378)  loss_bbox: 3.2040 (2.9905)  loss_giou: 1.8479 (1.5973)  loss_ce_unscaled: 1.4273 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6408 (0.5981)  loss_giou_unscaled: 0.9239 (0.7986)  cardinality_error_unscaled: 29.0000 (27.6697)  loss_ce_0_unscaled: 1.2814 (1.2778)  loss_bbox_0_unscaled: 0.6464 (0.5352)  loss_giou_0_unscaled: 0.8719 (0.7331)  cardinality_error_0_unscaled: 79.0000 (78.3683)  loss_ce_1_unscaled: 1.2319 (1.2464)  loss_bbox_1_unscaled: 0.6408 (0.5686)  loss_giou_1_unscaled: 0.9022 (0.7656)  cardinality_error_1_unscaled: 49.0000 (48.5202)  time: 0.1096  data: 0.0066  max mem: 594\n",
      "Test:  [2510/4410]  eta: 0:03:31  class_error: 100.00  loss: 7.6702 (7.4264)  loss_ce: 2.8496 (2.8378)  loss_bbox: 2.9417 (2.9905)  loss_giou: 1.8241 (1.5981)  loss_ce_unscaled: 1.4248 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5883 (0.5981)  loss_giou_unscaled: 0.9121 (0.7990)  cardinality_error_unscaled: 28.0000 (27.6718)  loss_ce_0_unscaled: 1.2669 (1.2777)  loss_bbox_0_unscaled: 0.5883 (0.5354)  loss_giou_0_unscaled: 0.9121 (0.7338)  cardinality_error_0_unscaled: 79.0000 (78.3708)  loss_ce_1_unscaled: 1.2308 (1.2464)  loss_bbox_1_unscaled: 0.5883 (0.5687)  loss_giou_1_unscaled: 0.9121 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5221)  time: 0.1108  data: 0.0067  max mem: 594\n",
      "Test:  [2520/4410]  eta: 0:03:30  class_error: 100.00  loss: 7.6146 (7.4263)  loss_ce: 2.8491 (2.8379)  loss_bbox: 2.9198 (2.9897)  loss_giou: 1.8241 (1.5988)  loss_ce_unscaled: 1.4245 (1.4189)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5840 (0.5979)  loss_giou_unscaled: 0.9121 (0.7994)  cardinality_error_unscaled: 28.0000 (27.6724)  loss_ce_0_unscaled: 1.2661 (1.2777)  loss_bbox_0_unscaled: 0.5840 (0.5355)  loss_giou_0_unscaled: 0.9121 (0.7344)  cardinality_error_0_unscaled: 79.0000 (78.3733)  loss_ce_1_unscaled: 1.2285 (1.2463)  loss_bbox_1_unscaled: 0.5840 (0.5686)  loss_giou_1_unscaled: 0.9121 (0.7667)  cardinality_error_1_unscaled: 49.0000 (48.5240)  time: 0.1104  data: 0.0065  max mem: 594\n",
      "Test:  [2530/4410]  eta: 0:03:29  class_error: 100.00  loss: 7.3694 (7.4242)  loss_ce: 2.8496 (2.8380)  loss_bbox: 2.7743 (2.9885)  loss_giou: 1.4690 (1.5977)  loss_ce_unscaled: 1.4248 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5549 (0.5977)  loss_giou_unscaled: 0.7345 (0.7988)  cardinality_error_unscaled: 29.0000 (27.6776)  loss_ce_0_unscaled: 1.2458 (1.2775)  loss_bbox_0_unscaled: 0.5549 (0.5355)  loss_giou_0_unscaled: 0.7345 (0.7341)  cardinality_error_0_unscaled: 79.0000 (78.3757)  loss_ce_1_unscaled: 1.2068 (1.2461)  loss_bbox_1_unscaled: 0.5549 (0.5685)  loss_giou_1_unscaled: 0.7345 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5259)  time: 0.1118  data: 0.0067  max mem: 594\n",
      "Test:  [2540/4410]  eta: 0:03:27  class_error: 100.00  loss: 6.9943 (7.4220)  loss_ce: 2.8572 (2.8381)  loss_bbox: 2.6791 (2.9870)  loss_giou: 1.3411 (1.5969)  loss_ce_unscaled: 1.4286 (1.4190)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5358 (0.5974)  loss_giou_unscaled: 0.6706 (0.7984)  cardinality_error_unscaled: 29.0000 (27.6828)  loss_ce_0_unscaled: 1.2439 (1.2775)  loss_bbox_0_unscaled: 0.5358 (0.5354)  loss_giou_0_unscaled: 0.6706 (0.7339)  cardinality_error_0_unscaled: 79.0000 (78.3782)  loss_ce_1_unscaled: 1.1934 (1.2459)  loss_bbox_1_unscaled: 0.5358 (0.5683)  loss_giou_1_unscaled: 0.6706 (0.7659)  cardinality_error_1_unscaled: 49.0000 (48.5277)  time: 0.1110  data: 0.0069  max mem: 594\n",
      "Test:  [2550/4410]  eta: 0:03:26  class_error: 100.00  loss: 7.0616 (7.4200)  loss_ce: 2.8499 (2.8382)  loss_bbox: 2.7093 (2.9862)  loss_giou: 1.3027 (1.5956)  loss_ce_unscaled: 1.4250 (1.4191)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5419 (0.5972)  loss_giou_unscaled: 0.6513 (0.7978)  cardinality_error_unscaled: 29.0000 (27.6880)  loss_ce_0_unscaled: 1.2551 (1.2774)  loss_bbox_0_unscaled: 0.5419 (0.5355)  loss_giou_0_unscaled: 0.6508 (0.7335)  cardinality_error_0_unscaled: 79.0000 (78.3806)  loss_ce_1_unscaled: 1.1926 (1.2457)  loss_bbox_1_unscaled: 0.5419 (0.5682)  loss_giou_1_unscaled: 0.6508 (0.7654)  cardinality_error_1_unscaled: 49.0000 (48.5296)  time: 0.1099  data: 0.0067  max mem: 594\n",
      "Test:  [2560/4410]  eta: 0:03:25  class_error: 100.00  loss: 6.8785 (7.4182)  loss_ce: 2.8712 (2.8384)  loss_bbox: 2.6247 (2.9854)  loss_giou: 1.2272 (1.5944)  loss_ce_unscaled: 1.4356 (1.4192)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5249 (0.5971)  loss_giou_unscaled: 0.6136 (0.7972)  cardinality_error_unscaled: 29.0000 (27.6927)  loss_ce_0_unscaled: 1.2523 (1.2773)  loss_bbox_0_unscaled: 0.5042 (0.5355)  loss_giou_0_unscaled: 0.6136 (0.7331)  cardinality_error_0_unscaled: 79.0000 (78.3831)  loss_ce_1_unscaled: 1.2040 (1.2456)  loss_bbox_1_unscaled: 0.5042 (0.5681)  loss_giou_1_unscaled: 0.6076 (0.7649)  cardinality_error_1_unscaled: 49.0000 (48.5314)  time: 0.1118  data: 0.0065  max mem: 594\n",
      "Test:  [2570/4410]  eta: 0:03:24  class_error: 100.00  loss: 6.7624 (7.4162)  loss_ce: 2.8759 (2.8386)  loss_bbox: 2.5714 (2.9841)  loss_giou: 1.2526 (1.5935)  loss_ce_unscaled: 1.4380 (1.4193)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5143 (0.5968)  loss_giou_unscaled: 0.6263 (0.7968)  cardinality_error_unscaled: 29.0000 (27.6978)  loss_ce_0_unscaled: 1.2514 (1.2772)  loss_bbox_0_unscaled: 0.4914 (0.5354)  loss_giou_0_unscaled: 0.6263 (0.7328)  cardinality_error_0_unscaled: 79.0000 (78.3855)  loss_ce_1_unscaled: 1.2054 (1.2454)  loss_bbox_1_unscaled: 0.4914 (0.5680)  loss_giou_1_unscaled: 0.5975 (0.7643)  cardinality_error_1_unscaled: 49.0000 (48.5333)  time: 0.1114  data: 0.0067  max mem: 594\n",
      "Test:  [2580/4410]  eta: 0:03:23  class_error: 100.00  loss: 7.5929 (7.4172)  loss_ce: 2.8802 (2.8388)  loss_bbox: 3.1677 (2.9853)  loss_giou: 1.3575 (1.5931)  loss_ce_unscaled: 1.4401 (1.4194)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6335 (0.5971)  loss_giou_unscaled: 0.6788 (0.7965)  cardinality_error_unscaled: 29.0000 (27.7017)  loss_ce_0_unscaled: 1.2593 (1.2772)  loss_bbox_0_unscaled: 0.6045 (0.5359)  loss_giou_0_unscaled: 0.6264 (0.7327)  cardinality_error_0_unscaled: 79.0000 (78.3878)  loss_ce_1_unscaled: 1.2279 (1.2454)  loss_bbox_1_unscaled: 0.6045 (0.5683)  loss_giou_1_unscaled: 0.5975 (0.7640)  cardinality_error_1_unscaled: 49.0000 (48.5351)  time: 0.1102  data: 0.0068  max mem: 594\n",
      "Test:  [2590/4410]  eta: 0:03:22  class_error: 100.00  loss: 8.0773 (7.4203)  loss_ce: 2.8910 (2.8391)  loss_bbox: 3.4645 (2.9876)  loss_giou: 1.6682 (1.5936)  loss_ce_unscaled: 1.4455 (1.4196)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6929 (0.5975)  loss_giou_unscaled: 0.8341 (0.7968)  cardinality_error_unscaled: 29.0000 (27.7067)  loss_ce_0_unscaled: 1.2593 (1.2771)  loss_bbox_0_unscaled: 0.6946 (0.5366)  loss_giou_0_unscaled: 0.8004 (0.7331)  cardinality_error_0_unscaled: 79.0000 (78.3902)  loss_ce_1_unscaled: 1.2362 (1.2454)  loss_bbox_1_unscaled: 0.7021 (0.5689)  loss_giou_1_unscaled: 0.8023 (0.7644)  cardinality_error_1_unscaled: 49.0000 (48.5369)  time: 0.1091  data: 0.0069  max mem: 594\n",
      "Test:  [2600/4410]  eta: 0:03:21  class_error: 100.00  loss: 8.1207 (7.4224)  loss_ce: 2.9259 (2.8395)  loss_bbox: 3.5103 (2.9891)  loss_giou: 1.6396 (1.5938)  loss_ce_unscaled: 1.4630 (1.4198)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7021 (0.5978)  loss_giou_unscaled: 0.8198 (0.7969)  cardinality_error_unscaled: 29.0000 (27.7116)  loss_ce_0_unscaled: 1.2560 (1.2770)  loss_bbox_0_unscaled: 0.7021 (0.5371)  loss_giou_0_unscaled: 0.8037 (0.7334)  cardinality_error_0_unscaled: 79.0000 (78.3925)  loss_ce_1_unscaled: 1.2106 (1.2453)  loss_bbox_1_unscaled: 0.7021 (0.5693)  loss_giou_1_unscaled: 0.8198 (0.7646)  cardinality_error_1_unscaled: 49.0000 (48.5386)  time: 0.1089  data: 0.0067  max mem: 594\n",
      "Test:  [2610/4410]  eta: 0:03:20  class_error: 100.00  loss: 8.0820 (7.4248)  loss_ce: 2.9034 (2.8397)  loss_bbox: 3.4425 (2.9907)  loss_giou: 1.6396 (1.5944)  loss_ce_unscaled: 1.4517 (1.4199)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6885 (0.5981)  loss_giou_unscaled: 0.8198 (0.7972)  cardinality_error_unscaled: 29.0000 (27.7162)  loss_ce_0_unscaled: 1.2550 (1.2770)  loss_bbox_0_unscaled: 0.6885 (0.5377)  loss_giou_0_unscaled: 0.8003 (0.7338)  cardinality_error_0_unscaled: 79.0000 (78.3945)  loss_ce_1_unscaled: 1.2127 (1.2452)  loss_bbox_1_unscaled: 0.6885 (0.5698)  loss_giou_1_unscaled: 0.8198 (0.7650)  cardinality_error_1_unscaled: 49.0000 (48.5400)  time: 0.1108  data: 0.0065  max mem: 594\n",
      "Test:  [2620/4410]  eta: 0:03:19  class_error: 100.00  loss: 7.9061 (7.4265)  loss_ce: 2.9162 (2.8401)  loss_bbox: 2.9937 (2.9904)  loss_giou: 1.8312 (1.5959)  loss_ce_unscaled: 1.4581 (1.4201)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5987 (0.5981)  loss_giou_unscaled: 0.9156 (0.7980)  cardinality_error_unscaled: 28.0000 (27.7169)  loss_ce_0_unscaled: 1.3050 (1.2772)  loss_bbox_0_unscaled: 0.5229 (0.5375)  loss_giou_0_unscaled: 0.8022 (0.7340)  cardinality_error_0_unscaled: 78.0000 (78.3930)  loss_ce_1_unscaled: 1.2184 (1.2451)  loss_bbox_1_unscaled: 0.5941 (0.5698)  loss_giou_1_unscaled: 0.8954 (0.7658)  cardinality_error_1_unscaled: 48.0000 (48.5380)  time: 0.1115  data: 0.0069  max mem: 594\n",
      "Test:  [2630/4410]  eta: 0:03:17  class_error: 100.00  loss: 7.7473 (7.4276)  loss_ce: 2.9514 (2.8406)  loss_bbox: 2.8973 (2.9898)  loss_giou: 2.0250 (1.5973)  loss_ce_unscaled: 1.4757 (1.4203)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5795 (0.5980)  loss_giou_unscaled: 1.0125 (0.7986)  cardinality_error_unscaled: 28.0000 (27.7180)  loss_ce_0_unscaled: 1.3284 (1.2774)  loss_bbox_0_unscaled: 0.4926 (0.5374)  loss_giou_0_unscaled: 0.7823 (0.7342)  cardinality_error_0_unscaled: 78.0000 (78.3915)  loss_ce_1_unscaled: 1.2274 (1.2450)  loss_bbox_1_unscaled: 0.5588 (0.5697)  loss_giou_1_unscaled: 0.9394 (0.7665)  cardinality_error_1_unscaled: 48.0000 (48.5359)  time: 0.1105  data: 0.0070  max mem: 594\n",
      "Test:  [2640/4410]  eta: 0:03:16  class_error: 100.00  loss: 7.6993 (7.4288)  loss_ce: 2.9536 (2.8410)  loss_bbox: 2.8498 (2.9892)  loss_giou: 2.0250 (1.5986)  loss_ce_unscaled: 1.4768 (1.4205)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5700 (0.5978)  loss_giou_unscaled: 1.0125 (0.7993)  cardinality_error_unscaled: 28.0000 (27.7194)  loss_ce_0_unscaled: 1.3278 (1.2776)  loss_bbox_0_unscaled: 0.4928 (0.5373)  loss_giou_0_unscaled: 0.7809 (0.7344)  cardinality_error_0_unscaled: 78.0000 (78.3904)  loss_ce_1_unscaled: 1.2291 (1.2450)  loss_bbox_1_unscaled: 0.5727 (0.5698)  loss_giou_1_unscaled: 0.8988 (0.7670)  cardinality_error_1_unscaled: 48.0000 (48.5343)  time: 0.1122  data: 0.0068  max mem: 594\n",
      "Test:  [2650/4410]  eta: 0:03:15  class_error: 100.00  loss: 7.3687 (7.4270)  loss_ce: 2.9629 (2.8414)  loss_bbox: 2.6142 (2.9878)  loss_giou: 1.6054 (1.5978)  loss_ce_unscaled: 1.4814 (1.4207)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5228 (0.5976)  loss_giou_unscaled: 0.8027 (0.7989)  cardinality_error_unscaled: 29.0000 (27.7243)  loss_ce_0_unscaled: 1.2756 (1.2775)  loss_bbox_0_unscaled: 0.4925 (0.5372)  loss_giou_0_unscaled: 0.7595 (0.7342)  cardinality_error_0_unscaled: 79.0000 (78.3927)  loss_ce_1_unscaled: 1.2401 (1.2449)  loss_bbox_1_unscaled: 0.5546 (0.5696)  loss_giou_1_unscaled: 0.7595 (0.7667)  cardinality_error_1_unscaled: 49.0000 (48.5360)  time: 0.1118  data: 0.0067  max mem: 594\n",
      "Test:  [2660/4410]  eta: 0:03:14  class_error: 100.00  loss: 7.0039 (7.4264)  loss_ce: 2.9629 (2.8417)  loss_bbox: 2.5831 (2.9876)  loss_giou: 1.3815 (1.5970)  loss_ce_unscaled: 1.4814 (1.4209)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5166 (0.5975)  loss_giou_unscaled: 0.6907 (0.7985)  cardinality_error_unscaled: 29.0000 (27.7290)  loss_ce_0_unscaled: 1.2441 (1.2773)  loss_bbox_0_unscaled: 0.5035 (0.5372)  loss_giou_0_unscaled: 0.6872 (0.7341)  cardinality_error_0_unscaled: 79.0000 (78.3950)  loss_ce_1_unscaled: 1.2466 (1.2447)  loss_bbox_1_unscaled: 0.5166 (0.5695)  loss_giou_1_unscaled: 0.6872 (0.7664)  cardinality_error_1_unscaled: 49.0000 (48.5378)  time: 0.1098  data: 0.0067  max mem: 594\n",
      "Test:  [2670/4410]  eta: 0:03:13  class_error: 100.00  loss: 7.2374 (7.4263)  loss_ce: 2.9357 (2.8421)  loss_bbox: 2.8266 (2.9874)  loss_giou: 1.4888 (1.5968)  loss_ce_unscaled: 1.4679 (1.4210)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5653 (0.5975)  loss_giou_unscaled: 0.7444 (0.7984)  cardinality_error_unscaled: 29.0000 (27.7338)  loss_ce_0_unscaled: 1.2324 (1.2771)  loss_bbox_0_unscaled: 0.5295 (0.5373)  loss_giou_0_unscaled: 0.7582 (0.7343)  cardinality_error_0_unscaled: 79.0000 (78.3972)  loss_ce_1_unscaled: 1.2162 (1.2446)  loss_bbox_1_unscaled: 0.5370 (0.5695)  loss_giou_1_unscaled: 0.7211 (0.7663)  cardinality_error_1_unscaled: 49.0000 (48.5395)  time: 0.1105  data: 0.0066  max mem: 594\n",
      "Test:  [2680/4410]  eta: 0:03:12  class_error: 100.00  loss: 7.6850 (7.4278)  loss_ce: 2.9380 (2.8425)  loss_bbox: 3.0972 (2.9886)  loss_giou: 1.5828 (1.5967)  loss_ce_unscaled: 1.4690 (1.4212)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6194 (0.5977)  loss_giou_unscaled: 0.7914 (0.7983)  cardinality_error_unscaled: 29.0000 (27.7385)  loss_ce_0_unscaled: 1.2345 (1.2771)  loss_bbox_0_unscaled: 0.5924 (0.5377)  loss_giou_0_unscaled: 0.7984 (0.7345)  cardinality_error_0_unscaled: 79.0000 (78.3991)  loss_ce_1_unscaled: 1.2365 (1.2446)  loss_bbox_1_unscaled: 0.6194 (0.5699)  loss_giou_1_unscaled: 0.7801 (0.7664)  cardinality_error_1_unscaled: 49.0000 (48.5412)  time: 0.1102  data: 0.0066  max mem: 594\n",
      "Test:  [2690/4410]  eta: 0:03:11  class_error: 100.00  loss: 7.9256 (7.4293)  loss_ce: 2.9448 (2.8429)  loss_bbox: 3.4881 (2.9901)  loss_giou: 1.5082 (1.5963)  loss_ce_unscaled: 1.4724 (1.4214)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6976 (0.5980)  loss_giou_unscaled: 0.7541 (0.7982)  cardinality_error_unscaled: 29.0000 (27.7432)  loss_ce_0_unscaled: 1.2563 (1.2770)  loss_bbox_0_unscaled: 0.6823 (0.5382)  loss_giou_0_unscaled: 0.7541 (0.7346)  cardinality_error_0_unscaled: 79.0000 (78.4013)  loss_ce_1_unscaled: 1.2365 (1.2446)  loss_bbox_1_unscaled: 0.6976 (0.5703)  loss_giou_1_unscaled: 0.7541 (0.7663)  cardinality_error_1_unscaled: 49.0000 (48.5429)  time: 0.1094  data: 0.0067  max mem: 594\n",
      "Test:  [2700/4410]  eta: 0:03:10  class_error: 100.00  loss: 7.9597 (7.4312)  loss_ce: 2.9598 (2.8433)  loss_bbox: 3.3791 (2.9915)  loss_giou: 1.5385 (1.5964)  loss_ce_unscaled: 1.4799 (1.4217)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6758 (0.5983)  loss_giou_unscaled: 0.7693 (0.7982)  cardinality_error_unscaled: 29.0000 (27.7479)  loss_ce_0_unscaled: 1.2563 (1.2769)  loss_bbox_0_unscaled: 0.6758 (0.5387)  loss_giou_0_unscaled: 0.7693 (0.7348)  cardinality_error_0_unscaled: 79.0000 (78.4036)  loss_ce_1_unscaled: 1.2389 (1.2446)  loss_bbox_1_unscaled: 0.6758 (0.5707)  loss_giou_1_unscaled: 0.7693 (0.7665)  cardinality_error_1_unscaled: 49.0000 (48.5446)  time: 0.1085  data: 0.0067  max mem: 594\n",
      "Test:  [2710/4410]  eta: 0:03:08  class_error: 100.00  loss: 7.7450 (7.4316)  loss_ce: 2.9475 (2.8436)  loss_bbox: 3.2567 (2.9921)  loss_giou: 1.5385 (1.5958)  loss_ce_unscaled: 1.4737 (1.4218)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6513 (0.5984)  loss_giou_unscaled: 0.7693 (0.7979)  cardinality_error_unscaled: 29.0000 (27.7525)  loss_ce_0_unscaled: 1.2446 (1.2768)  loss_bbox_0_unscaled: 0.6513 (0.5390)  loss_giou_0_unscaled: 0.7693 (0.7347)  cardinality_error_0_unscaled: 79.0000 (78.4058)  loss_ce_1_unscaled: 1.2441 (1.2446)  loss_bbox_1_unscaled: 0.6513 (0.5709)  loss_giou_1_unscaled: 0.7688 (0.7663)  cardinality_error_1_unscaled: 49.0000 (48.5463)  time: 0.1087  data: 0.0068  max mem: 594\n",
      "Test:  [2720/4410]  eta: 0:03:07  class_error: 100.00  loss: 7.6183 (7.4323)  loss_ce: 2.9226 (2.8438)  loss_bbox: 3.1982 (2.9932)  loss_giou: 1.4389 (1.5953)  loss_ce_unscaled: 1.4613 (1.4219)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6396 (0.5986)  loss_giou_unscaled: 0.7195 (0.7977)  cardinality_error_unscaled: 29.0000 (27.7571)  loss_ce_0_unscaled: 1.2406 (1.2767)  loss_bbox_0_unscaled: 0.6287 (0.5394)  loss_giou_0_unscaled: 0.7222 (0.7347)  cardinality_error_0_unscaled: 79.0000 (78.4079)  loss_ce_1_unscaled: 1.2255 (1.2445)  loss_bbox_1_unscaled: 0.6287 (0.5711)  loss_giou_1_unscaled: 0.7195 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5480)  time: 0.1105  data: 0.0067  max mem: 594\n",
      "Test:  [2730/4410]  eta: 0:03:06  class_error: 100.00  loss: 7.8733 (7.4345)  loss_ce: 2.9446 (2.8442)  loss_bbox: 3.4247 (2.9951)  loss_giou: 1.5154 (1.5952)  loss_ce_unscaled: 1.4723 (1.4221)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6849 (0.5990)  loss_giou_unscaled: 0.7577 (0.7976)  cardinality_error_unscaled: 29.0000 (27.7616)  loss_ce_0_unscaled: 1.2739 (1.2768)  loss_bbox_0_unscaled: 0.6890 (0.5400)  loss_giou_0_unscaled: 0.7195 (0.7346)  cardinality_error_0_unscaled: 79.0000 (78.4101)  loss_ce_1_unscaled: 1.2529 (1.2445)  loss_bbox_1_unscaled: 0.6558 (0.5716)  loss_giou_1_unscaled: 0.7577 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5496)  time: 0.1111  data: 0.0065  max mem: 594\n",
      "Test:  [2740/4410]  eta: 0:03:05  class_error: 100.00  loss: 8.0519 (7.4366)  loss_ce: 2.9472 (2.8445)  loss_bbox: 3.4247 (2.9965)  loss_giou: 1.5938 (1.5956)  loss_ce_unscaled: 1.4736 (1.4222)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6849 (0.5993)  loss_giou_unscaled: 0.7969 (0.7978)  cardinality_error_unscaled: 29.0000 (27.7661)  loss_ce_0_unscaled: 1.2756 (1.2767)  loss_bbox_0_unscaled: 0.7149 (0.5405)  loss_giou_0_unscaled: 0.5978 (0.7344)  cardinality_error_0_unscaled: 79.0000 (78.4123)  loss_ce_1_unscaled: 1.2447 (1.2444)  loss_bbox_1_unscaled: 0.6849 (0.5720)  loss_giou_1_unscaled: 0.8249 (0.7666)  cardinality_error_1_unscaled: 49.0000 (48.5513)  time: 0.1098  data: 0.0065  max mem: 594\n",
      "Test:  [2750/4410]  eta: 0:03:04  class_error: 100.00  loss: 8.0519 (7.4374)  loss_ce: 2.9280 (2.8448)  loss_bbox: 3.3193 (2.9972)  loss_giou: 1.6292 (1.5954)  loss_ce_unscaled: 1.4640 (1.4224)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6639 (0.5994)  loss_giou_unscaled: 0.8146 (0.7977)  cardinality_error_unscaled: 29.0000 (27.7706)  loss_ce_0_unscaled: 1.2693 (1.2766)  loss_bbox_0_unscaled: 0.7123 (0.5409)  loss_giou_0_unscaled: 0.5961 (0.7342)  cardinality_error_0_unscaled: 79.0000 (78.4144)  loss_ce_1_unscaled: 1.2327 (1.2444)  loss_bbox_1_unscaled: 0.6639 (0.5722)  loss_giou_1_unscaled: 0.8146 (0.7666)  cardinality_error_1_unscaled: 49.0000 (48.5529)  time: 0.1095  data: 0.0067  max mem: 594\n",
      "Test:  [2760/4410]  eta: 0:03:03  class_error: 100.00  loss: 7.9280 (7.4394)  loss_ce: 2.9249 (2.8451)  loss_bbox: 3.3193 (2.9986)  loss_giou: 1.6253 (1.5957)  loss_ce_unscaled: 1.4625 (1.4226)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6639 (0.5997)  loss_giou_unscaled: 0.8127 (0.7978)  cardinality_error_unscaled: 29.0000 (27.7751)  loss_ce_0_unscaled: 1.2676 (1.2766)  loss_bbox_0_unscaled: 0.6639 (0.5413)  loss_giou_0_unscaled: 0.5990 (0.7340)  cardinality_error_0_unscaled: 79.0000 (78.4165)  loss_ce_1_unscaled: 1.2352 (1.2444)  loss_bbox_1_unscaled: 0.6639 (0.5726)  loss_giou_1_unscaled: 0.8127 (0.7668)  cardinality_error_1_unscaled: 49.0000 (48.5545)  time: 0.1085  data: 0.0066  max mem: 594\n",
      "Test:  [2770/4410]  eta: 0:03:02  class_error: 100.00  loss: 8.0217 (7.4419)  loss_ce: 2.9208 (2.8455)  loss_bbox: 3.4839 (3.0006)  loss_giou: 1.6677 (1.5958)  loss_ce_unscaled: 1.4604 (1.4228)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6968 (0.6001)  loss_giou_unscaled: 0.8339 (0.7979)  cardinality_error_unscaled: 29.0000 (27.7795)  loss_ce_0_unscaled: 1.2692 (1.2766)  loss_bbox_0_unscaled: 0.7100 (0.5419)  loss_giou_0_unscaled: 0.5873 (0.7336)  cardinality_error_0_unscaled: 79.0000 (78.4186)  loss_ce_1_unscaled: 1.2183 (1.2443)  loss_bbox_1_unscaled: 0.6968 (0.5731)  loss_giou_1_unscaled: 0.8451 (0.7671)  cardinality_error_1_unscaled: 49.0000 (48.5561)  time: 0.1073  data: 0.0064  max mem: 594\n",
      "Test:  [2780/4410]  eta: 0:03:01  class_error: 100.00  loss: 8.0686 (7.4444)  loss_ce: 2.9107 (2.8458)  loss_bbox: 3.5264 (3.0027)  loss_giou: 1.6902 (1.5960)  loss_ce_unscaled: 1.4553 (1.4229)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7053 (0.6005)  loss_giou_unscaled: 0.8451 (0.7980)  cardinality_error_unscaled: 29.0000 (27.7839)  loss_ce_0_unscaled: 1.2800 (1.2766)  loss_bbox_0_unscaled: 0.7152 (0.5426)  loss_giou_0_unscaled: 0.5925 (0.7332)  cardinality_error_0_unscaled: 79.0000 (78.4207)  loss_ce_1_unscaled: 1.2117 (1.2442)  loss_bbox_1_unscaled: 0.7053 (0.5736)  loss_giou_1_unscaled: 0.8453 (0.7672)  cardinality_error_1_unscaled: 49.0000 (48.5577)  time: 0.1077  data: 0.0064  max mem: 594\n",
      "Test:  [2790/4410]  eta: 0:02:59  class_error: 100.00  loss: 8.0954 (7.4468)  loss_ce: 2.9107 (2.8461)  loss_bbox: 3.5264 (3.0046)  loss_giou: 1.6927 (1.5962)  loss_ce_unscaled: 1.4553 (1.4230)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7053 (0.6009)  loss_giou_unscaled: 0.8463 (0.7981)  cardinality_error_unscaled: 29.0000 (27.7882)  loss_ce_0_unscaled: 1.2678 (1.2765)  loss_bbox_0_unscaled: 0.7120 (0.5431)  loss_giou_0_unscaled: 0.5925 (0.7328)  cardinality_error_0_unscaled: 79.0000 (78.4228)  loss_ce_1_unscaled: 1.2105 (1.2440)  loss_bbox_1_unscaled: 0.7009 (0.5740)  loss_giou_1_unscaled: 0.8463 (0.7675)  cardinality_error_1_unscaled: 49.0000 (48.5593)  time: 0.1089  data: 0.0064  max mem: 594\n",
      "Test:  [2800/4410]  eta: 0:02:58  class_error: 100.00  loss: 8.0284 (7.4482)  loss_ce: 2.9381 (2.8464)  loss_bbox: 3.3836 (3.0059)  loss_giou: 1.5719 (1.5959)  loss_ce_unscaled: 1.4690 (1.4232)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6767 (0.6012)  loss_giou_unscaled: 0.7860 (0.7979)  cardinality_error_unscaled: 29.0000 (27.7926)  loss_ce_0_unscaled: 1.2632 (1.2764)  loss_bbox_0_unscaled: 0.6924 (0.5435)  loss_giou_0_unscaled: 0.5908 (0.7327)  cardinality_error_0_unscaled: 79.0000 (78.4248)  loss_ce_1_unscaled: 1.2118 (1.2440)  loss_bbox_1_unscaled: 0.6722 (0.5743)  loss_giou_1_unscaled: 0.8347 (0.7675)  cardinality_error_1_unscaled: 49.0000 (48.5609)  time: 0.1097  data: 0.0064  max mem: 594\n",
      "Test:  [2810/4410]  eta: 0:02:57  class_error: 100.00  loss: 7.7418 (7.4492)  loss_ce: 2.9537 (2.8468)  loss_bbox: 3.2120 (3.0066)  loss_giou: 1.5207 (1.5958)  loss_ce_unscaled: 1.4768 (1.4234)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6424 (0.6013)  loss_giou_unscaled: 0.7603 (0.7979)  cardinality_error_unscaled: 29.0000 (27.7969)  loss_ce_0_unscaled: 1.2340 (1.2763)  loss_bbox_0_unscaled: 0.6424 (0.5439)  loss_giou_0_unscaled: 0.8289 (0.7330)  cardinality_error_0_unscaled: 79.0000 (78.4269)  loss_ce_1_unscaled: 1.2326 (1.2440)  loss_bbox_1_unscaled: 0.6294 (0.5745)  loss_giou_1_unscaled: 0.8254 (0.7676)  cardinality_error_1_unscaled: 49.0000 (48.5624)  time: 0.1106  data: 0.0064  max mem: 594\n",
      "Test:  [2820/4410]  eta: 0:02:56  class_error: 100.00  loss: 7.7418 (7.4499)  loss_ce: 2.9610 (2.8472)  loss_bbox: 3.1887 (3.0073)  loss_giou: 1.5003 (1.5953)  loss_ce_unscaled: 1.4805 (1.4236)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6377 (0.6015)  loss_giou_unscaled: 0.7502 (0.7977)  cardinality_error_unscaled: 29.0000 (27.8011)  loss_ce_0_unscaled: 1.2340 (1.2762)  loss_bbox_0_unscaled: 0.6377 (0.5442)  loss_giou_0_unscaled: 0.7862 (0.7330)  cardinality_error_0_unscaled: 79.0000 (78.4289)  loss_ce_1_unscaled: 1.2421 (1.2440)  loss_bbox_1_unscaled: 0.6294 (0.5747)  loss_giou_1_unscaled: 0.7280 (0.7674)  cardinality_error_1_unscaled: 49.0000 (48.5640)  time: 0.1121  data: 0.0066  max mem: 594\n",
      "Test:  [2830/4410]  eta: 0:02:55  class_error: 100.00  loss: 7.6328 (7.4496)  loss_ce: 2.9610 (2.8475)  loss_bbox: 3.1887 (3.0074)  loss_giou: 1.3804 (1.5946)  loss_ce_unscaled: 1.4805 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6377 (0.6015)  loss_giou_unscaled: 0.6902 (0.7973)  cardinality_error_unscaled: 29.0000 (27.8054)  loss_ce_0_unscaled: 1.2424 (1.2761)  loss_bbox_0_unscaled: 0.6145 (0.5444)  loss_giou_0_unscaled: 0.7117 (0.7328)  cardinality_error_0_unscaled: 79.0000 (78.4309)  loss_ce_1_unscaled: 1.2394 (1.2440)  loss_bbox_1_unscaled: 0.6145 (0.5748)  loss_giou_1_unscaled: 0.6859 (0.7671)  cardinality_error_1_unscaled: 49.0000 (48.5655)  time: 0.1133  data: 0.0067  max mem: 594\n",
      "Test:  [2840/4410]  eta: 0:02:54  class_error: 100.00  loss: 6.9389 (7.4471)  loss_ce: 2.9636 (2.8480)  loss_bbox: 2.7469 (3.0058)  loss_giou: 1.2687 (1.5933)  loss_ce_unscaled: 1.4818 (1.4240)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5494 (0.6012)  loss_giou_unscaled: 0.6343 (0.7966)  cardinality_error_unscaled: 29.0000 (27.8096)  loss_ce_0_unscaled: 1.2346 (1.2759)  loss_bbox_0_unscaled: 0.5485 (0.5442)  loss_giou_0_unscaled: 0.6316 (0.7324)  cardinality_error_0_unscaled: 79.0000 (78.4329)  loss_ce_1_unscaled: 1.2447 (1.2440)  loss_bbox_1_unscaled: 0.5485 (0.5746)  loss_giou_1_unscaled: 0.6316 (0.7666)  cardinality_error_1_unscaled: 49.0000 (48.5671)  time: 0.1097  data: 0.0065  max mem: 594\n",
      "Test:  [2850/4410]  eta: 0:02:53  class_error: 100.00  loss: 6.8653 (7.4464)  loss_ce: 2.9636 (2.8483)  loss_bbox: 2.6466 (3.0055)  loss_giou: 1.2545 (1.5926)  loss_ce_unscaled: 1.4818 (1.4242)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5293 (0.6011)  loss_giou_unscaled: 0.6273 (0.7963)  cardinality_error_unscaled: 29.0000 (27.8134)  loss_ce_0_unscaled: 1.2342 (1.2758)  loss_bbox_0_unscaled: 0.4978 (0.5441)  loss_giou_0_unscaled: 0.6214 (0.7320)  cardinality_error_0_unscaled: 79.0000 (78.4346)  loss_ce_1_unscaled: 1.2471 (1.2440)  loss_bbox_1_unscaled: 0.4978 (0.5744)  loss_giou_1_unscaled: 0.6214 (0.7661)  cardinality_error_1_unscaled: 49.0000 (48.5682)  time: 0.1076  data: 0.0064  max mem: 594\n",
      "Test:  [2860/4410]  eta: 0:02:52  class_error: 100.00  loss: 7.7882 (7.4490)  loss_ce: 2.9290 (2.8486)  loss_bbox: 3.1594 (3.0069)  loss_giou: 1.6080 (1.5935)  loss_ce_unscaled: 1.4645 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6319 (0.6014)  loss_giou_unscaled: 0.8040 (0.7968)  cardinality_error_unscaled: 28.0000 (27.8141)  loss_ce_0_unscaled: 1.2388 (1.2757)  loss_bbox_0_unscaled: 0.4978 (0.5440)  loss_giou_0_unscaled: 0.6914 (0.7321)  cardinality_error_0_unscaled: 78.0000 (78.4331)  loss_ce_1_unscaled: 1.2277 (1.2438)  loss_bbox_1_unscaled: 0.5252 (0.5745)  loss_giou_1_unscaled: 0.7656 (0.7665)  cardinality_error_1_unscaled: 48.0000 (48.5662)  time: 0.1076  data: 0.0064  max mem: 594\n",
      "Test:  [2870/4410]  eta: 0:02:51  class_error: 100.00  loss: 8.2249 (7.4510)  loss_ce: 2.9207 (2.8488)  loss_bbox: 3.3664 (3.0079)  loss_giou: 1.8051 (1.5942)  loss_ce_unscaled: 1.4604 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6733 (0.6016)  loss_giou_unscaled: 0.9025 (0.7971)  cardinality_error_unscaled: 28.0000 (27.8144)  loss_ce_0_unscaled: 1.2604 (1.2757)  loss_bbox_0_unscaled: 0.4971 (0.5440)  loss_giou_0_unscaled: 0.7358 (0.7321)  cardinality_error_0_unscaled: 78.0000 (78.4316)  loss_ce_1_unscaled: 1.2030 (1.2437)  loss_bbox_1_unscaled: 0.5994 (0.5746)  loss_giou_1_unscaled: 0.8726 (0.7669)  cardinality_error_1_unscaled: 48.0000 (48.5643)  time: 0.1077  data: 0.0062  max mem: 594\n",
      "Test:  [2880/4410]  eta: 0:02:49  class_error: 100.00  loss: 8.1286 (7.4532)  loss_ce: 2.9191 (2.8491)  loss_bbox: 3.3251 (3.0091)  loss_giou: 1.7887 (1.5951)  loss_ce_unscaled: 1.4596 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6650 (0.6018)  loss_giou_unscaled: 0.8943 (0.7975)  cardinality_error_unscaled: 28.0000 (27.8153)  loss_ce_0_unscaled: 1.2609 (1.2756)  loss_bbox_0_unscaled: 0.4971 (0.5440)  loss_giou_0_unscaled: 0.7603 (0.7323)  cardinality_error_0_unscaled: 78.0000 (78.4304)  loss_ce_1_unscaled: 1.2285 (1.2436)  loss_bbox_1_unscaled: 0.5994 (0.5747)  loss_giou_1_unscaled: 0.8622 (0.7671)  cardinality_error_1_unscaled: 48.0000 (48.5627)  time: 0.1101  data: 0.0061  max mem: 594\n",
      "Test:  [2890/4410]  eta: 0:02:48  class_error: 100.00  loss: 8.0570 (7.4554)  loss_ce: 2.9214 (2.8494)  loss_bbox: 3.4873 (3.0116)  loss_giou: 1.7051 (1.5944)  loss_ce_unscaled: 1.4607 (1.4247)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6975 (0.6023)  loss_giou_unscaled: 0.8525 (0.7972)  cardinality_error_unscaled: 28.0000 (27.8191)  loss_ce_0_unscaled: 1.2683 (1.2756)  loss_bbox_0_unscaled: 0.6299 (0.5445)  loss_giou_0_unscaled: 0.7101 (0.7320)  cardinality_error_0_unscaled: 79.0000 (78.4324)  loss_ce_1_unscaled: 1.2411 (1.2437)  loss_bbox_1_unscaled: 0.6799 (0.5752)  loss_giou_1_unscaled: 0.7118 (0.7668)  cardinality_error_1_unscaled: 49.0000 (48.5642)  time: 0.1096  data: 0.0061  max mem: 594\n",
      "Test:  [2900/4410]  eta: 0:02:47  class_error: 100.00  loss: 7.9685 (7.4571)  loss_ce: 2.9405 (2.8496)  loss_bbox: 3.5849 (3.0133)  loss_giou: 1.3801 (1.5941)  loss_ce_unscaled: 1.4702 (1.4248)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7170 (0.6027)  loss_giou_unscaled: 0.6900 (0.7971)  cardinality_error_unscaled: 29.0000 (27.8221)  loss_ce_0_unscaled: 1.3011 (1.2757)  loss_bbox_0_unscaled: 0.7009 (0.5449)  loss_giou_0_unscaled: 0.6768 (0.7319)  cardinality_error_0_unscaled: 79.0000 (78.4340)  loss_ce_1_unscaled: 1.2578 (1.2437)  loss_bbox_1_unscaled: 0.7009 (0.5754)  loss_giou_1_unscaled: 0.6802 (0.7665)  cardinality_error_1_unscaled: 49.0000 (48.5657)  time: 0.1087  data: 0.0063  max mem: 594\n",
      "Test:  [2910/4410]  eta: 0:02:46  class_error: 100.00  loss: 7.9528 (7.4584)  loss_ce: 2.9382 (2.8499)  loss_bbox: 3.5507 (3.0149)  loss_giou: 1.3957 (1.5937)  loss_ce_unscaled: 1.4691 (1.4249)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7101 (0.6030)  loss_giou_unscaled: 0.6978 (0.7968)  cardinality_error_unscaled: 29.0000 (27.8245)  loss_ce_0_unscaled: 1.2902 (1.2757)  loss_bbox_0_unscaled: 0.6590 (0.5452)  loss_giou_0_unscaled: 0.6712 (0.7318)  cardinality_error_0_unscaled: 79.0000 (78.4346)  loss_ce_1_unscaled: 1.2582 (1.2438)  loss_bbox_1_unscaled: 0.6590 (0.5757)  loss_giou_1_unscaled: 0.6712 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5672)  time: 0.1098  data: 0.0063  max mem: 594\n",
      "Test:  [2920/4410]  eta: 0:02:45  class_error: 100.00  loss: 7.8920 (7.4588)  loss_ce: 2.8072 (2.8495)  loss_bbox: 3.5507 (3.0162)  loss_giou: 1.3701 (1.5931)  loss_ce_unscaled: 1.4036 (1.4247)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7101 (0.6032)  loss_giou_unscaled: 0.6851 (0.7966)  cardinality_error_unscaled: 29.0000 (27.8268)  loss_ce_0_unscaled: 1.2489 (1.2756)  loss_bbox_0_unscaled: 0.5840 (0.5452)  loss_giou_0_unscaled: 0.6559 (0.7315)  cardinality_error_0_unscaled: 79.0000 (78.4351)  loss_ce_1_unscaled: 1.2582 (1.2438)  loss_bbox_1_unscaled: 0.5840 (0.5756)  loss_giou_1_unscaled: 0.6567 (0.7659)  cardinality_error_1_unscaled: 49.0000 (48.5686)  time: 0.1080  data: 0.0061  max mem: 594\n",
      "Test:  [2930/4410]  eta: 0:02:44  class_error: 100.00  loss: 7.6159 (7.4588)  loss_ce: 2.7485 (2.8492)  loss_bbox: 3.5685 (3.0171)  loss_giou: 1.3649 (1.5924)  loss_ce_unscaled: 1.3742 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7137 (0.6034)  loss_giou_unscaled: 0.6825 (0.7962)  cardinality_error_unscaled: 29.0000 (27.8284)  loss_ce_0_unscaled: 1.2384 (1.2754)  loss_bbox_0_unscaled: 0.5246 (0.5450)  loss_giou_0_unscaled: 0.6534 (0.7313)  cardinality_error_0_unscaled: 79.0000 (78.4357)  loss_ce_1_unscaled: 1.2392 (1.2437)  loss_bbox_1_unscaled: 0.5413 (0.5755)  loss_giou_1_unscaled: 0.6534 (0.7656)  cardinality_error_1_unscaled: 49.0000 (48.5701)  time: 0.1089  data: 0.0063  max mem: 594\n",
      "Test:  [2940/4410]  eta: 0:02:43  class_error: 100.00  loss: 7.6159 (7.4595)  loss_ce: 2.7551 (2.8488)  loss_bbox: 3.1386 (3.0183)  loss_giou: 1.3869 (1.5923)  loss_ce_unscaled: 1.3775 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6277 (0.6037)  loss_giou_unscaled: 0.6934 (0.7962)  cardinality_error_unscaled: 29.0000 (27.8276)  loss_ce_0_unscaled: 1.2385 (1.2753)  loss_bbox_0_unscaled: 0.5407 (0.5451)  loss_giou_0_unscaled: 0.6792 (0.7313)  cardinality_error_0_unscaled: 79.0000 (78.4342)  loss_ce_1_unscaled: 1.2389 (1.2437)  loss_bbox_1_unscaled: 0.5686 (0.5755)  loss_giou_1_unscaled: 0.6792 (0.7654)  cardinality_error_1_unscaled: 49.0000 (48.5716)  time: 0.1109  data: 0.0066  max mem: 594\n",
      "Test:  [2950/4410]  eta: 0:02:42  class_error: 100.00  loss: 8.0902 (7.4638)  loss_ce: 2.7844 (2.8486)  loss_bbox: 3.1027 (3.0199)  loss_giou: 2.2568 (1.5953)  loss_ce_unscaled: 1.3922 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6205 (0.6040)  loss_giou_unscaled: 1.1284 (0.7976)  cardinality_error_unscaled: 28.0000 (27.8282)  loss_ce_0_unscaled: 1.2483 (1.2754)  loss_bbox_0_unscaled: 0.5766 (0.5452)  loss_giou_0_unscaled: 0.8849 (0.7321)  cardinality_error_0_unscaled: 78.0000 (78.4331)  loss_ce_1_unscaled: 1.2423 (1.2438)  loss_bbox_1_unscaled: 0.5779 (0.5755)  loss_giou_1_unscaled: 0.9020 (0.7663)  cardinality_error_1_unscaled: 49.0000 (48.5730)  time: 0.1094  data: 0.0067  max mem: 594\n",
      "Test:  [2960/4410]  eta: 0:02:40  class_error: 100.00  loss: 8.2581 (7.4678)  loss_ce: 2.7812 (2.8484)  loss_bbox: 3.0150 (3.0211)  loss_giou: 2.3548 (1.5983)  loss_ce_unscaled: 1.3906 (1.4242)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6030 (0.6042)  loss_giou_unscaled: 1.1774 (0.7991)  cardinality_error_unscaled: 28.0000 (27.8257)  loss_ce_0_unscaled: 1.2929 (1.2754)  loss_bbox_0_unscaled: 0.5760 (0.5452)  loss_giou_0_unscaled: 0.9773 (0.7331)  cardinality_error_0_unscaled: 78.0000 (78.4330)  loss_ce_1_unscaled: 1.2551 (1.2438)  loss_bbox_1_unscaled: 0.5917 (0.5755)  loss_giou_1_unscaled: 0.9736 (0.7670)  cardinality_error_1_unscaled: 49.0000 (48.5745)  time: 0.1104  data: 0.0067  max mem: 594\n",
      "Test:  [2970/4410]  eta: 0:02:39  class_error: 100.00  loss: 8.0793 (7.4709)  loss_ce: 2.7808 (2.8482)  loss_bbox: 2.9039 (3.0220)  loss_giou: 2.3603 (1.6008)  loss_ce_unscaled: 1.3904 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5808 (0.6044)  loss_giou_unscaled: 1.1801 (0.8004)  cardinality_error_unscaled: 28.0000 (27.8246)  loss_ce_0_unscaled: 1.2929 (1.2755)  loss_bbox_0_unscaled: 0.5827 (0.5453)  loss_giou_0_unscaled: 0.9400 (0.7336)  cardinality_error_0_unscaled: 78.0000 (78.4318)  loss_ce_1_unscaled: 1.2537 (1.2438)  loss_bbox_1_unscaled: 0.5931 (0.5755)  loss_giou_1_unscaled: 0.9555 (0.7675)  cardinality_error_1_unscaled: 49.0000 (48.5759)  time: 0.1100  data: 0.0067  max mem: 594\n",
      "Test:  [2980/4410]  eta: 0:02:38  class_error: 100.00  loss: 7.3106 (7.4684)  loss_ce: 2.7914 (2.8480)  loss_bbox: 2.7856 (3.0207)  loss_giou: 1.5642 (1.5997)  loss_ce_unscaled: 1.3957 (1.4240)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5571 (0.6041)  loss_giou_unscaled: 0.7821 (0.7998)  cardinality_error_unscaled: 29.0000 (27.8282)  loss_ce_0_unscaled: 1.2645 (1.2754)  loss_bbox_0_unscaled: 0.5090 (0.5451)  loss_giou_0_unscaled: 0.5950 (0.7331)  cardinality_error_0_unscaled: 78.0000 (78.4321)  loss_ce_1_unscaled: 1.2488 (1.2438)  loss_bbox_1_unscaled: 0.5339 (0.5752)  loss_giou_1_unscaled: 0.7821 (0.7671)  cardinality_error_1_unscaled: 49.0000 (48.5773)  time: 0.1086  data: 0.0067  max mem: 594\n",
      "Test:  [2990/4410]  eta: 0:02:37  class_error: 100.00  loss: 6.5114 (7.4652)  loss_ce: 2.7874 (2.8478)  loss_bbox: 2.5191 (3.0191)  loss_giou: 1.1882 (1.5983)  loss_ce_unscaled: 1.3937 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5038 (0.6038)  loss_giou_unscaled: 0.5941 (0.7992)  cardinality_error_unscaled: 29.0000 (27.8322)  loss_ce_0_unscaled: 1.2481 (1.2753)  loss_bbox_0_unscaled: 0.4674 (0.5447)  loss_giou_0_unscaled: 0.5897 (0.7326)  cardinality_error_0_unscaled: 79.0000 (78.4340)  loss_ce_1_unscaled: 1.2382 (1.2438)  loss_bbox_1_unscaled: 0.4911 (0.5749)  loss_giou_1_unscaled: 0.5944 (0.7666)  cardinality_error_1_unscaled: 49.0000 (48.5787)  time: 0.1098  data: 0.0068  max mem: 594\n",
      "Test:  [3000/4410]  eta: 0:02:36  class_error: 100.00  loss: 6.4803 (7.4627)  loss_ce: 2.7676 (2.8476)  loss_bbox: 2.5175 (3.0179)  loss_giou: 1.1865 (1.5973)  loss_ce_unscaled: 1.3838 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5035 (0.6036)  loss_giou_unscaled: 0.5932 (0.7986)  cardinality_error_unscaled: 29.0000 (27.8357)  loss_ce_0_unscaled: 1.2454 (1.2753)  loss_bbox_0_unscaled: 0.4223 (0.5443)  loss_giou_0_unscaled: 0.5911 (0.7322)  cardinality_error_0_unscaled: 79.0000 (78.4355)  loss_ce_1_unscaled: 1.2308 (1.2438)  loss_bbox_1_unscaled: 0.4819 (0.5745)  loss_giou_1_unscaled: 0.5976 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5801)  time: 0.1087  data: 0.0068  max mem: 594\n",
      "Test:  [3010/4410]  eta: 0:02:35  class_error: 100.00  loss: 6.9208 (7.4626)  loss_ce: 2.8321 (2.8478)  loss_bbox: 2.8972 (3.0183)  loss_giou: 1.2615 (1.5965)  loss_ce_unscaled: 1.4160 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5794 (0.6037)  loss_giou_unscaled: 0.6308 (0.7982)  cardinality_error_unscaled: 29.0000 (27.8393)  loss_ce_0_unscaled: 1.2733 (1.2753)  loss_bbox_0_unscaled: 0.5365 (0.5446)  loss_giou_0_unscaled: 0.6194 (0.7320)  cardinality_error_0_unscaled: 79.0000 (78.4374)  loss_ce_1_unscaled: 1.2617 (1.2439)  loss_bbox_1_unscaled: 0.5706 (0.5747)  loss_giou_1_unscaled: 0.6308 (0.7659)  cardinality_error_1_unscaled: 49.0000 (48.5815)  time: 0.1071  data: 0.0066  max mem: 594\n",
      "Test:  [3020/4410]  eta: 0:02:34  class_error: 100.00  loss: 7.7230 (7.4637)  loss_ce: 2.9140 (2.8480)  loss_bbox: 3.3011 (3.0194)  loss_giou: 1.4552 (1.5962)  loss_ce_unscaled: 1.4570 (1.4240)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6602 (0.6039)  loss_giou_unscaled: 0.7276 (0.7981)  cardinality_error_unscaled: 29.0000 (27.8428)  loss_ce_0_unscaled: 1.2863 (1.2753)  loss_bbox_0_unscaled: 0.6560 (0.5450)  loss_giou_0_unscaled: 0.6580 (0.7319)  cardinality_error_0_unscaled: 79.0000 (78.4393)  loss_ce_1_unscaled: 1.2705 (1.2439)  loss_bbox_1_unscaled: 0.6602 (0.5750)  loss_giou_1_unscaled: 0.7127 (0.7658)  cardinality_error_1_unscaled: 49.0000 (48.5829)  time: 0.1099  data: 0.0066  max mem: 594\n",
      "Test:  [3030/4410]  eta: 0:02:33  class_error: 100.00  loss: 7.7484 (7.4641)  loss_ce: 2.9129 (2.8482)  loss_bbox: 3.2995 (3.0196)  loss_giou: 1.5499 (1.5963)  loss_ce_unscaled: 1.4565 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6599 (0.6039)  loss_giou_unscaled: 0.7750 (0.7981)  cardinality_error_unscaled: 29.0000 (27.8459)  loss_ce_0_unscaled: 1.2863 (1.2754)  loss_bbox_0_unscaled: 0.6530 (0.5452)  loss_giou_0_unscaled: 0.6580 (0.7317)  cardinality_error_0_unscaled: 79.0000 (78.4411)  loss_ce_1_unscaled: 1.2653 (1.2440)  loss_bbox_1_unscaled: 0.6530 (0.5751)  loss_giou_1_unscaled: 0.7467 (0.7659)  cardinality_error_1_unscaled: 49.0000 (48.5843)  time: 0.1103  data: 0.0068  max mem: 594\n",
      "Test:  [3040/4410]  eta: 0:02:32  class_error: 100.00  loss: 6.9270 (7.4604)  loss_ce: 2.8157 (2.8478)  loss_bbox: 2.7807 (3.0176)  loss_giou: 1.3815 (1.5951)  loss_ce_unscaled: 1.4078 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5561 (0.6035)  loss_giou_unscaled: 0.6907 (0.7975)  cardinality_error_unscaled: 29.0000 (27.8497)  loss_ce_0_unscaled: 1.2708 (1.2753)  loss_bbox_0_unscaled: 0.4082 (0.5445)  loss_giou_0_unscaled: 0.5838 (0.7309)  cardinality_error_0_unscaled: 79.0000 (78.4429)  loss_ce_1_unscaled: 1.2416 (1.2439)  loss_bbox_1_unscaled: 0.5561 (0.5748)  loss_giou_1_unscaled: 0.6779 (0.7653)  cardinality_error_1_unscaled: 49.0000 (48.5857)  time: 0.1092  data: 0.0065  max mem: 594\n",
      "Test:  [3050/4410]  eta: 0:02:30  class_error: 100.00  loss: 6.6302 (7.4583)  loss_ce: 2.7085 (2.8474)  loss_bbox: 2.5730 (3.0168)  loss_giou: 1.3557 (1.5941)  loss_ce_unscaled: 1.3542 (1.4237)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5146 (0.6034)  loss_giou_unscaled: 0.6779 (0.7971)  cardinality_error_unscaled: 29.0000 (27.8535)  loss_ce_0_unscaled: 1.2708 (1.2754)  loss_bbox_0_unscaled: 0.3131 (0.5437)  loss_giou_0_unscaled: 0.4698 (0.7302)  cardinality_error_0_unscaled: 79.0000 (78.4448)  loss_ce_1_unscaled: 1.2275 (1.2438)  loss_bbox_1_unscaled: 0.5128 (0.5747)  loss_giou_1_unscaled: 0.6559 (0.7649)  cardinality_error_1_unscaled: 49.0000 (48.5870)  time: 0.1112  data: 0.0066  max mem: 594\n",
      "Test:  [3060/4410]  eta: 0:02:29  class_error: 100.00  loss: 6.6402 (7.4558)  loss_ce: 2.7432 (2.8471)  loss_bbox: 2.5823 (3.0155)  loss_giou: 1.3559 (1.5933)  loss_ce_unscaled: 1.3716 (1.4235)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5165 (0.6031)  loss_giou_unscaled: 0.6780 (0.7966)  cardinality_error_unscaled: 29.0000 (27.8566)  loss_ce_0_unscaled: 1.2981 (1.2754)  loss_bbox_0_unscaled: 0.3902 (0.5433)  loss_giou_0_unscaled: 0.5041 (0.7295)  cardinality_error_0_unscaled: 79.0000 (78.4466)  loss_ce_1_unscaled: 1.2317 (1.2438)  loss_bbox_1_unscaled: 0.5165 (0.5745)  loss_giou_1_unscaled: 0.6537 (0.7646)  cardinality_error_1_unscaled: 49.0000 (48.5884)  time: 0.1137  data: 0.0070  max mem: 594\n",
      "Test:  [3070/4410]  eta: 0:02:28  class_error: 100.00  loss: 6.4765 (7.4517)  loss_ce: 2.8734 (2.8474)  loss_bbox: 2.3841 (3.0128)  loss_giou: 1.1899 (1.5915)  loss_ce_unscaled: 1.4367 (1.4237)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4768 (0.6026)  loss_giou_unscaled: 0.5950 (0.7958)  cardinality_error_unscaled: 29.0000 (27.8593)  loss_ce_0_unscaled: 1.3243 (1.2756)  loss_bbox_0_unscaled: 0.4161 (0.5429)  loss_giou_0_unscaled: 0.5014 (0.7287)  cardinality_error_0_unscaled: 79.0000 (78.4484)  loss_ce_1_unscaled: 1.2413 (1.2438)  loss_bbox_1_unscaled: 0.4708 (0.5741)  loss_giou_1_unscaled: 0.5914 (0.7638)  cardinality_error_1_unscaled: 49.0000 (48.5897)  time: 0.1143  data: 0.0070  max mem: 594\n",
      "Test:  [3080/4410]  eta: 0:02:27  class_error: 100.00  loss: 6.3647 (7.4479)  loss_ce: 2.9462 (2.8479)  loss_bbox: 2.2998 (3.0102)  loss_giou: 1.0807 (1.5899)  loss_ce_unscaled: 1.4731 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4600 (0.6020)  loss_giou_unscaled: 0.5404 (0.7949)  cardinality_error_unscaled: 29.0000 (27.8617)  loss_ce_0_unscaled: 1.3377 (1.2759)  loss_bbox_0_unscaled: 0.4068 (0.5425)  loss_giou_0_unscaled: 0.4627 (0.7280)  cardinality_error_0_unscaled: 79.0000 (78.4502)  loss_ce_1_unscaled: 1.2518 (1.2438)  loss_bbox_1_unscaled: 0.4552 (0.5736)  loss_giou_1_unscaled: 0.5314 (0.7631)  cardinality_error_1_unscaled: 49.0000 (48.5910)  time: 0.1117  data: 0.0070  max mem: 594\n",
      "Test:  [3090/4410]  eta: 0:02:26  class_error: 100.00  loss: 6.4615 (7.4449)  loss_ce: 2.9470 (2.8483)  loss_bbox: 2.2998 (3.0083)  loss_giou: 1.0627 (1.5884)  loss_ce_unscaled: 1.4735 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4600 (0.6017)  loss_giou_unscaled: 0.5314 (0.7942)  cardinality_error_unscaled: 29.0000 (27.8648)  loss_ce_0_unscaled: 1.3492 (1.2761)  loss_bbox_0_unscaled: 0.4473 (0.5422)  loss_giou_0_unscaled: 0.4929 (0.7273)  cardinality_error_0_unscaled: 79.0000 (78.4520)  loss_ce_1_unscaled: 1.2443 (1.2438)  loss_bbox_1_unscaled: 0.4577 (0.5733)  loss_giou_1_unscaled: 0.5314 (0.7624)  cardinality_error_1_unscaled: 49.0000 (48.5924)  time: 0.1116  data: 0.0071  max mem: 594\n",
      "Test:  [3100/4410]  eta: 0:02:25  class_error: 100.00  loss: 6.9257 (7.4443)  loss_ce: 2.9391 (2.8485)  loss_bbox: 2.7374 (3.0081)  loss_giou: 1.2861 (1.5877)  loss_ce_unscaled: 1.4695 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5475 (0.6016)  loss_giou_unscaled: 0.6430 (0.7938)  cardinality_error_unscaled: 29.0000 (27.8684)  loss_ce_0_unscaled: 1.3291 (1.2762)  loss_bbox_0_unscaled: 0.5475 (0.5424)  loss_giou_0_unscaled: 0.5975 (0.7272)  cardinality_error_0_unscaled: 79.0000 (78.4537)  loss_ce_1_unscaled: 1.2291 (1.2437)  loss_bbox_1_unscaled: 0.5475 (0.5734)  loss_giou_1_unscaled: 0.6430 (0.7622)  cardinality_error_1_unscaled: 49.0000 (48.5937)  time: 0.1134  data: 0.0070  max mem: 594\n",
      "Test:  [3110/4410]  eta: 0:02:24  class_error: 100.00  loss: 7.2616 (7.4443)  loss_ce: 2.9333 (2.8488)  loss_bbox: 2.9288 (3.0084)  loss_giou: 1.3855 (1.5871)  loss_ce_unscaled: 1.4667 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5858 (0.6017)  loss_giou_unscaled: 0.6927 (0.7936)  cardinality_error_unscaled: 29.0000 (27.8717)  loss_ce_0_unscaled: 1.3220 (1.2764)  loss_bbox_0_unscaled: 0.5858 (0.5426)  loss_giou_0_unscaled: 0.6927 (0.7271)  cardinality_error_0_unscaled: 79.0000 (78.4555)  loss_ce_1_unscaled: 1.2278 (1.2437)  loss_bbox_1_unscaled: 0.5858 (0.5735)  loss_giou_1_unscaled: 0.6927 (0.7620)  cardinality_error_1_unscaled: 49.0000 (48.5950)  time: 0.1131  data: 0.0069  max mem: 594\n",
      "Test:  [3120/4410]  eta: 0:02:23  class_error: 100.00  loss: 7.2616 (7.4438)  loss_ce: 2.9418 (2.8491)  loss_bbox: 2.9492 (3.0083)  loss_giou: 1.3589 (1.5863)  loss_ce_unscaled: 1.4709 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5898 (0.6017)  loss_giou_unscaled: 0.6794 (0.7931)  cardinality_error_unscaled: 29.0000 (27.8750)  loss_ce_0_unscaled: 1.3122 (1.2765)  loss_bbox_0_unscaled: 0.5898 (0.5427)  loss_giou_0_unscaled: 0.6794 (0.7269)  cardinality_error_0_unscaled: 79.0000 (78.4569)  loss_ce_1_unscaled: 1.2101 (1.2435)  loss_bbox_1_unscaled: 0.5898 (0.5736)  loss_giou_1_unscaled: 0.6794 (0.7617)  cardinality_error_1_unscaled: 49.0000 (48.5960)  time: 0.1102  data: 0.0067  max mem: 594\n",
      "Test:  [3130/4410]  eta: 0:02:22  class_error: 100.00  loss: 7.0536 (7.4427)  loss_ce: 2.9202 (2.8493)  loss_bbox: 2.7424 (3.0073)  loss_giou: 1.4716 (1.5860)  loss_ce_unscaled: 1.4601 (1.4247)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5485 (0.6015)  loss_giou_unscaled: 0.7358 (0.7930)  cardinality_error_unscaled: 28.0000 (27.8754)  loss_ce_0_unscaled: 1.2692 (1.2764)  loss_bbox_0_unscaled: 0.5485 (0.5427)  loss_giou_0_unscaled: 0.7457 (0.7270)  cardinality_error_0_unscaled: 78.0000 (78.4554)  loss_ce_1_unscaled: 1.2004 (1.2434)  loss_bbox_1_unscaled: 0.5485 (0.5734)  loss_giou_1_unscaled: 0.7358 (0.7617)  cardinality_error_1_unscaled: 48.0000 (48.5941)  time: 0.1077  data: 0.0066  max mem: 594\n",
      "Test:  [3140/4410]  eta: 0:02:20  class_error: 100.00  loss: 7.0678 (7.4415)  loss_ce: 2.9116 (2.8495)  loss_bbox: 2.6572 (3.0061)  loss_giou: 1.5096 (1.5858)  loss_ce_unscaled: 1.4558 (1.4248)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5314 (0.6012)  loss_giou_unscaled: 0.7548 (0.7929)  cardinality_error_unscaled: 28.0000 (27.8758)  loss_ce_0_unscaled: 1.2524 (1.2763)  loss_bbox_0_unscaled: 0.5314 (0.5427)  loss_giou_0_unscaled: 0.7550 (0.7271)  cardinality_error_0_unscaled: 78.0000 (78.4540)  loss_ce_1_unscaled: 1.1944 (1.2432)  loss_bbox_1_unscaled: 0.5314 (0.5733)  loss_giou_1_unscaled: 0.7550 (0.7617)  cardinality_error_1_unscaled: 48.0000 (48.5922)  time: 0.1089  data: 0.0066  max mem: 594\n",
      "Test:  [3150/4410]  eta: 0:02:19  class_error: 100.00  loss: 7.0526 (7.4404)  loss_ce: 2.9044 (2.8497)  loss_bbox: 2.6333 (3.0052)  loss_giou: 1.5095 (1.5855)  loss_ce_unscaled: 1.4522 (1.4249)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5267 (0.6010)  loss_giou_unscaled: 0.7548 (0.7928)  cardinality_error_unscaled: 28.0000 (27.8762)  loss_ce_0_unscaled: 1.2514 (1.2763)  loss_bbox_0_unscaled: 0.5267 (0.5426)  loss_giou_0_unscaled: 0.7548 (0.7272)  cardinality_error_0_unscaled: 78.0000 (78.4529)  loss_ce_1_unscaled: 1.1944 (1.2431)  loss_bbox_1_unscaled: 0.5267 (0.5731)  loss_giou_1_unscaled: 0.7548 (0.7616)  cardinality_error_1_unscaled: 48.0000 (48.5906)  time: 0.1110  data: 0.0065  max mem: 594\n",
      "Test:  [3160/4410]  eta: 0:02:18  class_error: 100.00  loss: 7.2329 (7.4418)  loss_ce: 2.9088 (2.8500)  loss_bbox: 2.8544 (3.0068)  loss_giou: 1.5055 (1.5850)  loss_ce_unscaled: 1.4544 (1.4250)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5709 (0.6014)  loss_giou_unscaled: 0.7528 (0.7925)  cardinality_error_unscaled: 28.0000 (27.8792)  loss_ce_0_unscaled: 1.2574 (1.2763)  loss_bbox_0_unscaled: 0.5267 (0.5426)  loss_giou_0_unscaled: 0.6664 (0.7267)  cardinality_error_0_unscaled: 79.0000 (78.4546)  loss_ce_1_unscaled: 1.2070 (1.2431)  loss_bbox_1_unscaled: 0.5267 (0.5730)  loss_giou_1_unscaled: 0.6664 (0.7610)  cardinality_error_1_unscaled: 49.0000 (48.5919)  time: 0.1109  data: 0.0065  max mem: 594\n",
      "Test:  [3170/4410]  eta: 0:02:17  class_error: 100.00  loss: 7.8843 (7.4433)  loss_ce: 2.9583 (2.8503)  loss_bbox: 3.5899 (3.0084)  loss_giou: 1.3991 (1.5846)  loss_ce_unscaled: 1.4791 (1.4252)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7180 (0.6017)  loss_giou_unscaled: 0.6995 (0.7923)  cardinality_error_unscaled: 29.0000 (27.8821)  loss_ce_0_unscaled: 1.2709 (1.2762)  loss_bbox_0_unscaled: 0.5049 (0.5425)  loss_giou_0_unscaled: 0.5518 (0.7262)  cardinality_error_0_unscaled: 79.0000 (78.4563)  loss_ce_1_unscaled: 1.2477 (1.2431)  loss_bbox_1_unscaled: 0.5049 (0.5728)  loss_giou_1_unscaled: 0.5518 (0.7605)  cardinality_error_1_unscaled: 49.0000 (48.5932)  time: 0.1136  data: 0.0069  max mem: 594\n",
      "Test:  [3180/4410]  eta: 0:02:16  class_error: 100.00  loss: 7.6664 (7.4438)  loss_ce: 2.9534 (2.8506)  loss_bbox: 3.2615 (3.0090)  loss_giou: 1.3991 (1.5841)  loss_ce_unscaled: 1.4767 (1.4253)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6523 (0.6018)  loss_giou_unscaled: 0.6995 (0.7921)  cardinality_error_unscaled: 29.0000 (27.8849)  loss_ce_0_unscaled: 1.2827 (1.2763)  loss_bbox_0_unscaled: 0.4992 (0.5424)  loss_giou_0_unscaled: 0.5630 (0.7259)  cardinality_error_0_unscaled: 79.0000 (78.4574)  loss_ce_1_unscaled: 1.2583 (1.2431)  loss_bbox_1_unscaled: 0.4992 (0.5726)  loss_giou_1_unscaled: 0.5630 (0.7601)  cardinality_error_1_unscaled: 49.0000 (48.5938)  time: 0.1156  data: 0.0071  max mem: 594\n",
      "Test:  [3190/4410]  eta: 0:02:15  class_error: 100.00  loss: 7.7432 (7.4459)  loss_ce: 2.8871 (2.8507)  loss_bbox: 3.2362 (3.0106)  loss_giou: 1.6482 (1.5846)  loss_ce_unscaled: 1.4436 (1.4254)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6472 (0.6021)  loss_giou_unscaled: 0.8241 (0.7923)  cardinality_error_unscaled: 27.0000 (27.8819)  loss_ce_0_unscaled: 1.2828 (1.2763)  loss_bbox_0_unscaled: 0.5629 (0.5426)  loss_giou_0_unscaled: 0.7528 (0.7262)  cardinality_error_0_unscaled: 77.0000 (78.4528)  loss_ce_1_unscaled: 1.2026 (1.2429)  loss_bbox_1_unscaled: 0.5881 (0.5729)  loss_giou_1_unscaled: 0.8083 (0.7604)  cardinality_error_1_unscaled: 47.0000 (48.5888)  time: 0.1124  data: 0.0068  max mem: 594\n",
      "Test:  [3200/4410]  eta: 0:02:14  class_error: 100.00  loss: 7.7839 (7.4468)  loss_ce: 2.8761 (2.8508)  loss_bbox: 3.1576 (3.0112)  loss_giou: 1.7149 (1.5849)  loss_ce_unscaled: 1.4380 (1.4254)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6315 (0.6022)  loss_giou_unscaled: 0.8575 (0.7924)  cardinality_error_unscaled: 27.0000 (27.8782)  loss_ce_0_unscaled: 1.2816 (1.2763)  loss_bbox_0_unscaled: 0.5928 (0.5427)  loss_giou_0_unscaled: 0.8419 (0.7266)  cardinality_error_0_unscaled: 77.0000 (78.4483)  loss_ce_1_unscaled: 1.1855 (1.2427)  loss_bbox_1_unscaled: 0.6275 (0.5730)  loss_giou_1_unscaled: 0.8430 (0.7606)  cardinality_error_1_unscaled: 47.0000 (48.5839)  time: 0.1159  data: 0.0073  max mem: 594\n",
      "Test:  [3210/4410]  eta: 0:02:13  class_error: 100.00  loss: 7.7067 (7.4480)  loss_ce: 2.8764 (2.8509)  loss_bbox: 3.1576 (3.0119)  loss_giou: 1.6312 (1.5851)  loss_ce_unscaled: 1.4382 (1.4255)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6315 (0.6024)  loss_giou_unscaled: 0.8156 (0.7926)  cardinality_error_unscaled: 27.0000 (27.8757)  loss_ce_0_unscaled: 1.2787 (1.2763)  loss_bbox_0_unscaled: 0.5862 (0.5429)  loss_giou_0_unscaled: 0.8148 (0.7268)  cardinality_error_0_unscaled: 77.0000 (78.4444)  loss_ce_1_unscaled: 1.1855 (1.2426)  loss_bbox_1_unscaled: 0.6313 (0.5732)  loss_giou_1_unscaled: 0.8140 (0.7608)  cardinality_error_1_unscaled: 47.0000 (48.5796)  time: 0.1193  data: 0.0077  max mem: 594\n",
      "Test:  [3220/4410]  eta: 0:02:12  class_error: 100.00  loss: 7.6234 (7.4483)  loss_ce: 2.9343 (2.8512)  loss_bbox: 3.1870 (3.0122)  loss_giou: 1.5315 (1.5848)  loss_ce_unscaled: 1.4672 (1.4256)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6374 (0.6024)  loss_giou_unscaled: 0.7657 (0.7924)  cardinality_error_unscaled: 29.0000 (27.8792)  loss_ce_0_unscaled: 1.2733 (1.2763)  loss_bbox_0_unscaled: 0.6094 (0.5430)  loss_giou_0_unscaled: 0.7951 (0.7269)  cardinality_error_0_unscaled: 79.0000 (78.4461)  loss_ce_1_unscaled: 1.1911 (1.2424)  loss_bbox_1_unscaled: 0.6309 (0.5732)  loss_giou_1_unscaled: 0.7664 (0.7608)  cardinality_error_1_unscaled: 49.0000 (48.5809)  time: 0.1183  data: 0.0075  max mem: 594\n",
      "Test:  [3230/4410]  eta: 0:02:11  class_error: 100.00  loss: 7.5409 (7.4486)  loss_ce: 2.9365 (2.8515)  loss_bbox: 3.1367 (3.0125)  loss_giou: 1.4847 (1.5845)  loss_ce_unscaled: 1.4682 (1.4257)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6273 (0.6025)  loss_giou_unscaled: 0.7423 (0.7923)  cardinality_error_unscaled: 29.0000 (27.8827)  loss_ce_0_unscaled: 1.2333 (1.2763)  loss_bbox_0_unscaled: 0.6016 (0.5431)  loss_giou_0_unscaled: 0.7534 (0.7270)  cardinality_error_0_unscaled: 79.0000 (78.4478)  loss_ce_1_unscaled: 1.1983 (1.2423)  loss_bbox_1_unscaled: 0.6016 (0.5733)  loss_giou_1_unscaled: 0.7534 (0.7608)  cardinality_error_1_unscaled: 49.0000 (48.5822)  time: 0.1171  data: 0.0073  max mem: 594\n",
      "Test:  [3240/4410]  eta: 0:02:09  class_error: 100.00  loss: 7.5102 (7.4487)  loss_ce: 2.9368 (2.8518)  loss_bbox: 3.0383 (3.0127)  loss_giou: 1.4787 (1.5842)  loss_ce_unscaled: 1.4684 (1.4259)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6077 (0.6025)  loss_giou_unscaled: 0.7393 (0.7921)  cardinality_error_unscaled: 29.0000 (27.8858)  loss_ce_0_unscaled: 1.2550 (1.2763)  loss_bbox_0_unscaled: 0.6016 (0.5433)  loss_giou_0_unscaled: 0.7539 (0.7271)  cardinality_error_0_unscaled: 79.0000 (78.4492)  loss_ce_1_unscaled: 1.2009 (1.2422)  loss_bbox_1_unscaled: 0.6016 (0.5733)  loss_giou_1_unscaled: 0.7428 (0.7607)  cardinality_error_1_unscaled: 49.0000 (48.5832)  time: 0.1146  data: 0.0071  max mem: 594\n",
      "Test:  [3250/4410]  eta: 0:02:08  class_error: 100.00  loss: 7.6051 (7.4502)  loss_ce: 2.9422 (2.8521)  loss_bbox: 3.1593 (3.0138)  loss_giou: 1.5575 (1.5844)  loss_ce_unscaled: 1.4711 (1.4260)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6319 (0.6028)  loss_giou_unscaled: 0.7787 (0.7922)  cardinality_error_unscaled: 28.0000 (27.8862)  loss_ce_0_unscaled: 1.2686 (1.2762)  loss_bbox_0_unscaled: 0.6235 (0.5436)  loss_giou_0_unscaled: 0.7787 (0.7274)  cardinality_error_0_unscaled: 78.0000 (78.4479)  loss_ce_1_unscaled: 1.2123 (1.2422)  loss_bbox_1_unscaled: 0.6291 (0.5736)  loss_giou_1_unscaled: 0.7787 (0.7609)  cardinality_error_1_unscaled: 48.0000 (48.5814)  time: 0.1133  data: 0.0070  max mem: 594\n",
      "Test:  [3260/4410]  eta: 0:02:07  class_error: 100.00  loss: 8.0744 (7.4529)  loss_ce: 2.9232 (2.8522)  loss_bbox: 3.4122 (3.0158)  loss_giou: 1.6902 (1.5848)  loss_ce_unscaled: 1.4616 (1.4261)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6824 (0.6032)  loss_giou_unscaled: 0.8451 (0.7924)  cardinality_error_unscaled: 28.0000 (27.8865)  loss_ce_0_unscaled: 1.2628 (1.2762)  loss_bbox_0_unscaled: 0.6553 (0.5441)  loss_giou_0_unscaled: 0.8504 (0.7278)  cardinality_error_0_unscaled: 78.0000 (78.4465)  loss_ce_1_unscaled: 1.2131 (1.2421)  loss_bbox_1_unscaled: 0.6824 (0.5741)  loss_giou_1_unscaled: 0.8205 (0.7611)  cardinality_error_1_unscaled: 48.0000 (48.5796)  time: 0.1126  data: 0.0068  max mem: 594\n",
      "Test:  [3270/4410]  eta: 0:02:06  class_error: 100.00  loss: 8.3055 (7.4554)  loss_ce: 2.9262 (2.8525)  loss_bbox: 3.6904 (3.0182)  loss_giou: 1.6070 (1.5847)  loss_ce_unscaled: 1.4631 (1.4262)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7381 (0.6036)  loss_giou_unscaled: 0.8035 (0.7924)  cardinality_error_unscaled: 28.0000 (27.8869)  loss_ce_0_unscaled: 1.2517 (1.2761)  loss_bbox_0_unscaled: 0.7030 (0.5446)  loss_giou_0_unscaled: 0.8579 (0.7280)  cardinality_error_0_unscaled: 78.0000 (78.4451)  loss_ce_1_unscaled: 1.2126 (1.2420)  loss_bbox_1_unscaled: 0.7237 (0.5745)  loss_giou_1_unscaled: 0.8378 (0.7612)  cardinality_error_1_unscaled: 48.0000 (48.5778)  time: 0.1115  data: 0.0067  max mem: 594\n",
      "Test:  [3280/4410]  eta: 0:02:05  class_error: 100.00  loss: 8.1439 (7.4566)  loss_ce: 2.9313 (2.8527)  loss_bbox: 3.6186 (3.0198)  loss_giou: 1.4438 (1.5841)  loss_ce_unscaled: 1.4657 (1.4264)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7237 (0.6040)  loss_giou_unscaled: 0.7219 (0.7920)  cardinality_error_unscaled: 28.0000 (27.8872)  loss_ce_0_unscaled: 1.2699 (1.2762)  loss_bbox_0_unscaled: 0.7030 (0.5450)  loss_giou_0_unscaled: 0.7064 (0.7277)  cardinality_error_0_unscaled: 78.0000 (78.4438)  loss_ce_1_unscaled: 1.2218 (1.2419)  loss_bbox_1_unscaled: 0.7030 (0.5749)  loss_giou_1_unscaled: 0.7219 (0.7610)  cardinality_error_1_unscaled: 48.0000 (48.5760)  time: 0.1124  data: 0.0067  max mem: 594\n",
      "Test:  [3290/4410]  eta: 0:02:04  class_error: 100.00  loss: 7.5417 (7.4564)  loss_ce: 2.9275 (2.8529)  loss_bbox: 3.2479 (3.0200)  loss_giou: 1.3295 (1.5835)  loss_ce_unscaled: 1.4637 (1.4265)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6496 (0.6040)  loss_giou_unscaled: 0.6647 (0.7917)  cardinality_error_unscaled: 28.0000 (27.8876)  loss_ce_0_unscaled: 1.2699 (1.2761)  loss_bbox_0_unscaled: 0.6340 (0.5451)  loss_giou_0_unscaled: 0.6447 (0.7276)  cardinality_error_0_unscaled: 78.0000 (78.4424)  loss_ce_1_unscaled: 1.2183 (1.2418)  loss_bbox_1_unscaled: 0.6455 (0.5750)  loss_giou_1_unscaled: 0.6647 (0.7608)  cardinality_error_1_unscaled: 48.0000 (48.5743)  time: 0.1112  data: 0.0064  max mem: 594\n",
      "Test:  [3300/4410]  eta: 0:02:03  class_error: 100.00  loss: 7.5216 (7.4570)  loss_ce: 2.9193 (2.8532)  loss_bbox: 3.1789 (3.0208)  loss_giou: 1.4018 (1.5830)  loss_ce_unscaled: 1.4597 (1.4266)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6358 (0.6042)  loss_giou_unscaled: 0.7009 (0.7915)  cardinality_error_unscaled: 28.0000 (27.8882)  loss_ce_0_unscaled: 1.2565 (1.2761)  loss_bbox_0_unscaled: 0.6115 (0.5454)  loss_giou_0_unscaled: 0.6647 (0.7275)  cardinality_error_0_unscaled: 78.0000 (78.4414)  loss_ce_1_unscaled: 1.2179 (1.2418)  loss_bbox_1_unscaled: 0.6336 (0.5752)  loss_giou_1_unscaled: 0.6920 (0.7606)  cardinality_error_1_unscaled: 48.0000 (48.5729)  time: 0.1110  data: 0.0065  max mem: 594\n",
      "Test:  [3310/4410]  eta: 0:02:02  class_error: 100.00  loss: 7.4366 (7.4569)  loss_ce: 2.9285 (2.8534)  loss_bbox: 2.8902 (3.0201)  loss_giou: 1.5425 (1.5833)  loss_ce_unscaled: 1.4642 (1.4267)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5780 (0.6040)  loss_giou_unscaled: 0.7712 (0.7917)  cardinality_error_unscaled: 29.0000 (27.8916)  loss_ce_0_unscaled: 1.2427 (1.2759)  loss_bbox_0_unscaled: 0.5780 (0.5454)  loss_giou_0_unscaled: 0.7072 (0.7278)  cardinality_error_0_unscaled: 79.0000 (78.4431)  loss_ce_1_unscaled: 1.2377 (1.2418)  loss_bbox_1_unscaled: 0.5780 (0.5752)  loss_giou_1_unscaled: 0.7712 (0.7608)  cardinality_error_1_unscaled: 49.0000 (48.5741)  time: 0.1120  data: 0.0065  max mem: 594\n",
      "Test:  [3320/4410]  eta: 0:02:01  class_error: 100.00  loss: 7.4366 (7.4572)  loss_ce: 2.9285 (2.8536)  loss_bbox: 2.8367 (3.0201)  loss_giou: 1.6878 (1.5836)  loss_ce_unscaled: 1.4642 (1.4268)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5673 (0.6040)  loss_giou_unscaled: 0.8439 (0.7918)  cardinality_error_unscaled: 29.0000 (27.8949)  loss_ce_0_unscaled: 1.2254 (1.2758)  loss_bbox_0_unscaled: 0.5643 (0.5456)  loss_giou_0_unscaled: 0.8454 (0.7282)  cardinality_error_0_unscaled: 79.0000 (78.4447)  loss_ce_1_unscaled: 1.2445 (1.2418)  loss_bbox_1_unscaled: 0.5643 (0.5752)  loss_giou_1_unscaled: 0.8454 (0.7611)  cardinality_error_1_unscaled: 49.0000 (48.5754)  time: 0.1137  data: 0.0063  max mem: 594\n",
      "Test:  [3330/4410]  eta: 0:02:00  class_error: 100.00  loss: 7.4816 (7.4568)  loss_ce: 2.9199 (2.8538)  loss_bbox: 2.8855 (3.0193)  loss_giou: 1.6603 (1.5837)  loss_ce_unscaled: 1.4600 (1.4269)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5771 (0.6039)  loss_giou_unscaled: 0.8302 (0.7919)  cardinality_error_unscaled: 29.0000 (27.8979)  loss_ce_0_unscaled: 1.2254 (1.2757)  loss_bbox_0_unscaled: 0.5659 (0.5456)  loss_giou_0_unscaled: 0.8304 (0.7284)  cardinality_error_0_unscaled: 79.0000 (78.4461)  loss_ce_1_unscaled: 1.2437 (1.2418)  loss_bbox_1_unscaled: 0.5659 (0.5752)  loss_giou_1_unscaled: 0.8304 (0.7612)  cardinality_error_1_unscaled: 49.0000 (48.5764)  time: 0.1154  data: 0.0065  max mem: 594\n",
      "Test:  [3340/4410]  eta: 0:01:58  class_error: 100.00  loss: 7.2342 (7.4557)  loss_ce: 2.8994 (2.8539)  loss_bbox: 2.7063 (3.0180)  loss_giou: 1.6517 (1.5839)  loss_ce_unscaled: 1.4497 (1.4269)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5413 (0.6036)  loss_giou_unscaled: 0.8259 (0.7919)  cardinality_error_unscaled: 28.0000 (27.8979)  loss_ce_0_unscaled: 1.2367 (1.2756)  loss_bbox_0_unscaled: 0.5198 (0.5454)  loss_giou_0_unscaled: 0.7893 (0.7285)  cardinality_error_0_unscaled: 78.0000 (78.4448)  loss_ce_1_unscaled: 1.2326 (1.2418)  loss_bbox_1_unscaled: 0.5201 (0.5749)  loss_giou_1_unscaled: 0.7893 (0.7612)  cardinality_error_1_unscaled: 48.0000 (48.5747)  time: 0.1122  data: 0.0070  max mem: 594\n",
      "Test:  [3350/4410]  eta: 0:01:57  class_error: 100.00  loss: 7.1018 (7.4548)  loss_ce: 2.8723 (2.8539)  loss_bbox: 2.6045 (3.0170)  loss_giou: 1.5939 (1.5839)  loss_ce_unscaled: 1.4362 (1.4270)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5209 (0.6034)  loss_giou_unscaled: 0.7969 (0.7920)  cardinality_error_unscaled: 28.0000 (27.8967)  loss_ce_0_unscaled: 1.2520 (1.2755)  loss_bbox_0_unscaled: 0.4701 (0.5452)  loss_giou_0_unscaled: 0.7347 (0.7284)  cardinality_error_0_unscaled: 78.0000 (78.4434)  loss_ce_1_unscaled: 1.2331 (1.2418)  loss_bbox_1_unscaled: 0.4615 (0.5746)  loss_giou_1_unscaled: 0.7805 (0.7613)  cardinality_error_1_unscaled: 48.0000 (48.5730)  time: 0.1104  data: 0.0069  max mem: 594\n",
      "Test:  [3360/4410]  eta: 0:01:56  class_error: 100.00  loss: 7.0456 (7.4534)  loss_ce: 2.8882 (2.8541)  loss_bbox: 2.5661 (3.0153)  loss_giou: 1.5980 (1.5840)  loss_ce_unscaled: 1.4441 (1.4270)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5132 (0.6031)  loss_giou_unscaled: 0.7990 (0.7920)  cardinality_error_unscaled: 28.0000 (27.8971)  loss_ce_0_unscaled: 1.2534 (1.2755)  loss_bbox_0_unscaled: 0.4527 (0.5450)  loss_giou_0_unscaled: 0.7138 (0.7284)  cardinality_error_0_unscaled: 78.0000 (78.4424)  loss_ce_1_unscaled: 1.2399 (1.2418)  loss_bbox_1_unscaled: 0.4508 (0.5743)  loss_giou_1_unscaled: 0.7844 (0.7613)  cardinality_error_1_unscaled: 48.0000 (48.5716)  time: 0.1136  data: 0.0066  max mem: 594\n",
      "Test:  [3370/4410]  eta: 0:01:55  class_error: 100.00  loss: 6.7526 (7.4516)  loss_ce: 2.8873 (2.8542)  loss_bbox: 2.4733 (3.0140)  loss_giou: 1.4744 (1.5835)  loss_ce_unscaled: 1.4437 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4947 (0.6028)  loss_giou_unscaled: 0.7372 (0.7917)  cardinality_error_unscaled: 29.0000 (27.9003)  loss_ce_0_unscaled: 1.2586 (1.2755)  loss_bbox_0_unscaled: 0.4864 (0.5448)  loss_giou_0_unscaled: 0.6724 (0.7282)  cardinality_error_0_unscaled: 79.0000 (78.4441)  loss_ce_1_unscaled: 1.2270 (1.2418)  loss_bbox_1_unscaled: 0.4864 (0.5740)  loss_giou_1_unscaled: 0.6899 (0.7610)  cardinality_error_1_unscaled: 49.0000 (48.5728)  time: 0.1120  data: 0.0067  max mem: 594\n",
      "Test:  [3380/4410]  eta: 0:01:54  class_error: 100.00  loss: 6.7526 (7.4493)  loss_ce: 2.8816 (2.8543)  loss_bbox: 2.5198 (3.0122)  loss_giou: 1.3560 (1.5829)  loss_ce_unscaled: 1.4408 (1.4272)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5040 (0.6024)  loss_giou_unscaled: 0.6780 (0.7914)  cardinality_error_unscaled: 29.0000 (27.9036)  loss_ce_0_unscaled: 1.2586 (1.2754)  loss_bbox_0_unscaled: 0.4947 (0.5446)  loss_giou_0_unscaled: 0.6649 (0.7281)  cardinality_error_0_unscaled: 79.0000 (78.4457)  loss_ce_1_unscaled: 1.2214 (1.2417)  loss_bbox_1_unscaled: 0.4947 (0.5737)  loss_giou_1_unscaled: 0.6649 (0.7607)  cardinality_error_1_unscaled: 49.0000 (48.5741)  time: 0.1100  data: 0.0067  max mem: 594\n",
      "Test:  [3390/4410]  eta: 0:01:53  class_error: 100.00  loss: 6.8348 (7.4480)  loss_ce: 2.8796 (2.8544)  loss_bbox: 2.5198 (3.0112)  loss_giou: 1.3797 (1.5823)  loss_ce_unscaled: 1.4398 (1.4272)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5040 (0.6022)  loss_giou_unscaled: 0.6898 (0.7912)  cardinality_error_unscaled: 29.0000 (27.9068)  loss_ce_0_unscaled: 1.2580 (1.2753)  loss_bbox_0_unscaled: 0.5039 (0.5444)  loss_giou_0_unscaled: 0.6563 (0.7279)  cardinality_error_0_unscaled: 79.0000 (78.4474)  loss_ce_1_unscaled: 1.2230 (1.2417)  loss_bbox_1_unscaled: 0.5039 (0.5735)  loss_giou_1_unscaled: 0.6563 (0.7604)  cardinality_error_1_unscaled: 49.0000 (48.5753)  time: 0.1114  data: 0.0067  max mem: 594\n",
      "Test:  [3400/4410]  eta: 0:01:52  class_error: 100.00  loss: 7.6094 (7.4502)  loss_ce: 2.9031 (2.8547)  loss_bbox: 3.0340 (3.0132)  loss_giou: 1.4384 (1.5823)  loss_ce_unscaled: 1.4515 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6068 (0.6026)  loss_giou_unscaled: 0.7192 (0.7912)  cardinality_error_unscaled: 29.0000 (27.9100)  loss_ce_0_unscaled: 1.2668 (1.2754)  loss_bbox_0_unscaled: 0.5378 (0.5449)  loss_giou_0_unscaled: 0.7126 (0.7280)  cardinality_error_0_unscaled: 79.0000 (78.4487)  loss_ce_1_unscaled: 1.2296 (1.2417)  loss_bbox_1_unscaled: 0.5378 (0.5739)  loss_giou_1_unscaled: 0.7172 (0.7605)  cardinality_error_1_unscaled: 49.0000 (48.5766)  time: 0.1098  data: 0.0067  max mem: 594\n",
      "Test:  [3410/4410]  eta: 0:01:51  class_error: 100.00  loss: 8.1914 (7.4526)  loss_ce: 2.9341 (2.8549)  loss_bbox: 3.7447 (3.0157)  loss_giou: 1.4436 (1.5821)  loss_ce_unscaled: 1.4671 (1.4274)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7489 (0.6031)  loss_giou_unscaled: 0.7218 (0.7910)  cardinality_error_unscaled: 29.0000 (27.9132)  loss_ce_0_unscaled: 1.3284 (1.2756)  loss_bbox_0_unscaled: 0.7440 (0.5455)  loss_giou_0_unscaled: 0.7224 (0.7281)  cardinality_error_0_unscaled: 79.0000 (78.4503)  loss_ce_1_unscaled: 1.2499 (1.2417)  loss_bbox_1_unscaled: 0.7457 (0.5744)  loss_giou_1_unscaled: 0.7368 (0.7605)  cardinality_error_1_unscaled: 49.0000 (48.5778)  time: 0.1106  data: 0.0066  max mem: 594\n",
      "Test:  [3420/4410]  eta: 0:01:50  class_error: 100.00  loss: 8.3720 (7.4556)  loss_ce: 2.8989 (2.8550)  loss_bbox: 3.8911 (3.0183)  loss_giou: 1.5240 (1.5823)  loss_ce_unscaled: 1.4494 (1.4275)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7782 (0.6037)  loss_giou_unscaled: 0.7620 (0.7912)  cardinality_error_unscaled: 29.0000 (27.9164)  loss_ce_0_unscaled: 1.3214 (1.2757)  loss_bbox_0_unscaled: 0.7440 (0.5460)  loss_giou_0_unscaled: 0.7804 (0.7284)  cardinality_error_0_unscaled: 79.0000 (78.4516)  loss_ce_1_unscaled: 1.2457 (1.2417)  loss_bbox_1_unscaled: 0.7489 (0.5750)  loss_giou_1_unscaled: 0.7620 (0.7607)  cardinality_error_1_unscaled: 49.0000 (48.5791)  time: 0.1093  data: 0.0066  max mem: 594\n",
      "Test:  [3430/4410]  eta: 0:01:48  class_error: 100.00  loss: 8.2180 (7.4581)  loss_ce: 2.8389 (2.8549)  loss_bbox: 3.6686 (3.0200)  loss_giou: 1.7988 (1.5833)  loss_ce_unscaled: 1.4194 (1.4274)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7337 (0.6040)  loss_giou_unscaled: 0.8994 (0.7916)  cardinality_error_unscaled: 29.0000 (27.9196)  loss_ce_0_unscaled: 1.2443 (1.2756)  loss_bbox_0_unscaled: 0.6885 (0.5464)  loss_giou_0_unscaled: 0.8950 (0.7290)  cardinality_error_0_unscaled: 78.0000 (78.4494)  loss_ce_1_unscaled: 1.2234 (1.2416)  loss_bbox_1_unscaled: 0.6885 (0.5753)  loss_giou_1_unscaled: 0.9090 (0.7612)  cardinality_error_1_unscaled: 49.0000 (48.5803)  time: 0.1101  data: 0.0066  max mem: 594\n",
      "Test:  [3440/4410]  eta: 0:01:47  class_error: 100.00  loss: 8.2911 (7.4612)  loss_ce: 2.8052 (2.8547)  loss_bbox: 3.5592 (3.0221)  loss_giou: 1.9592 (1.5844)  loss_ce_unscaled: 1.4026 (1.4274)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7118 (0.6044)  loss_giou_unscaled: 0.9796 (0.7922)  cardinality_error_unscaled: 29.0000 (27.9227)  loss_ce_0_unscaled: 1.2277 (1.2754)  loss_bbox_0_unscaled: 0.6888 (0.5469)  loss_giou_0_unscaled: 0.9647 (0.7296)  cardinality_error_0_unscaled: 78.0000 (78.4484)  loss_ce_1_unscaled: 1.2114 (1.2416)  loss_bbox_1_unscaled: 0.6816 (0.5757)  loss_giou_1_unscaled: 0.9647 (0.7617)  cardinality_error_1_unscaled: 49.0000 (48.5815)  time: 0.1110  data: 0.0065  max mem: 594\n",
      "Test:  [3450/4410]  eta: 0:01:46  class_error: 100.00  loss: 8.3072 (7.4632)  loss_ce: 2.8435 (2.8548)  loss_bbox: 3.5512 (3.0233)  loss_giou: 1.9673 (1.5851)  loss_ce_unscaled: 1.4217 (1.4274)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7102 (0.6047)  loss_giou_unscaled: 0.9836 (0.7925)  cardinality_error_unscaled: 29.0000 (27.9255)  loss_ce_0_unscaled: 1.2352 (1.2754)  loss_bbox_0_unscaled: 0.6888 (0.5473)  loss_giou_0_unscaled: 0.8791 (0.7301)  cardinality_error_0_unscaled: 78.0000 (78.4462)  loss_ce_1_unscaled: 1.2232 (1.2416)  loss_bbox_1_unscaled: 0.6917 (0.5760)  loss_giou_1_unscaled: 0.8674 (0.7621)  cardinality_error_1_unscaled: 49.0000 (48.5824)  time: 0.1093  data: 0.0065  max mem: 594\n",
      "Test:  [3460/4410]  eta: 0:01:45  class_error: 100.00  loss: 7.8604 (7.4640)  loss_ce: 2.8241 (2.8546)  loss_bbox: 3.1427 (3.0225)  loss_giou: 2.0479 (1.5869)  loss_ce_unscaled: 1.4121 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6285 (0.6045)  loss_giou_unscaled: 1.0240 (0.7934)  cardinality_error_unscaled: 28.0000 (27.9191)  loss_ce_0_unscaled: 1.2629 (1.2754)  loss_bbox_0_unscaled: 0.4834 (0.5470)  loss_giou_0_unscaled: 0.8190 (0.7303)  cardinality_error_0_unscaled: 78.0000 (78.4447)  loss_ce_1_unscaled: 1.2224 (1.2415)  loss_bbox_1_unscaled: 0.5255 (0.5757)  loss_giou_1_unscaled: 0.8398 (0.7626)  cardinality_error_1_unscaled: 48.0000 (48.5808)  time: 0.1101  data: 0.0065  max mem: 594\n",
      "Test:  [3470/4410]  eta: 0:01:44  class_error: 100.00  loss: 7.6350 (7.4634)  loss_ce: 2.7870 (2.8544)  loss_bbox: 2.6681 (3.0206)  loss_giou: 2.1240 (1.5885)  loss_ce_unscaled: 1.3935 (1.4272)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5336 (0.6041)  loss_giou_unscaled: 1.0620 (0.7942)  cardinality_error_unscaled: 27.0000 (27.9141)  loss_ce_0_unscaled: 1.2669 (1.2754)  loss_bbox_0_unscaled: 0.4253 (0.5465)  loss_giou_0_unscaled: 0.8233 (0.7308)  cardinality_error_0_unscaled: 78.0000 (78.4425)  loss_ce_1_unscaled: 1.2150 (1.2414)  loss_bbox_1_unscaled: 0.4490 (0.5751)  loss_giou_1_unscaled: 0.9061 (0.7631)  cardinality_error_1_unscaled: 48.0000 (48.5791)  time: 0.1117  data: 0.0065  max mem: 594\n",
      "Test:  [3480/4410]  eta: 0:01:43  class_error: 100.00  loss: 7.2683 (7.4630)  loss_ce: 2.7785 (2.8542)  loss_bbox: 2.5254 (3.0190)  loss_giou: 2.0979 (1.5898)  loss_ce_unscaled: 1.3892 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5051 (0.6038)  loss_giou_unscaled: 1.0490 (0.7949)  cardinality_error_unscaled: 28.0000 (27.9138)  loss_ce_0_unscaled: 1.2707 (1.2753)  loss_bbox_0_unscaled: 0.4104 (0.5461)  loss_giou_0_unscaled: 0.8233 (0.7312)  cardinality_error_0_unscaled: 78.0000 (78.4410)  loss_ce_1_unscaled: 1.2073 (1.2412)  loss_bbox_1_unscaled: 0.3952 (0.5747)  loss_giou_1_unscaled: 0.9710 (0.7637)  cardinality_error_1_unscaled: 48.0000 (48.5777)  time: 0.1114  data: 0.0065  max mem: 594\n",
      "Test:  [3490/4410]  eta: 0:01:42  class_error: 100.00  loss: 7.3035 (7.4628)  loss_ce: 2.8172 (2.8541)  loss_bbox: 2.6891 (3.0187)  loss_giou: 1.7672 (1.5900)  loss_ce_unscaled: 1.4086 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5378 (0.6037)  loss_giou_unscaled: 0.8836 (0.7950)  cardinality_error_unscaled: 29.0000 (27.9169)  loss_ce_0_unscaled: 1.2736 (1.2753)  loss_bbox_0_unscaled: 0.4665 (0.5462)  loss_giou_0_unscaled: 0.8190 (0.7314)  cardinality_error_0_unscaled: 79.0000 (78.4426)  loss_ce_1_unscaled: 1.2125 (1.2412)  loss_bbox_1_unscaled: 0.4665 (0.5747)  loss_giou_1_unscaled: 0.8481 (0.7638)  cardinality_error_1_unscaled: 49.0000 (48.5789)  time: 0.1119  data: 0.0065  max mem: 594\n",
      "Test:  [3500/4410]  eta: 0:01:41  class_error: 100.00  loss: 7.4143 (7.4636)  loss_ce: 2.8249 (2.8541)  loss_bbox: 2.8136 (3.0190)  loss_giou: 1.7414 (1.5905)  loss_ce_unscaled: 1.4124 (1.4270)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5627 (0.6038)  loss_giou_unscaled: 0.8707 (0.7952)  cardinality_error_unscaled: 29.0000 (27.9197)  loss_ce_0_unscaled: 1.2831 (1.2754)  loss_bbox_0_unscaled: 0.5627 (0.5464)  loss_giou_0_unscaled: 0.8348 (0.7318)  cardinality_error_0_unscaled: 79.0000 (78.4433)  loss_ce_1_unscaled: 1.2323 (1.2412)  loss_bbox_1_unscaled: 0.5627 (0.5748)  loss_giou_1_unscaled: 0.8306 (0.7640)  cardinality_error_1_unscaled: 49.0000 (48.5801)  time: 0.1156  data: 0.0066  max mem: 594\n",
      "Test:  [3510/4410]  eta: 0:01:40  class_error: 100.00  loss: 7.6892 (7.4648)  loss_ce: 2.8319 (2.8541)  loss_bbox: 3.0696 (3.0198)  loss_giou: 1.7562 (1.5909)  loss_ce_unscaled: 1.4159 (1.4270)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6139 (0.6040)  loss_giou_unscaled: 0.8781 (0.7954)  cardinality_error_unscaled: 29.0000 (27.9222)  loss_ce_0_unscaled: 1.2959 (1.2755)  loss_bbox_0_unscaled: 0.5766 (0.5465)  loss_giou_0_unscaled: 0.8707 (0.7321)  cardinality_error_0_unscaled: 79.0000 (78.4437)  loss_ce_1_unscaled: 1.2348 (1.2412)  loss_bbox_1_unscaled: 0.5857 (0.5751)  loss_giou_1_unscaled: 0.8703 (0.7642)  cardinality_error_1_unscaled: 49.0000 (48.5813)  time: 0.1163  data: 0.0070  max mem: 594\n",
      "Test:  [3520/4410]  eta: 0:01:38  class_error: 100.00  loss: 8.8337 (7.4699)  loss_ce: 2.8751 (2.8542)  loss_bbox: 3.8729 (3.0237)  loss_giou: 1.8470 (1.5921)  loss_ce_unscaled: 1.4376 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7746 (0.6047)  loss_giou_unscaled: 0.9235 (0.7960)  cardinality_error_unscaled: 29.0000 (27.9253)  loss_ce_0_unscaled: 1.2761 (1.2754)  loss_bbox_0_unscaled: 0.7621 (0.5474)  loss_giou_0_unscaled: 0.9153 (0.7326)  cardinality_error_0_unscaled: 78.0000 (78.4425)  loss_ce_1_unscaled: 1.2167 (1.2411)  loss_bbox_1_unscaled: 0.7746 (0.5758)  loss_giou_1_unscaled: 0.9176 (0.7648)  cardinality_error_1_unscaled: 49.0000 (48.5825)  time: 0.1175  data: 0.0075  max mem: 594\n",
      "Test:  [3530/4410]  eta: 0:01:37  class_error: 100.00  loss: 9.2967 (7.4759)  loss_ce: 2.8666 (2.8541)  loss_bbox: 4.5734 (3.0284)  loss_giou: 2.0602 (1.5934)  loss_ce_unscaled: 1.4333 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.9147 (0.6057)  loss_giou_unscaled: 1.0301 (0.7967)  cardinality_error_unscaled: 29.0000 (27.9283)  loss_ce_0_unscaled: 1.2378 (1.2753)  loss_bbox_0_unscaled: 0.8871 (0.5484)  loss_giou_0_unscaled: 0.9220 (0.7332)  cardinality_error_0_unscaled: 78.0000 (78.4410)  loss_ce_1_unscaled: 1.2054 (1.2409)  loss_bbox_1_unscaled: 0.8851 (0.5768)  loss_giou_1_unscaled: 0.9989 (0.7655)  cardinality_error_1_unscaled: 49.0000 (48.5837)  time: 0.1190  data: 0.0073  max mem: 594\n",
      "Test:  [3540/4410]  eta: 0:01:36  class_error: 100.00  loss: 9.5416 (7.4812)  loss_ce: 2.8666 (2.8542)  loss_bbox: 4.6661 (3.0324)  loss_giou: 2.0602 (1.5946)  loss_ce_unscaled: 1.4333 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.9332 (0.6065)  loss_giou_unscaled: 1.0301 (0.7973)  cardinality_error_unscaled: 29.0000 (27.9311)  loss_ce_0_unscaled: 1.2336 (1.2752)  loss_bbox_0_unscaled: 0.8967 (0.5493)  loss_giou_0_unscaled: 0.9198 (0.7338)  cardinality_error_0_unscaled: 78.0000 (78.4389)  loss_ce_1_unscaled: 1.1930 (1.2408)  loss_bbox_1_unscaled: 0.8986 (0.5776)  loss_giou_1_unscaled: 1.0226 (0.7662)  cardinality_error_1_unscaled: 49.0000 (48.5846)  time: 0.1162  data: 0.0071  max mem: 594\n",
      "Test:  [3550/4410]  eta: 0:01:35  class_error: 100.00  loss: 8.8170 (7.4840)  loss_ce: 2.8854 (2.8543)  loss_bbox: 3.7119 (3.0336)  loss_giou: 2.0846 (1.5961)  loss_ce_unscaled: 1.4427 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7424 (0.6067)  loss_giou_unscaled: 1.0423 (0.7980)  cardinality_error_unscaled: 28.0000 (27.9313)  loss_ce_0_unscaled: 1.2358 (1.2751)  loss_bbox_0_unscaled: 0.7174 (0.5496)  loss_giou_0_unscaled: 1.0322 (0.7348)  cardinality_error_0_unscaled: 78.0000 (78.4357)  loss_ce_1_unscaled: 1.1975 (1.2407)  loss_bbox_1_unscaled: 0.6951 (0.5778)  loss_giou_1_unscaled: 1.0526 (0.7671)  cardinality_error_1_unscaled: 48.0000 (48.5829)  time: 0.1119  data: 0.0069  max mem: 594\n",
      "Test:  [3560/4410]  eta: 0:01:34  class_error: 100.00  loss: 8.5106 (7.4871)  loss_ce: 2.9036 (2.8544)  loss_bbox: 3.4449 (3.0350)  loss_giou: 2.1661 (1.5977)  loss_ce_unscaled: 1.4518 (1.4272)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6890 (0.6070)  loss_giou_unscaled: 1.0830 (0.7989)  cardinality_error_unscaled: 28.0000 (27.9315)  loss_ce_0_unscaled: 1.2346 (1.2750)  loss_bbox_0_unscaled: 0.6729 (0.5500)  loss_giou_0_unscaled: 1.0469 (0.7356)  cardinality_error_0_unscaled: 77.0000 (78.4325)  loss_ce_1_unscaled: 1.2025 (1.2406)  loss_bbox_1_unscaled: 0.6846 (0.5782)  loss_giou_1_unscaled: 1.0536 (0.7679)  cardinality_error_1_unscaled: 48.0000 (48.5813)  time: 0.1110  data: 0.0067  max mem: 594\n",
      "Test:  [3570/4410]  eta: 0:01:33  class_error: 100.00  loss: 8.5157 (7.4898)  loss_ce: 2.8825 (2.8545)  loss_bbox: 3.4515 (3.0364)  loss_giou: 2.1553 (1.5990)  loss_ce_unscaled: 1.4412 (1.4272)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6903 (0.6073)  loss_giou_unscaled: 1.0777 (0.7995)  cardinality_error_unscaled: 28.0000 (27.9320)  loss_ce_0_unscaled: 1.2376 (1.2749)  loss_bbox_0_unscaled: 0.6695 (0.5503)  loss_giou_0_unscaled: 1.0269 (0.7365)  cardinality_error_0_unscaled: 77.0000 (78.4307)  loss_ce_1_unscaled: 1.1999 (1.2405)  loss_bbox_1_unscaled: 0.6747 (0.5784)  loss_giou_1_unscaled: 1.0747 (0.7688)  cardinality_error_1_unscaled: 48.0000 (48.5799)  time: 0.1118  data: 0.0067  max mem: 594\n",
      "Test:  [3580/4410]  eta: 0:01:32  class_error: 100.00  loss: 8.2305 (7.4911)  loss_ce: 2.8786 (2.8546)  loss_bbox: 3.5403 (3.0376)  loss_giou: 1.7219 (1.5989)  loss_ce_unscaled: 1.4393 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7081 (0.6075)  loss_giou_unscaled: 0.8610 (0.7995)  cardinality_error_unscaled: 29.0000 (27.9349)  loss_ce_0_unscaled: 1.2533 (1.2750)  loss_bbox_0_unscaled: 0.6734 (0.5506)  loss_giou_0_unscaled: 0.8631 (0.7366)  cardinality_error_0_unscaled: 78.0000 (78.4317)  loss_ce_1_unscaled: 1.1970 (1.2405)  loss_bbox_1_unscaled: 0.6737 (0.5787)  loss_giou_1_unscaled: 0.8610 (0.7688)  cardinality_error_1_unscaled: 49.0000 (48.5811)  time: 0.1118  data: 0.0066  max mem: 594\n",
      "Test:  [3590/4410]  eta: 0:01:31  class_error: 100.00  loss: 7.9359 (7.4928)  loss_ce: 2.8786 (2.8546)  loss_bbox: 3.5403 (3.0392)  loss_giou: 1.5642 (1.5990)  loss_ce_unscaled: 1.4393 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7081 (0.6078)  loss_giou_unscaled: 0.7821 (0.7995)  cardinality_error_unscaled: 29.0000 (27.9379)  loss_ce_0_unscaled: 1.2829 (1.2750)  loss_bbox_0_unscaled: 0.6716 (0.5509)  loss_giou_0_unscaled: 0.7498 (0.7366)  cardinality_error_0_unscaled: 79.0000 (78.4319)  loss_ce_1_unscaled: 1.2450 (1.2405)  loss_bbox_1_unscaled: 0.6760 (0.5790)  loss_giou_1_unscaled: 0.7789 (0.7689)  cardinality_error_1_unscaled: 49.0000 (48.5823)  time: 0.1125  data: 0.0066  max mem: 594\n",
      "Test:  [3600/4410]  eta: 0:01:30  class_error: 100.00  loss: 7.8373 (7.4935)  loss_ce: 2.8592 (2.8546)  loss_bbox: 3.5125 (3.0402)  loss_giou: 1.5379 (1.5987)  loss_ce_unscaled: 1.4296 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7025 (0.6080)  loss_giou_unscaled: 0.7689 (0.7994)  cardinality_error_unscaled: 29.0000 (27.9408)  loss_ce_0_unscaled: 1.2832 (1.2750)  loss_bbox_0_unscaled: 0.6716 (0.5512)  loss_giou_0_unscaled: 0.7335 (0.7367)  cardinality_error_0_unscaled: 79.0000 (78.4329)  loss_ce_1_unscaled: 1.2450 (1.2405)  loss_bbox_1_unscaled: 0.6818 (0.5792)  loss_giou_1_unscaled: 0.7689 (0.7689)  cardinality_error_1_unscaled: 49.0000 (48.5834)  time: 0.1127  data: 0.0066  max mem: 594\n",
      "Test:  [3610/4410]  eta: 0:01:28  class_error: 100.00  loss: 7.7793 (7.4944)  loss_ce: 2.8588 (2.8546)  loss_bbox: 3.4576 (3.0411)  loss_giou: 1.5197 (1.5987)  loss_ce_unscaled: 1.4294 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6915 (0.6082)  loss_giou_unscaled: 0.7599 (0.7994)  cardinality_error_unscaled: 29.0000 (27.9435)  loss_ce_0_unscaled: 1.3084 (1.2751)  loss_bbox_0_unscaled: 0.6219 (0.5513)  loss_giou_0_unscaled: 0.7384 (0.7368)  cardinality_error_0_unscaled: 79.0000 (78.4334)  loss_ce_1_unscaled: 1.2437 (1.2405)  loss_bbox_1_unscaled: 0.6762 (0.5794)  loss_giou_1_unscaled: 0.7509 (0.7689)  cardinality_error_1_unscaled: 49.0000 (48.5846)  time: 0.1121  data: 0.0066  max mem: 594\n",
      "Test:  [3620/4410]  eta: 0:01:27  class_error: 100.00  loss: 7.5369 (7.4948)  loss_ce: 2.8623 (2.8546)  loss_bbox: 3.2408 (3.0417)  loss_giou: 1.5708 (1.5986)  loss_ce_unscaled: 1.4312 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6482 (0.6083)  loss_giou_unscaled: 0.7854 (0.7993)  cardinality_error_unscaled: 29.0000 (27.9464)  loss_ce_0_unscaled: 1.3175 (1.2753)  loss_bbox_0_unscaled: 0.6089 (0.5515)  loss_giou_0_unscaled: 0.7403 (0.7368)  cardinality_error_0_unscaled: 79.0000 (78.4347)  loss_ce_1_unscaled: 1.2538 (1.2405)  loss_bbox_1_unscaled: 0.6316 (0.5795)  loss_giou_1_unscaled: 0.7520 (0.7689)  cardinality_error_1_unscaled: 49.0000 (48.5857)  time: 0.1102  data: 0.0067  max mem: 594\n",
      "Test:  [3630/4410]  eta: 0:01:26  class_error: 100.00  loss: 7.5011 (7.4952)  loss_ce: 2.8715 (2.8546)  loss_bbox: 3.1994 (3.0422)  loss_giou: 1.5625 (1.5984)  loss_ce_unscaled: 1.4357 (1.4273)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6399 (0.6084)  loss_giou_unscaled: 0.7812 (0.7992)  cardinality_error_unscaled: 29.0000 (27.9446)  loss_ce_0_unscaled: 1.3270 (1.2754)  loss_bbox_0_unscaled: 0.6029 (0.5516)  loss_giou_0_unscaled: 0.7403 (0.7369)  cardinality_error_0_unscaled: 79.0000 (78.4362)  loss_ce_1_unscaled: 1.2591 (1.2405)  loss_bbox_1_unscaled: 0.6089 (0.5796)  loss_giou_1_unscaled: 0.7323 (0.7688)  cardinality_error_1_unscaled: 49.0000 (48.5869)  time: 0.1115  data: 0.0068  max mem: 594\n",
      "Test:  [3640/4410]  eta: 0:01:25  class_error: 100.00  loss: 7.9918 (7.4973)  loss_ce: 2.7962 (2.8542)  loss_bbox: 3.4671 (3.0442)  loss_giou: 1.5829 (1.5988)  loss_ce_unscaled: 1.3981 (1.4271)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6934 (0.6088)  loss_giou_unscaled: 0.7915 (0.7994)  cardinality_error_unscaled: 16.0000 (27.9072)  loss_ce_0_unscaled: 1.2216 (1.2752)  loss_bbox_0_unscaled: 0.6738 (0.5520)  loss_giou_0_unscaled: 0.7480 (0.7371)  cardinality_error_0_unscaled: 79.0000 (78.4378)  loss_ce_1_unscaled: 1.2444 (1.2405)  loss_bbox_1_unscaled: 0.6738 (0.5800)  loss_giou_1_unscaled: 0.7480 (0.7690)  cardinality_error_1_unscaled: 49.0000 (48.5880)  time: 0.1127  data: 0.0068  max mem: 594\n",
      "Test:  [3650/4410]  eta: 0:01:24  class_error: 100.00  loss: 8.2913 (7.4993)  loss_ce: 2.7419 (2.8539)  loss_bbox: 3.5845 (3.0461)  loss_giou: 1.7123 (1.5992)  loss_ce_unscaled: 1.3710 (1.4270)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7169 (0.6092)  loss_giou_unscaled: 0.8561 (0.7996)  cardinality_error_unscaled: 15.0000 (27.8732)  loss_ce_0_unscaled: 1.1795 (1.2749)  loss_bbox_0_unscaled: 0.6937 (0.5524)  loss_giou_0_unscaled: 0.8561 (0.7375)  cardinality_error_0_unscaled: 79.0000 (78.4393)  loss_ce_1_unscaled: 1.2344 (1.2405)  loss_bbox_1_unscaled: 0.7151 (0.5804)  loss_giou_1_unscaled: 0.8561 (0.7693)  cardinality_error_1_unscaled: 49.0000 (48.5892)  time: 0.1112  data: 0.0070  max mem: 594\n",
      "Test:  [3660/4410]  eta: 0:01:23  class_error: 100.00  loss: 8.4854 (7.5026)  loss_ce: 2.7328 (2.8536)  loss_bbox: 3.8623 (3.0491)  loss_giou: 1.8160 (1.6000)  loss_ce_unscaled: 1.3664 (1.4268)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7725 (0.6098)  loss_giou_unscaled: 0.9080 (0.8000)  cardinality_error_unscaled: 15.0000 (27.8377)  loss_ce_0_unscaled: 1.1706 (1.2747)  loss_bbox_0_unscaled: 0.6886 (0.5527)  loss_giou_0_unscaled: 0.8619 (0.7379)  cardinality_error_0_unscaled: 79.0000 (78.4409)  loss_ce_1_unscaled: 1.2386 (1.2406)  loss_bbox_1_unscaled: 0.7435 (0.5810)  loss_giou_1_unscaled: 0.8640 (0.7697)  cardinality_error_1_unscaled: 49.0000 (48.5903)  time: 0.1087  data: 0.0069  max mem: 594\n",
      "Test:  [3670/4410]  eta: 0:01:22  class_error: 100.00  loss: 8.8528 (7.5063)  loss_ce: 2.7044 (2.8531)  loss_bbox: 4.1970 (3.0520)  loss_giou: 1.9676 (1.6012)  loss_ce_unscaled: 1.3522 (1.4265)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8394 (0.6104)  loss_giou_unscaled: 0.9838 (0.8006)  cardinality_error_unscaled: 14.0000 (27.7987)  loss_ce_0_unscaled: 1.1846 (1.2744)  loss_bbox_0_unscaled: 0.6766 (0.5531)  loss_giou_0_unscaled: 0.8749 (0.7383)  cardinality_error_0_unscaled: 79.0000 (78.4424)  loss_ce_1_unscaled: 1.2386 (1.2405)  loss_bbox_1_unscaled: 0.7762 (0.5816)  loss_giou_1_unscaled: 0.9234 (0.7703)  cardinality_error_1_unscaled: 49.0000 (48.5914)  time: 0.1088  data: 0.0067  max mem: 594\n",
      "Test:  [3680/4410]  eta: 0:01:21  class_error: 100.00  loss: 9.1962 (7.5113)  loss_ce: 2.6811 (2.8526)  loss_bbox: 4.3959 (3.0561)  loss_giou: 2.0946 (1.6026)  loss_ce_unscaled: 1.3405 (1.4263)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8792 (0.6112)  loss_giou_unscaled: 1.0473 (0.8013)  cardinality_error_unscaled: 13.0000 (27.7569)  loss_ce_0_unscaled: 1.1752 (1.2742)  loss_bbox_0_unscaled: 0.7060 (0.5537)  loss_giou_0_unscaled: 0.8873 (0.7388)  cardinality_error_0_unscaled: 79.0000 (78.4439)  loss_ce_1_unscaled: 1.2301 (1.2405)  loss_bbox_1_unscaled: 0.8378 (0.5824)  loss_giou_1_unscaled: 0.8799 (0.7708)  cardinality_error_1_unscaled: 49.0000 (48.5925)  time: 0.1110  data: 0.0066  max mem: 594\n",
      "Test:  [3690/4410]  eta: 0:01:20  class_error: 100.00  loss: 9.0974 (7.5148)  loss_ce: 2.6861 (2.8522)  loss_bbox: 4.3545 (3.0588)  loss_giou: 2.0946 (1.6038)  loss_ce_unscaled: 1.3430 (1.4261)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8709 (0.6118)  loss_giou_unscaled: 1.0473 (0.8019)  cardinality_error_unscaled: 12.0000 (27.7131)  loss_ce_0_unscaled: 1.1742 (1.2739)  loss_bbox_0_unscaled: 0.7349 (0.5541)  loss_giou_0_unscaled: 0.8489 (0.7392)  cardinality_error_0_unscaled: 79.0000 (78.4454)  loss_ce_1_unscaled: 1.2299 (1.2405)  loss_bbox_1_unscaled: 0.8378 (0.5830)  loss_giou_1_unscaled: 0.8482 (0.7713)  cardinality_error_1_unscaled: 49.0000 (48.5936)  time: 0.1112  data: 0.0066  max mem: 594\n",
      "Test:  [3700/4410]  eta: 0:01:18  class_error: 100.00  loss: 8.7485 (7.5177)  loss_ce: 2.6869 (2.8518)  loss_bbox: 4.1263 (3.0616)  loss_giou: 1.7308 (1.6042)  loss_ce_unscaled: 1.3435 (1.4259)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8253 (0.6123)  loss_giou_unscaled: 0.8654 (0.8021)  cardinality_error_unscaled: 12.0000 (27.6785)  loss_ce_0_unscaled: 1.1839 (1.2737)  loss_bbox_0_unscaled: 0.7357 (0.5545)  loss_giou_0_unscaled: 0.8383 (0.7395)  cardinality_error_0_unscaled: 79.0000 (78.4466)  loss_ce_1_unscaled: 1.2280 (1.2405)  loss_bbox_1_unscaled: 0.8253 (0.5837)  loss_giou_1_unscaled: 0.8306 (0.7716)  cardinality_error_1_unscaled: 49.0000 (48.5947)  time: 0.1107  data: 0.0067  max mem: 594\n",
      "Test:  [3710/4410]  eta: 0:01:17  class_error: 100.00  loss: 8.7355 (7.5220)  loss_ce: 2.6869 (2.8514)  loss_bbox: 4.3842 (3.0659)  loss_giou: 1.6301 (1.6046)  loss_ce_unscaled: 1.3435 (1.4257)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8768 (0.6132)  loss_giou_unscaled: 0.8150 (0.8023)  cardinality_error_unscaled: 14.0000 (27.6424)  loss_ce_0_unscaled: 1.1801 (1.2735)  loss_bbox_0_unscaled: 0.7395 (0.5549)  loss_giou_0_unscaled: 0.8311 (0.7397)  cardinality_error_0_unscaled: 79.0000 (78.4481)  loss_ce_1_unscaled: 1.2330 (1.2404)  loss_bbox_1_unscaled: 0.8529 (0.5844)  loss_giou_1_unscaled: 0.8150 (0.7718)  cardinality_error_1_unscaled: 49.0000 (48.5958)  time: 0.1111  data: 0.0067  max mem: 594\n",
      "Test:  [3720/4410]  eta: 0:01:16  class_error: 100.00  loss: 8.8205 (7.5254)  loss_ce: 2.6952 (2.8510)  loss_bbox: 4.3581 (3.0691)  loss_giou: 1.6301 (1.6053)  loss_ce_unscaled: 1.3476 (1.4255)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8716 (0.6138)  loss_giou_unscaled: 0.8150 (0.8026)  cardinality_error_unscaled: 12.0000 (27.5980)  loss_ce_0_unscaled: 1.1756 (1.2732)  loss_bbox_0_unscaled: 0.7317 (0.5554)  loss_giou_0_unscaled: 0.8305 (0.7400)  cardinality_error_0_unscaled: 79.0000 (78.4496)  loss_ce_1_unscaled: 1.2330 (1.2404)  loss_bbox_1_unscaled: 0.8529 (0.5850)  loss_giou_1_unscaled: 0.8148 (0.7723)  cardinality_error_1_unscaled: 49.0000 (48.5969)  time: 0.1106  data: 0.0068  max mem: 594\n",
      "Test:  [3730/4410]  eta: 0:01:15  class_error: 100.00  loss: 8.6538 (7.5281)  loss_ce: 2.7087 (2.8506)  loss_bbox: 4.0339 (3.0712)  loss_giou: 1.9472 (1.6062)  loss_ce_unscaled: 1.3544 (1.4253)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8068 (0.6142)  loss_giou_unscaled: 0.9736 (0.8031)  cardinality_error_unscaled: 8.0000 (27.5436)  loss_ce_0_unscaled: 1.1776 (1.2730)  loss_bbox_0_unscaled: 0.7091 (0.5557)  loss_giou_0_unscaled: 0.8477 (0.7403)  cardinality_error_0_unscaled: 79.0000 (78.4511)  loss_ce_1_unscaled: 1.2352 (1.2404)  loss_bbox_1_unscaled: 0.7748 (0.5854)  loss_giou_1_unscaled: 0.9647 (0.7728)  cardinality_error_1_unscaled: 49.0000 (48.5980)  time: 0.1097  data: 0.0069  max mem: 594\n",
      "Test:  [3740/4410]  eta: 0:01:14  class_error: 100.00  loss: 8.5181 (7.5305)  loss_ce: 2.7132 (2.8503)  loss_bbox: 3.7678 (3.0730)  loss_giou: 1.9950 (1.6072)  loss_ce_unscaled: 1.3566 (1.4251)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7536 (0.6146)  loss_giou_unscaled: 0.9975 (0.8036)  cardinality_error_unscaled: 6.0000 (27.4910)  loss_ce_0_unscaled: 1.1886 (1.2728)  loss_bbox_0_unscaled: 0.7186 (0.5561)  loss_giou_0_unscaled: 0.8467 (0.7406)  cardinality_error_0_unscaled: 79.0000 (78.4526)  loss_ce_1_unscaled: 1.2390 (1.2404)  loss_bbox_1_unscaled: 0.7478 (0.5858)  loss_giou_1_unscaled: 0.9647 (0.7733)  cardinality_error_1_unscaled: 49.0000 (48.5990)  time: 0.1105  data: 0.0068  max mem: 594\n",
      "Test:  [3750/4410]  eta: 0:01:13  class_error: 100.00  loss: 8.3807 (7.5331)  loss_ce: 2.7265 (2.8500)  loss_bbox: 3.7678 (3.0750)  loss_giou: 2.0092 (1.6081)  loss_ce_unscaled: 1.3633 (1.4250)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7536 (0.6150)  loss_giou_unscaled: 1.0046 (0.8040)  cardinality_error_unscaled: 8.0000 (27.4503)  loss_ce_0_unscaled: 1.1819 (1.2725)  loss_bbox_0_unscaled: 0.7247 (0.5565)  loss_giou_0_unscaled: 0.8467 (0.7411)  cardinality_error_0_unscaled: 79.0000 (78.4537)  loss_ce_1_unscaled: 1.2400 (1.2404)  loss_bbox_1_unscaled: 0.7444 (0.5862)  loss_giou_1_unscaled: 0.9900 (0.7738)  cardinality_error_1_unscaled: 49.0000 (48.6001)  time: 0.1100  data: 0.0065  max mem: 594\n",
      "Test:  [3760/4410]  eta: 0:01:12  class_error: 100.00  loss: 8.3774 (7.5332)  loss_ce: 2.9165 (2.8504)  loss_bbox: 3.6286 (3.0754)  loss_giou: 1.6621 (1.6075)  loss_ce_unscaled: 1.4582 (1.4252)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7257 (0.6151)  loss_giou_unscaled: 0.8310 (0.8037)  cardinality_error_unscaled: 29.0000 (27.4544)  loss_ce_0_unscaled: 1.2538 (1.2726)  loss_bbox_0_unscaled: 0.5626 (0.5563)  loss_giou_0_unscaled: 0.7974 (0.7407)  cardinality_error_0_unscaled: 79.0000 (78.4552)  loss_ce_1_unscaled: 1.2337 (1.2404)  loss_bbox_1_unscaled: 0.6597 (0.5862)  loss_giou_1_unscaled: 0.8190 (0.7735)  cardinality_error_1_unscaled: 49.0000 (48.6012)  time: 0.1098  data: 0.0066  max mem: 594\n",
      "Test:  [3770/4410]  eta: 0:01:11  class_error: 100.00  loss: 7.7549 (7.5336)  loss_ce: 2.9820 (2.8506)  loss_bbox: 3.3967 (3.0758)  loss_giou: 1.3347 (1.6072)  loss_ce_unscaled: 1.4910 (1.4253)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6793 (0.6152)  loss_giou_unscaled: 0.6673 (0.8036)  cardinality_error_unscaled: 29.0000 (27.4585)  loss_ce_0_unscaled: 1.2783 (1.2726)  loss_bbox_0_unscaled: 0.4828 (0.5561)  loss_giou_0_unscaled: 0.5770 (0.7403)  cardinality_error_0_unscaled: 79.0000 (78.4566)  loss_ce_1_unscaled: 1.2336 (1.2404)  loss_bbox_1_unscaled: 0.5626 (0.5861)  loss_giou_1_unscaled: 0.6145 (0.7731)  cardinality_error_1_unscaled: 49.0000 (48.6022)  time: 0.1118  data: 0.0067  max mem: 594\n",
      "Test:  [3780/4410]  eta: 0:01:10  class_error: 100.00  loss: 7.7430 (7.5336)  loss_ce: 2.9716 (2.8510)  loss_bbox: 3.3184 (3.0762)  loss_giou: 1.2984 (1.6065)  loss_ce_unscaled: 1.4858 (1.4255)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6637 (0.6152)  loss_giou_unscaled: 0.6492 (0.8033)  cardinality_error_unscaled: 29.0000 (27.4623)  loss_ce_0_unscaled: 1.2781 (1.2726)  loss_bbox_0_unscaled: 0.5362 (0.5558)  loss_giou_0_unscaled: 0.6145 (0.7400)  cardinality_error_0_unscaled: 79.0000 (78.4581)  loss_ce_1_unscaled: 1.2213 (1.2404)  loss_bbox_1_unscaled: 0.5534 (0.5862)  loss_giou_1_unscaled: 0.6259 (0.7729)  cardinality_error_1_unscaled: 49.0000 (48.6033)  time: 0.1112  data: 0.0066  max mem: 594\n",
      "Test:  [3790/4410]  eta: 0:01:08  class_error: 100.00  loss: 7.7430 (7.5345)  loss_ce: 2.9150 (2.8511)  loss_bbox: 3.3384 (3.0773)  loss_giou: 1.3604 (1.6061)  loss_ce_unscaled: 1.4575 (1.4255)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6677 (0.6155)  loss_giou_unscaled: 0.6802 (0.8030)  cardinality_error_unscaled: 29.0000 (27.4664)  loss_ce_0_unscaled: 1.2858 (1.2727)  loss_bbox_0_unscaled: 0.5530 (0.5559)  loss_giou_0_unscaled: 0.6310 (0.7397)  cardinality_error_0_unscaled: 79.0000 (78.4592)  loss_ce_1_unscaled: 1.2466 (1.2404)  loss_bbox_1_unscaled: 0.6189 (0.5863)  loss_giou_1_unscaled: 0.6328 (0.7725)  cardinality_error_1_unscaled: 49.0000 (48.6043)  time: 0.1173  data: 0.0066  max mem: 594\n",
      "Test:  [3800/4410]  eta: 0:01:07  class_error: 100.00  loss: 8.0783 (7.5360)  loss_ce: 2.8918 (2.8512)  loss_bbox: 3.8092 (3.0792)  loss_giou: 1.4417 (1.6057)  loss_ce_unscaled: 1.4459 (1.4256)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7618 (0.6158)  loss_giou_unscaled: 0.7208 (0.8028)  cardinality_error_unscaled: 29.0000 (27.4704)  loss_ce_0_unscaled: 1.2729 (1.2726)  loss_bbox_0_unscaled: 0.5832 (0.5560)  loss_giou_0_unscaled: 0.6773 (0.7396)  cardinality_error_0_unscaled: 79.0000 (78.4604)  loss_ce_1_unscaled: 1.2449 (1.2404)  loss_bbox_1_unscaled: 0.6854 (0.5866)  loss_giou_1_unscaled: 0.6242 (0.7722)  cardinality_error_1_unscaled: 49.0000 (48.6054)  time: 0.1189  data: 0.0067  max mem: 594\n",
      "Test:  [3810/4410]  eta: 0:01:06  class_error: 100.00  loss: 8.0783 (7.5370)  loss_ce: 2.8832 (2.8513)  loss_bbox: 3.8853 (3.0803)  loss_giou: 1.3814 (1.6054)  loss_ce_unscaled: 1.4416 (1.4256)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7771 (0.6161)  loss_giou_unscaled: 0.6907 (0.8027)  cardinality_error_unscaled: 29.0000 (27.4739)  loss_ce_0_unscaled: 1.2592 (1.2726)  loss_bbox_0_unscaled: 0.5513 (0.5561)  loss_giou_0_unscaled: 0.6773 (0.7394)  cardinality_error_0_unscaled: 79.0000 (78.4616)  loss_ce_1_unscaled: 1.2449 (1.2404)  loss_bbox_1_unscaled: 0.6568 (0.5867)  loss_giou_1_unscaled: 0.6635 (0.7720)  cardinality_error_1_unscaled: 49.0000 (48.6061)  time: 0.1112  data: 0.0066  max mem: 594\n",
      "Test:  [3820/4410]  eta: 0:01:05  class_error: 100.00  loss: 8.5445 (7.5401)  loss_ce: 2.8113 (2.8511)  loss_bbox: 3.6311 (3.0821)  loss_giou: 1.9669 (1.6069)  loss_ce_unscaled: 1.4056 (1.4255)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7262 (0.6164)  loss_giou_unscaled: 0.9834 (0.8034)  cardinality_error_unscaled: 28.0000 (27.4747)  loss_ce_0_unscaled: 1.2469 (1.2725)  loss_bbox_0_unscaled: 0.5781 (0.5562)  loss_giou_0_unscaled: 0.8596 (0.7398)  cardinality_error_0_unscaled: 78.0000 (78.4604)  loss_ce_1_unscaled: 1.1810 (1.2402)  loss_bbox_1_unscaled: 0.6460 (0.5869)  loss_giou_1_unscaled: 0.9537 (0.7727)  cardinality_error_1_unscaled: 48.0000 (48.6046)  time: 0.1124  data: 0.0065  max mem: 594\n",
      "Test:  [3830/4410]  eta: 0:01:04  class_error: 100.00  loss: 8.6255 (7.5433)  loss_ce: 2.7676 (2.8508)  loss_bbox: 3.6661 (3.0842)  loss_giou: 2.1383 (1.6083)  loss_ce_unscaled: 1.3838 (1.4254)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7332 (0.6168)  loss_giou_unscaled: 1.0692 (0.8041)  cardinality_error_unscaled: 28.0000 (27.4759)  loss_ce_0_unscaled: 1.2245 (1.2724)  loss_bbox_0_unscaled: 0.6225 (0.5564)  loss_giou_0_unscaled: 0.8919 (0.7403)  cardinality_error_0_unscaled: 78.0000 (78.4591)  loss_ce_1_unscaled: 1.1371 (1.2400)  loss_bbox_1_unscaled: 0.6617 (0.5872)  loss_giou_1_unscaled: 1.0343 (0.7734)  cardinality_error_1_unscaled: 48.0000 (48.6030)  time: 0.1122  data: 0.0065  max mem: 594\n",
      "Test:  [3840/4410]  eta: 0:01:03  class_error: 100.00  loss: 8.8667 (7.5467)  loss_ce: 2.7573 (2.8506)  loss_bbox: 4.0183 (3.0869)  loss_giou: 2.0836 (1.6092)  loss_ce_unscaled: 1.3787 (1.4253)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8037 (0.6174)  loss_giou_unscaled: 1.0418 (0.8046)  cardinality_error_unscaled: 28.0000 (27.4754)  loss_ce_0_unscaled: 1.2310 (1.2723)  loss_bbox_0_unscaled: 0.6250 (0.5565)  loss_giou_0_unscaled: 0.8884 (0.7406)  cardinality_error_0_unscaled: 78.0000 (78.4580)  loss_ce_1_unscaled: 1.1883 (1.2399)  loss_bbox_1_unscaled: 0.6901 (0.5875)  loss_giou_1_unscaled: 1.0052 (0.7739)  cardinality_error_1_unscaled: 48.0000 (48.6017)  time: 0.1093  data: 0.0066  max mem: 594\n",
      "Test:  [3850/4410]  eta: 0:01:02  class_error: 100.00  loss: 7.4254 (7.5449)  loss_ce: 2.7421 (2.8503)  loss_bbox: 3.5147 (3.0869)  loss_giou: 1.1991 (1.6078)  loss_ce_unscaled: 1.3711 (1.4252)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7029 (0.6174)  loss_giou_unscaled: 0.5995 (0.8039)  cardinality_error_unscaled: 28.0000 (27.4770)  loss_ce_0_unscaled: 1.2310 (1.2722)  loss_bbox_0_unscaled: 0.6250 (0.5567)  loss_giou_0_unscaled: 0.6475 (0.7400)  cardinality_error_0_unscaled: 79.0000 (78.4594)  loss_ce_1_unscaled: 1.2169 (1.2398)  loss_bbox_1_unscaled: 0.6667 (0.5876)  loss_giou_1_unscaled: 0.6750 (0.7733)  cardinality_error_1_unscaled: 49.0000 (48.6027)  time: 0.1116  data: 0.0066  max mem: 594\n",
      "Test:  [3860/4410]  eta: 0:01:01  class_error: 100.00  loss: 6.9607 (7.5436)  loss_ce: 2.7154 (2.8500)  loss_bbox: 3.1917 (3.0871)  loss_giou: 1.0604 (1.6065)  loss_ce_unscaled: 1.3577 (1.4250)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6383 (0.6174)  loss_giou_unscaled: 0.5302 (0.8032)  cardinality_error_unscaled: 28.0000 (27.4786)  loss_ce_0_unscaled: 1.2221 (1.2720)  loss_bbox_0_unscaled: 0.6189 (0.5568)  loss_giou_0_unscaled: 0.5080 (0.7395)  cardinality_error_0_unscaled: 79.0000 (78.4608)  loss_ce_1_unscaled: 1.2242 (1.2398)  loss_bbox_1_unscaled: 0.6383 (0.5877)  loss_giou_1_unscaled: 0.5302 (0.7727)  cardinality_error_1_unscaled: 49.0000 (48.6037)  time: 0.1129  data: 0.0066  max mem: 594\n",
      "Test:  [3870/4410]  eta: 0:01:00  class_error: 100.00  loss: 6.9013 (7.5427)  loss_ce: 2.7246 (2.8497)  loss_bbox: 3.1917 (3.0876)  loss_giou: 1.1791 (1.6054)  loss_ce_unscaled: 1.3623 (1.4248)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6383 (0.6175)  loss_giou_unscaled: 0.5895 (0.8027)  cardinality_error_unscaled: 29.0000 (27.4808)  loss_ce_0_unscaled: 1.2240 (1.2719)  loss_bbox_0_unscaled: 0.5496 (0.5569)  loss_giou_0_unscaled: 0.6122 (0.7393)  cardinality_error_0_unscaled: 79.0000 (78.4622)  loss_ce_1_unscaled: 1.2241 (1.2398)  loss_bbox_1_unscaled: 0.6189 (0.5877)  loss_giou_1_unscaled: 0.5895 (0.7723)  cardinality_error_1_unscaled: 49.0000 (48.6048)  time: 0.1118  data: 0.0065  max mem: 594\n",
      "Test:  [3880/4410]  eta: 0:00:58  class_error: 100.00  loss: 7.0122 (7.5420)  loss_ce: 2.7276 (2.8494)  loss_bbox: 3.1403 (3.0878)  loss_giou: 1.3330 (1.6049)  loss_ce_unscaled: 1.3638 (1.4247)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6281 (0.6176)  loss_giou_unscaled: 0.6665 (0.8024)  cardinality_error_unscaled: 29.0000 (27.4823)  loss_ce_0_unscaled: 1.2243 (1.2718)  loss_bbox_0_unscaled: 0.5486 (0.5569)  loss_giou_0_unscaled: 0.6608 (0.7391)  cardinality_error_0_unscaled: 79.0000 (78.4635)  loss_ce_1_unscaled: 1.2186 (1.2397)  loss_bbox_1_unscaled: 0.5908 (0.5877)  loss_giou_1_unscaled: 0.6608 (0.7721)  cardinality_error_1_unscaled: 49.0000 (48.6058)  time: 0.1126  data: 0.0065  max mem: 594\n",
      "Test:  [3890/4410]  eta: 0:00:57  class_error: 100.00  loss: 7.4679 (7.5425)  loss_ce: 2.7270 (2.8491)  loss_bbox: 3.5445 (3.0891)  loss_giou: 1.4015 (1.6043)  loss_ce_unscaled: 1.3635 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7089 (0.6178)  loss_giou_unscaled: 0.7008 (0.8022)  cardinality_error_unscaled: 28.0000 (27.4816)  loss_ce_0_unscaled: 1.2243 (1.2718)  loss_bbox_0_unscaled: 0.6071 (0.5571)  loss_giou_0_unscaled: 0.6759 (0.7390)  cardinality_error_0_unscaled: 79.0000 (78.4649)  loss_ce_1_unscaled: 1.2172 (1.2396)  loss_bbox_1_unscaled: 0.6071 (0.5878)  loss_giou_1_unscaled: 0.6759 (0.7719)  cardinality_error_1_unscaled: 49.0000 (48.6068)  time: 0.1120  data: 0.0066  max mem: 594\n",
      "Test:  [3900/4410]  eta: 0:00:56  class_error: 100.00  loss: 7.7227 (7.5417)  loss_ce: 2.7321 (2.8488)  loss_bbox: 3.5852 (3.0892)  loss_giou: 1.4089 (1.6037)  loss_ce_unscaled: 1.3660 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7170 (0.6178)  loss_giou_unscaled: 0.7044 (0.8018)  cardinality_error_unscaled: 28.0000 (27.4835)  loss_ce_0_unscaled: 1.2281 (1.2717)  loss_bbox_0_unscaled: 0.6282 (0.5572)  loss_giou_0_unscaled: 0.6437 (0.7388)  cardinality_error_0_unscaled: 79.0000 (78.4660)  loss_ce_1_unscaled: 1.2175 (1.2396)  loss_bbox_1_unscaled: 0.6462 (0.5879)  loss_giou_1_unscaled: 0.6601 (0.7715)  cardinality_error_1_unscaled: 49.0000 (48.6078)  time: 0.1105  data: 0.0066  max mem: 594\n",
      "Test:  [3910/4410]  eta: 0:00:55  class_error: 100.00  loss: 7.8624 (7.5432)  loss_ce: 2.7463 (2.8486)  loss_bbox: 3.5558 (3.0903)  loss_giou: 1.4871 (1.6043)  loss_ce_unscaled: 1.3731 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7112 (0.6181)  loss_giou_unscaled: 0.7436 (0.8022)  cardinality_error_unscaled: 29.0000 (27.4873)  loss_ce_0_unscaled: 1.2376 (1.2716)  loss_bbox_0_unscaled: 0.6111 (0.5574)  loss_giou_0_unscaled: 0.7240 (0.7390)  cardinality_error_0_unscaled: 78.0000 (78.4651)  loss_ce_1_unscaled: 1.2057 (1.2395)  loss_bbox_1_unscaled: 0.6462 (0.5881)  loss_giou_1_unscaled: 0.7088 (0.7716)  cardinality_error_1_unscaled: 49.0000 (48.6088)  time: 0.1083  data: 0.0066  max mem: 594\n",
      "Test:  [3920/4410]  eta: 0:00:54  class_error: 100.00  loss: 7.8287 (7.5439)  loss_ce: 2.7735 (2.8484)  loss_bbox: 3.2916 (3.0912)  loss_giou: 1.8479 (1.6043)  loss_ce_unscaled: 1.3868 (1.4242)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6583 (0.6182)  loss_giou_unscaled: 0.9240 (0.8022)  cardinality_error_unscaled: 29.0000 (27.4912)  loss_ce_0_unscaled: 1.2556 (1.2716)  loss_bbox_0_unscaled: 0.6477 (0.5577)  loss_giou_0_unscaled: 0.8162 (0.7391)  cardinality_error_0_unscaled: 78.0000 (78.4631)  loss_ce_1_unscaled: 1.1997 (1.2394)  loss_bbox_1_unscaled: 0.6543 (0.5883)  loss_giou_1_unscaled: 0.8162 (0.7717)  cardinality_error_1_unscaled: 49.0000 (48.6098)  time: 0.1090  data: 0.0066  max mem: 594\n",
      "Test:  [3930/4410]  eta: 0:00:53  class_error: 100.00  loss: 7.8287 (7.5448)  loss_ce: 2.7690 (2.8482)  loss_bbox: 3.3927 (3.0920)  loss_giou: 1.5919 (1.6046)  loss_ce_unscaled: 1.3845 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6785 (0.6184)  loss_giou_unscaled: 0.7960 (0.8023)  cardinality_error_unscaled: 29.0000 (27.4950)  loss_ce_0_unscaled: 1.2648 (1.2716)  loss_bbox_0_unscaled: 0.6634 (0.5580)  loss_giou_0_unscaled: 0.7257 (0.7391)  cardinality_error_0_unscaled: 78.0000 (78.4612)  loss_ce_1_unscaled: 1.2021 (1.2393)  loss_bbox_1_unscaled: 0.6634 (0.5885)  loss_giou_1_unscaled: 0.7257 (0.7717)  cardinality_error_1_unscaled: 49.0000 (48.6108)  time: 0.1101  data: 0.0064  max mem: 594\n",
      "Test:  [3940/4410]  eta: 0:00:52  class_error: 100.00  loss: 8.3410 (7.5469)  loss_ce: 2.7649 (2.8480)  loss_bbox: 3.7234 (3.0938)  loss_giou: 1.7288 (1.6051)  loss_ce_unscaled: 1.3824 (1.4240)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7447 (0.6188)  loss_giou_unscaled: 0.8644 (0.8025)  cardinality_error_unscaled: 29.0000 (27.4981)  loss_ce_0_unscaled: 1.2272 (1.2714)  loss_bbox_0_unscaled: 0.7397 (0.5585)  loss_giou_0_unscaled: 0.8367 (0.7395)  cardinality_error_0_unscaled: 78.0000 (78.4603)  loss_ce_1_unscaled: 1.2128 (1.2392)  loss_bbox_1_unscaled: 0.7397 (0.5889)  loss_giou_1_unscaled: 0.8168 (0.7719)  cardinality_error_1_unscaled: 49.0000 (48.6118)  time: 0.1107  data: 0.0064  max mem: 594\n",
      "Test:  [3950/4410]  eta: 0:00:51  class_error: 100.00  loss: 8.3193 (7.5492)  loss_ce: 2.7500 (2.8477)  loss_bbox: 3.8097 (3.0957)  loss_giou: 1.8059 (1.6058)  loss_ce_unscaled: 1.3750 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7619 (0.6191)  loss_giou_unscaled: 0.9030 (0.8029)  cardinality_error_unscaled: 29.0000 (27.5011)  loss_ce_0_unscaled: 1.2017 (1.2712)  loss_bbox_0_unscaled: 0.7443 (0.5589)  loss_giou_0_unscaled: 0.8995 (0.7400)  cardinality_error_0_unscaled: 78.0000 (78.4591)  loss_ce_1_unscaled: 1.2126 (1.2392)  loss_bbox_1_unscaled: 0.7619 (0.5893)  loss_giou_1_unscaled: 0.8995 (0.7723)  cardinality_error_1_unscaled: 49.0000 (48.6128)  time: 0.1118  data: 0.0065  max mem: 594\n",
      "Test:  [3960/4410]  eta: 0:00:50  class_error: 100.00  loss: 8.1583 (7.5500)  loss_ce: 2.7529 (2.8475)  loss_bbox: 3.6405 (3.0966)  loss_giou: 1.7160 (1.6059)  loss_ce_unscaled: 1.3764 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7281 (0.6193)  loss_giou_unscaled: 0.8580 (0.8029)  cardinality_error_unscaled: 29.0000 (27.5042)  loss_ce_0_unscaled: 1.2080 (1.2711)  loss_bbox_0_unscaled: 0.7187 (0.5592)  loss_giou_0_unscaled: 0.8782 (0.7402)  cardinality_error_0_unscaled: 78.0000 (78.4580)  loss_ce_1_unscaled: 1.1945 (1.2391)  loss_bbox_1_unscaled: 0.7246 (0.5896)  loss_giou_1_unscaled: 0.8493 (0.7724)  cardinality_error_1_unscaled: 49.0000 (48.6137)  time: 0.1112  data: 0.0067  max mem: 594\n",
      "Test:  [3970/4410]  eta: 0:00:48  class_error: 100.00  loss: 6.5622 (7.5469)  loss_ce: 2.8575 (2.8476)  loss_bbox: 2.1407 (3.0937)  loss_giou: 1.4902 (1.6056)  loss_ce_unscaled: 1.4287 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4281 (0.6187)  loss_giou_unscaled: 0.7451 (0.8028)  cardinality_error_unscaled: 29.0000 (27.5074)  loss_ce_0_unscaled: 1.2483 (1.2711)  loss_bbox_0_unscaled: 0.4281 (0.5588)  loss_giou_0_unscaled: 0.7451 (0.7402)  cardinality_error_0_unscaled: 78.0000 (78.4563)  loss_ce_1_unscaled: 1.1948 (1.2390)  loss_bbox_1_unscaled: 0.4281 (0.5891)  loss_giou_1_unscaled: 0.7451 (0.7723)  cardinality_error_1_unscaled: 49.0000 (48.6147)  time: 0.1105  data: 0.0067  max mem: 594\n",
      "Test:  [3980/4410]  eta: 0:00:47  class_error: 100.00  loss: 6.2547 (7.5436)  loss_ce: 2.8914 (2.8477)  loss_bbox: 1.8999 (3.0906)  loss_giou: 1.4612 (1.6052)  loss_ce_unscaled: 1.4457 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3800 (0.6181)  loss_giou_unscaled: 0.7306 (0.8026)  cardinality_error_unscaled: 29.0000 (27.5112)  loss_ce_0_unscaled: 1.2657 (1.2711)  loss_bbox_0_unscaled: 0.3800 (0.5583)  loss_giou_0_unscaled: 0.7306 (0.7402)  cardinality_error_0_unscaled: 78.0000 (78.4539)  loss_ce_1_unscaled: 1.1979 (1.2389)  loss_bbox_1_unscaled: 0.3800 (0.5885)  loss_giou_1_unscaled: 0.7306 (0.7723)  cardinality_error_1_unscaled: 49.0000 (48.6157)  time: 0.1086  data: 0.0066  max mem: 594\n",
      "Test:  [3990/4410]  eta: 0:00:46  class_error: 100.00  loss: 6.2428 (7.5409)  loss_ce: 2.8816 (2.8478)  loss_bbox: 1.8872 (3.0880)  loss_giou: 1.4671 (1.6051)  loss_ce_unscaled: 1.4408 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3774 (0.6176)  loss_giou_unscaled: 0.7336 (0.8025)  cardinality_error_unscaled: 29.0000 (27.5149)  loss_ce_0_unscaled: 1.2713 (1.2711)  loss_bbox_0_unscaled: 0.3744 (0.5579)  loss_giou_0_unscaled: 0.7305 (0.7403)  cardinality_error_0_unscaled: 78.0000 (78.4515)  loss_ce_1_unscaled: 1.1979 (1.2388)  loss_bbox_1_unscaled: 0.3744 (0.5881)  loss_giou_1_unscaled: 0.7305 (0.7722)  cardinality_error_1_unscaled: 49.0000 (48.6166)  time: 0.1101  data: 0.0066  max mem: 594\n",
      "Test:  [4000/4410]  eta: 0:00:45  class_error: 100.00  loss: 8.1149 (7.5428)  loss_ce: 2.8484 (2.8477)  loss_bbox: 3.1328 (3.0888)  loss_giou: 2.0116 (1.6063)  loss_ce_unscaled: 1.4242 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6266 (0.6178)  loss_giou_unscaled: 1.0058 (0.8031)  cardinality_error_unscaled: 29.0000 (27.5186)  loss_ce_0_unscaled: 1.2744 (1.2712)  loss_bbox_0_unscaled: 0.6120 (0.5582)  loss_giou_0_unscaled: 0.9999 (0.7410)  cardinality_error_0_unscaled: 78.0000 (78.4496)  loss_ce_1_unscaled: 1.2111 (1.2388)  loss_bbox_1_unscaled: 0.6120 (0.5883)  loss_giou_1_unscaled: 0.9999 (0.7729)  cardinality_error_1_unscaled: 49.0000 (48.6176)  time: 0.1120  data: 0.0066  max mem: 594\n",
      "Test:  [4010/4410]  eta: 0:00:44  class_error: 100.00  loss: 8.3715 (7.5451)  loss_ce: 2.8270 (2.8477)  loss_bbox: 3.4932 (3.0900)  loss_giou: 2.0749 (1.6074)  loss_ce_unscaled: 1.4135 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6986 (0.6180)  loss_giou_unscaled: 1.0375 (0.8037)  cardinality_error_unscaled: 29.0000 (27.5223)  loss_ce_0_unscaled: 1.2738 (1.2711)  loss_bbox_0_unscaled: 0.6546 (0.5585)  loss_giou_0_unscaled: 1.0387 (0.7418)  cardinality_error_0_unscaled: 78.0000 (78.4473)  loss_ce_1_unscaled: 1.2117 (1.2387)  loss_bbox_1_unscaled: 0.6708 (0.5885)  loss_giou_1_unscaled: 1.0378 (0.7735)  cardinality_error_1_unscaled: 49.0000 (48.6185)  time: 0.1116  data: 0.0067  max mem: 594\n",
      "Test:  [4020/4410]  eta: 0:00:43  class_error: 100.00  loss: 8.3605 (7.5469)  loss_ce: 2.8196 (2.8476)  loss_bbox: 3.4987 (3.0907)  loss_giou: 2.0596 (1.6085)  loss_ce_unscaled: 1.4098 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6997 (0.6181)  loss_giou_unscaled: 1.0298 (0.8043)  cardinality_error_unscaled: 29.0000 (27.5260)  loss_ce_0_unscaled: 1.2643 (1.2711)  loss_bbox_0_unscaled: 0.6708 (0.5588)  loss_giou_0_unscaled: 1.0409 (0.7425)  cardinality_error_0_unscaled: 77.0000 (78.4444)  loss_ce_1_unscaled: 1.1998 (1.2386)  loss_bbox_1_unscaled: 0.6914 (0.5887)  loss_giou_1_unscaled: 1.0351 (0.7742)  cardinality_error_1_unscaled: 49.0000 (48.6195)  time: 0.1127  data: 0.0068  max mem: 594\n",
      "Test:  [4030/4410]  eta: 0:00:42  class_error: 100.00  loss: 8.3222 (7.5488)  loss_ce: 2.8172 (2.8476)  loss_bbox: 3.4169 (3.0915)  loss_giou: 2.0659 (1.6097)  loss_ce_unscaled: 1.4086 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6834 (0.6183)  loss_giou_unscaled: 1.0329 (0.8048)  cardinality_error_unscaled: 29.0000 (27.5296)  loss_ce_0_unscaled: 1.2698 (1.2711)  loss_bbox_0_unscaled: 0.6599 (0.5590)  loss_giou_0_unscaled: 1.0436 (0.7432)  cardinality_error_0_unscaled: 77.0000 (78.4423)  loss_ce_1_unscaled: 1.2029 (1.2385)  loss_bbox_1_unscaled: 0.6739 (0.5889)  loss_giou_1_unscaled: 1.0329 (0.7748)  cardinality_error_1_unscaled: 49.0000 (48.6204)  time: 0.1114  data: 0.0068  max mem: 594\n",
      "Test:  [4040/4410]  eta: 0:00:41  class_error: 100.00  loss: 8.2102 (7.5502)  loss_ce: 2.8245 (2.8475)  loss_bbox: 3.2908 (3.0919)  loss_giou: 2.0659 (1.6108)  loss_ce_unscaled: 1.4122 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6582 (0.6184)  loss_giou_unscaled: 1.0329 (0.8054)  cardinality_error_unscaled: 29.0000 (27.5333)  loss_ce_0_unscaled: 1.2738 (1.2711)  loss_bbox_0_unscaled: 0.6499 (0.5592)  loss_giou_0_unscaled: 1.0329 (0.7439)  cardinality_error_0_unscaled: 78.0000 (78.4405)  loss_ce_1_unscaled: 1.2113 (1.2385)  loss_bbox_1_unscaled: 0.6500 (0.5890)  loss_giou_1_unscaled: 1.0329 (0.7755)  cardinality_error_1_unscaled: 49.0000 (48.6214)  time: 0.1097  data: 0.0068  max mem: 594\n",
      "Test:  [4050/4410]  eta: 0:00:40  class_error: 100.00  loss: 8.1664 (7.5516)  loss_ce: 2.8267 (2.8475)  loss_bbox: 3.2617 (3.0923)  loss_giou: 2.0510 (1.6118)  loss_ce_unscaled: 1.4134 (1.4237)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6523 (0.6185)  loss_giou_unscaled: 1.0255 (0.8059)  cardinality_error_unscaled: 29.0000 (27.5364)  loss_ce_0_unscaled: 1.2698 (1.2711)  loss_bbox_0_unscaled: 0.6523 (0.5594)  loss_giou_0_unscaled: 1.0408 (0.7446)  cardinality_error_0_unscaled: 77.0000 (78.4382)  loss_ce_1_unscaled: 1.2065 (1.2384)  loss_bbox_1_unscaled: 0.6523 (0.5892)  loss_giou_1_unscaled: 1.0237 (0.7760)  cardinality_error_1_unscaled: 49.0000 (48.6223)  time: 0.1100  data: 0.0065  max mem: 594\n",
      "Test:  [4060/4410]  eta: 0:00:38  class_error: 100.00  loss: 7.6947 (7.5509)  loss_ce: 2.8646 (2.8476)  loss_bbox: 3.0407 (3.0916)  loss_giou: 1.6339 (1.6117)  loss_ce_unscaled: 1.4323 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6081 (0.6183)  loss_giou_unscaled: 0.8169 (0.8058)  cardinality_error_unscaled: 28.0000 (27.5353)  loss_ce_0_unscaled: 1.2744 (1.2711)  loss_bbox_0_unscaled: 0.6081 (0.5594)  loss_giou_0_unscaled: 0.8169 (0.7447)  cardinality_error_0_unscaled: 79.0000 (78.4395)  loss_ce_1_unscaled: 1.2297 (1.2384)  loss_bbox_1_unscaled: 0.6081 (0.5891)  loss_giou_1_unscaled: 0.8169 (0.7760)  cardinality_error_1_unscaled: 49.0000 (48.6232)  time: 0.1110  data: 0.0064  max mem: 594\n",
      "Test:  [4070/4410]  eta: 0:00:37  class_error: 100.00  loss: 7.2069 (7.5503)  loss_ce: 2.9005 (2.8478)  loss_bbox: 2.7732 (3.0909)  loss_giou: 1.5755 (1.6117)  loss_ce_unscaled: 1.4502 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5546 (0.6182)  loss_giou_unscaled: 0.7878 (0.8058)  cardinality_error_unscaled: 27.0000 (27.5345)  loss_ce_0_unscaled: 1.2803 (1.2711)  loss_bbox_0_unscaled: 0.5546 (0.5594)  loss_giou_0_unscaled: 0.7875 (0.7448)  cardinality_error_0_unscaled: 79.0000 (78.4409)  loss_ce_1_unscaled: 1.2410 (1.2384)  loss_bbox_1_unscaled: 0.5546 (0.5891)  loss_giou_1_unscaled: 0.7875 (0.7761)  cardinality_error_1_unscaled: 49.0000 (48.6242)  time: 0.1108  data: 0.0067  max mem: 594\n",
      "Test:  [4080/4410]  eta: 0:00:36  class_error: 100.00  loss: 7.3184 (7.5497)  loss_ce: 2.9001 (2.8479)  loss_bbox: 2.8296 (3.0903)  loss_giou: 1.5789 (1.6116)  loss_ce_unscaled: 1.4501 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5659 (0.6181)  loss_giou_unscaled: 0.7895 (0.8058)  cardinality_error_unscaled: 27.0000 (27.5347)  loss_ce_0_unscaled: 1.2817 (1.2712)  loss_bbox_0_unscaled: 0.5659 (0.5594)  loss_giou_0_unscaled: 0.7875 (0.7449)  cardinality_error_0_unscaled: 79.0000 (78.4423)  loss_ce_1_unscaled: 1.2359 (1.2384)  loss_bbox_1_unscaled: 0.5659 (0.5890)  loss_giou_1_unscaled: 0.7875 (0.7761)  cardinality_error_1_unscaled: 49.0000 (48.6251)  time: 0.1096  data: 0.0068  max mem: 594\n",
      "Test:  [4090/4410]  eta: 0:00:35  class_error: 100.00  loss: 7.1703 (7.5482)  loss_ce: 2.8921 (2.8480)  loss_bbox: 2.7362 (3.0890)  loss_giou: 1.5425 (1.6112)  loss_ce_unscaled: 1.4461 (1.4240)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5472 (0.6178)  loss_giou_unscaled: 0.7712 (0.8056)  cardinality_error_unscaled: 28.0000 (27.5378)  loss_ce_0_unscaled: 1.2895 (1.2713)  loss_bbox_0_unscaled: 0.5472 (0.5593)  loss_giou_0_unscaled: 0.7712 (0.7449)  cardinality_error_0_unscaled: 79.0000 (78.4432)  loss_ce_1_unscaled: 1.2415 (1.2384)  loss_bbox_1_unscaled: 0.5472 (0.5888)  loss_giou_1_unscaled: 0.7712 (0.7760)  cardinality_error_1_unscaled: 49.0000 (48.6260)  time: 0.1095  data: 0.0068  max mem: 594\n",
      "Test:  [4100/4410]  eta: 0:00:34  class_error: 100.00  loss: 6.9434 (7.5463)  loss_ce: 2.9135 (2.8482)  loss_bbox: 2.4888 (3.0872)  loss_giou: 1.4721 (1.6109)  loss_ce_unscaled: 1.4568 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4978 (0.6174)  loss_giou_unscaled: 0.7360 (0.8055)  cardinality_error_unscaled: 28.0000 (27.5389)  loss_ce_0_unscaled: 1.3032 (1.2713)  loss_bbox_0_unscaled: 0.4918 (0.5591)  loss_giou_0_unscaled: 0.7360 (0.7449)  cardinality_error_0_unscaled: 79.0000 (78.4445)  loss_ce_1_unscaled: 1.2539 (1.2385)  loss_bbox_1_unscaled: 0.4918 (0.5885)  loss_giou_1_unscaled: 0.7360 (0.7759)  cardinality_error_1_unscaled: 49.0000 (48.6269)  time: 0.1099  data: 0.0069  max mem: 594\n",
      "Test:  [4110/4410]  eta: 0:00:33  class_error: 100.00  loss: 6.9074 (7.5446)  loss_ce: 2.9247 (2.8484)  loss_bbox: 2.5118 (3.0856)  loss_giou: 1.4867 (1.6107)  loss_ce_unscaled: 1.4623 (1.4242)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5024 (0.6171)  loss_giou_unscaled: 0.7434 (0.8053)  cardinality_error_unscaled: 28.0000 (27.5398)  loss_ce_0_unscaled: 1.2986 (1.2714)  loss_bbox_0_unscaled: 0.5045 (0.5589)  loss_giou_0_unscaled: 0.7653 (0.7449)  cardinality_error_0_unscaled: 79.0000 (78.4456)  loss_ce_1_unscaled: 1.2551 (1.2385)  loss_bbox_1_unscaled: 0.5024 (0.5883)  loss_giou_1_unscaled: 0.7434 (0.7758)  cardinality_error_1_unscaled: 49.0000 (48.6278)  time: 0.1103  data: 0.0067  max mem: 594\n",
      "Test:  [4120/4410]  eta: 0:00:32  class_error: 100.00  loss: 6.7582 (7.5429)  loss_ce: 2.9098 (2.8485)  loss_bbox: 2.3471 (3.0840)  loss_giou: 1.4967 (1.6104)  loss_ce_unscaled: 1.4549 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4694 (0.6168)  loss_giou_unscaled: 0.7483 (0.8052)  cardinality_error_unscaled: 29.0000 (27.5426)  loss_ce_0_unscaled: 1.2916 (1.2714)  loss_bbox_0_unscaled: 0.4663 (0.5587)  loss_giou_0_unscaled: 0.7653 (0.7449)  cardinality_error_0_unscaled: 79.0000 (78.4465)  loss_ce_1_unscaled: 1.2510 (1.2385)  loss_bbox_1_unscaled: 0.4663 (0.5880)  loss_giou_1_unscaled: 0.7580 (0.7758)  cardinality_error_1_unscaled: 49.0000 (48.6287)  time: 0.1094  data: 0.0065  max mem: 594\n",
      "Test:  [4130/4410]  eta: 0:00:31  class_error: 100.00  loss: 6.7513 (7.5411)  loss_ce: 2.9002 (2.8487)  loss_bbox: 2.3313 (3.0822)  loss_giou: 1.5085 (1.6102)  loss_ce_unscaled: 1.4501 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4663 (0.6164)  loss_giou_unscaled: 0.7543 (0.8051)  cardinality_error_unscaled: 29.0000 (27.5456)  loss_ce_0_unscaled: 1.2818 (1.2715)  loss_bbox_0_unscaled: 0.4643 (0.5585)  loss_giou_0_unscaled: 0.7771 (0.7451)  cardinality_error_0_unscaled: 79.0000 (78.4478)  loss_ce_1_unscaled: 1.2384 (1.2385)  loss_bbox_1_unscaled: 0.4643 (0.5877)  loss_giou_1_unscaled: 0.7670 (0.7758)  cardinality_error_1_unscaled: 49.0000 (48.6296)  time: 0.1104  data: 0.0067  max mem: 594\n",
      "Test:  [4140/4410]  eta: 0:00:30  class_error: 100.00  loss: 6.7280 (7.5390)  loss_ce: 2.8975 (2.8488)  loss_bbox: 2.2601 (3.0801)  loss_giou: 1.5272 (1.6101)  loss_ce_unscaled: 1.4488 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4520 (0.6160)  loss_giou_unscaled: 0.7636 (0.8051)  cardinality_error_unscaled: 29.0000 (27.5472)  loss_ce_0_unscaled: 1.2813 (1.2715)  loss_bbox_0_unscaled: 0.4478 (0.5582)  loss_giou_0_unscaled: 0.7915 (0.7452)  cardinality_error_0_unscaled: 79.0000 (78.4484)  loss_ce_1_unscaled: 1.2375 (1.2385)  loss_bbox_1_unscaled: 0.4520 (0.5874)  loss_giou_1_unscaled: 0.7648 (0.7758)  cardinality_error_1_unscaled: 49.0000 (48.6305)  time: 0.1089  data: 0.0070  max mem: 594\n",
      "Test:  [4150/4410]  eta: 0:00:28  class_error: 100.00  loss: 7.0453 (7.5383)  loss_ce: 2.8818 (2.8488)  loss_bbox: 2.4689 (3.0790)  loss_giou: 1.6818 (1.6105)  loss_ce_unscaled: 1.4409 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4938 (0.6158)  loss_giou_unscaled: 0.8409 (0.8052)  cardinality_error_unscaled: 27.0000 (27.5437)  loss_ce_0_unscaled: 1.2685 (1.2715)  loss_bbox_0_unscaled: 0.4938 (0.5581)  loss_giou_0_unscaled: 0.8474 (0.7455)  cardinality_error_0_unscaled: 79.0000 (78.4498)  loss_ce_1_unscaled: 1.2379 (1.2385)  loss_bbox_1_unscaled: 0.4938 (0.5872)  loss_giou_1_unscaled: 0.8412 (0.7761)  cardinality_error_1_unscaled: 49.0000 (48.6314)  time: 0.1074  data: 0.0071  max mem: 594\n",
      "Test:  [4160/4410]  eta: 0:00:27  class_error: 100.00  loss: 7.2059 (7.5375)  loss_ce: 2.8616 (2.8489)  loss_bbox: 2.6458 (3.0781)  loss_giou: 1.6948 (1.6106)  loss_ce_unscaled: 1.4308 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5292 (0.6156)  loss_giou_unscaled: 0.8474 (0.8053)  cardinality_error_unscaled: 26.0000 (27.5400)  loss_ce_0_unscaled: 1.2494 (1.2714)  loss_bbox_0_unscaled: 0.5292 (0.5580)  loss_giou_0_unscaled: 0.8523 (0.7457)  cardinality_error_0_unscaled: 79.0000 (78.4506)  loss_ce_1_unscaled: 1.2379 (1.2385)  loss_bbox_1_unscaled: 0.5247 (0.5871)  loss_giou_1_unscaled: 0.8614 (0.7762)  cardinality_error_1_unscaled: 49.0000 (48.6323)  time: 0.1083  data: 0.0069  max mem: 594\n",
      "Test:  [4170/4410]  eta: 0:00:26  class_error: 100.00  loss: 7.1522 (7.5365)  loss_ce: 2.8655 (2.8490)  loss_bbox: 2.5945 (3.0767)  loss_giou: 1.6853 (1.6108)  loss_ce_unscaled: 1.4328 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5189 (0.6153)  loss_giou_unscaled: 0.8427 (0.8054)  cardinality_error_unscaled: 26.0000 (27.5380)  loss_ce_0_unscaled: 1.2518 (1.2714)  loss_bbox_0_unscaled: 0.5189 (0.5579)  loss_giou_0_unscaled: 0.8427 (0.7460)  cardinality_error_0_unscaled: 79.0000 (78.4519)  loss_ce_1_unscaled: 1.2360 (1.2385)  loss_bbox_1_unscaled: 0.5182 (0.5869)  loss_giou_1_unscaled: 0.8429 (0.7764)  cardinality_error_1_unscaled: 49.0000 (48.6332)  time: 0.1088  data: 0.0067  max mem: 594\n",
      "Test:  [4180/4410]  eta: 0:00:25  class_error: 100.00  loss: 6.6391 (7.5341)  loss_ce: 2.8675 (2.8490)  loss_bbox: 2.2143 (3.0743)  loss_giou: 1.6521 (1.6108)  loss_ce_unscaled: 1.4337 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4429 (0.6149)  loss_giou_unscaled: 0.8261 (0.8054)  cardinality_error_unscaled: 28.0000 (27.5398)  loss_ce_0_unscaled: 1.2644 (1.2714)  loss_bbox_0_unscaled: 0.4429 (0.5575)  loss_giou_0_unscaled: 0.8223 (0.7461)  cardinality_error_0_unscaled: 79.0000 (78.4523)  loss_ce_1_unscaled: 1.2349 (1.2385)  loss_bbox_1_unscaled: 0.4429 (0.5864)  loss_giou_1_unscaled: 0.8223 (0.7764)  cardinality_error_1_unscaled: 49.0000 (48.6341)  time: 0.1101  data: 0.0067  max mem: 594\n",
      "Test:  [4190/4410]  eta: 0:00:24  class_error: 100.00  loss: 6.4825 (7.5318)  loss_ce: 2.8618 (2.8490)  loss_bbox: 2.0330 (3.0719)  loss_giou: 1.6009 (1.6109)  loss_ce_unscaled: 1.4309 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4066 (0.6144)  loss_giou_unscaled: 0.8005 (0.8054)  cardinality_error_unscaled: 28.0000 (27.5421)  loss_ce_0_unscaled: 1.2763 (1.2714)  loss_bbox_0_unscaled: 0.4060 (0.5572)  loss_giou_0_unscaled: 0.7952 (0.7462)  cardinality_error_0_unscaled: 79.0000 (78.4529)  loss_ce_1_unscaled: 1.2327 (1.2385)  loss_bbox_1_unscaled: 0.4060 (0.5860)  loss_giou_1_unscaled: 0.7952 (0.7765)  cardinality_error_1_unscaled: 49.0000 (48.6349)  time: 0.1102  data: 0.0067  max mem: 594\n",
      "Test:  [4200/4410]  eta: 0:00:23  class_error: 100.00  loss: 6.5560 (7.5296)  loss_ce: 2.8658 (2.8491)  loss_bbox: 2.0831 (3.0697)  loss_giou: 1.5741 (1.6108)  loss_ce_unscaled: 1.4329 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4166 (0.6139)  loss_giou_unscaled: 0.7871 (0.8054)  cardinality_error_unscaled: 29.0000 (27.5439)  loss_ce_0_unscaled: 1.2651 (1.2715)  loss_bbox_0_unscaled: 0.4066 (0.5568)  loss_giou_0_unscaled: 0.7871 (0.7464)  cardinality_error_0_unscaled: 79.0000 (78.4527)  loss_ce_1_unscaled: 1.2273 (1.2384)  loss_bbox_1_unscaled: 0.4066 (0.5856)  loss_giou_1_unscaled: 0.7871 (0.7766)  cardinality_error_1_unscaled: 49.0000 (48.6353)  time: 0.1086  data: 0.0067  max mem: 594\n",
      "Test:  [4210/4410]  eta: 0:00:22  class_error: 100.00  loss: 7.4286 (7.5305)  loss_ce: 2.8695 (2.8491)  loss_bbox: 2.7109 (3.0696)  loss_giou: 1.7231 (1.6118)  loss_ce_unscaled: 1.4347 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5422 (0.6139)  loss_giou_unscaled: 0.8616 (0.8059)  cardinality_error_unscaled: 27.0000 (27.5388)  loss_ce_0_unscaled: 1.2611 (1.2714)  loss_bbox_0_unscaled: 0.4941 (0.5568)  loss_giou_0_unscaled: 0.8505 (0.7468)  cardinality_error_0_unscaled: 77.0000 (78.4476)  loss_ce_1_unscaled: 1.2280 (1.2384)  loss_bbox_1_unscaled: 0.5401 (0.5856)  loss_giou_1_unscaled: 0.8490 (0.7769)  cardinality_error_1_unscaled: 47.0000 (48.6314)  time: 0.1087  data: 0.0066  max mem: 594\n",
      "Test:  [4220/4410]  eta: 0:00:21  class_error: 100.00  loss: 7.6774 (7.5309)  loss_ce: 2.8616 (2.8491)  loss_bbox: 2.9131 (3.0692)  loss_giou: 1.8649 (1.6126)  loss_ce_unscaled: 1.4308 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5826 (0.6138)  loss_giou_unscaled: 0.9324 (0.8063)  cardinality_error_unscaled: 26.0000 (27.5352)  loss_ce_0_unscaled: 1.2561 (1.2714)  loss_bbox_0_unscaled: 0.5219 (0.5567)  loss_giou_0_unscaled: 0.9217 (0.7472)  cardinality_error_0_unscaled: 76.0000 (78.4397)  loss_ce_1_unscaled: 1.2211 (1.2384)  loss_bbox_1_unscaled: 0.5761 (0.5856)  loss_giou_1_unscaled: 0.9205 (0.7772)  cardinality_error_1_unscaled: 47.0000 (48.6276)  time: 0.1100  data: 0.0066  max mem: 594\n",
      "Test:  [4230/4410]  eta: 0:00:20  class_error: 100.00  loss: 7.6375 (7.5313)  loss_ce: 2.8450 (2.8492)  loss_bbox: 2.8476 (3.0690)  loss_giou: 1.8657 (1.6132)  loss_ce_unscaled: 1.4225 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5695 (0.6138)  loss_giou_unscaled: 0.9329 (0.8066)  cardinality_error_unscaled: 26.0000 (27.5304)  loss_ce_0_unscaled: 1.2561 (1.2714)  loss_bbox_0_unscaled: 0.5221 (0.5567)  loss_giou_0_unscaled: 0.9217 (0.7476)  cardinality_error_0_unscaled: 76.0000 (78.4349)  loss_ce_1_unscaled: 1.2211 (1.2383)  loss_bbox_1_unscaled: 0.5677 (0.5855)  loss_giou_1_unscaled: 0.9303 (0.7776)  cardinality_error_1_unscaled: 47.0000 (48.6242)  time: 0.1102  data: 0.0065  max mem: 594\n",
      "Test:  [4240/4410]  eta: 0:00:18  class_error: 100.00  loss: 7.3291 (7.5298)  loss_ce: 2.8587 (2.8492)  loss_bbox: 2.7425 (3.0680)  loss_giou: 1.4974 (1.6126)  loss_ce_unscaled: 1.4293 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5485 (0.6136)  loss_giou_unscaled: 0.7487 (0.8063)  cardinality_error_unscaled: 27.0000 (27.5327)  loss_ce_0_unscaled: 1.2552 (1.2713)  loss_bbox_0_unscaled: 0.5247 (0.5566)  loss_giou_0_unscaled: 0.7487 (0.7475)  cardinality_error_0_unscaled: 79.0000 (78.4362)  loss_ce_1_unscaled: 1.2234 (1.2383)  loss_bbox_1_unscaled: 0.5433 (0.5854)  loss_giou_1_unscaled: 0.7487 (0.7774)  cardinality_error_1_unscaled: 49.0000 (48.6251)  time: 0.1106  data: 0.0065  max mem: 594\n",
      "Test:  [4250/4410]  eta: 0:00:17  class_error: 100.00  loss: 6.7844 (7.5286)  loss_ce: 2.8587 (2.8492)  loss_bbox: 2.6840 (3.0672)  loss_giou: 1.4170 (1.6122)  loss_ce_unscaled: 1.4294 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5368 (0.6134)  loss_giou_unscaled: 0.7085 (0.8061)  cardinality_error_unscaled: 29.0000 (27.5340)  loss_ce_0_unscaled: 1.2542 (1.2713)  loss_bbox_0_unscaled: 0.5368 (0.5566)  loss_giou_0_unscaled: 0.7085 (0.7474)  cardinality_error_0_unscaled: 79.0000 (78.4375)  loss_ce_1_unscaled: 1.2351 (1.2383)  loss_bbox_1_unscaled: 0.5368 (0.5853)  loss_giou_1_unscaled: 0.7085 (0.7773)  cardinality_error_1_unscaled: 49.0000 (48.6260)  time: 0.1103  data: 0.0066  max mem: 594\n",
      "Test:  [4260/4410]  eta: 0:00:16  class_error: 100.00  loss: 6.7863 (7.5267)  loss_ce: 2.8479 (2.8492)  loss_bbox: 2.5659 (3.0659)  loss_giou: 1.3791 (1.6115)  loss_ce_unscaled: 1.4240 (1.4246)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5132 (0.6132)  loss_giou_unscaled: 0.6895 (0.8058)  cardinality_error_unscaled: 28.0000 (27.5351)  loss_ce_0_unscaled: 1.2555 (1.2713)  loss_bbox_0_unscaled: 0.5132 (0.5564)  loss_giou_0_unscaled: 0.6895 (0.7472)  cardinality_error_0_unscaled: 79.0000 (78.4386)  loss_ce_1_unscaled: 1.2371 (1.2383)  loss_bbox_1_unscaled: 0.5132 (0.5851)  loss_giou_1_unscaled: 0.6895 (0.7770)  cardinality_error_1_unscaled: 49.0000 (48.6266)  time: 0.1093  data: 0.0066  max mem: 594\n",
      "Test:  [4270/4410]  eta: 0:00:15  class_error: 100.00  loss: 5.8854 (7.5220)  loss_ce: 2.8333 (2.8491)  loss_bbox: 1.7036 (3.0623)  loss_giou: 1.2656 (1.6107)  loss_ce_unscaled: 1.4167 (1.4245)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3407 (0.6125)  loss_giou_unscaled: 0.6328 (0.8054)  cardinality_error_unscaled: 28.0000 (27.5362)  loss_ce_0_unscaled: 1.2600 (1.2713)  loss_bbox_0_unscaled: 0.3189 (0.5557)  loss_giou_0_unscaled: 0.6172 (0.7468)  cardinality_error_0_unscaled: 78.0000 (78.4364)  loss_ce_1_unscaled: 1.2167 (1.2382)  loss_bbox_1_unscaled: 0.3189 (0.5843)  loss_giou_1_unscaled: 0.6221 (0.7766)  cardinality_error_1_unscaled: 48.0000 (48.6251)  time: 0.1098  data: 0.0067  max mem: 594\n",
      "Test:  [4280/4410]  eta: 0:00:14  class_error: 100.00  loss: 5.4060 (7.5168)  loss_ce: 2.7512 (2.8488)  loss_bbox: 1.4287 (3.0583)  loss_giou: 1.2455 (1.6097)  loss_ce_unscaled: 1.3756 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.2857 (0.6117)  loss_giou_unscaled: 0.6228 (0.8049)  cardinality_error_unscaled: 28.0000 (27.5366)  loss_ce_0_unscaled: 1.2667 (1.2713)  loss_bbox_0_unscaled: 0.2660 (0.5550)  loss_giou_0_unscaled: 0.5778 (0.7465)  cardinality_error_0_unscaled: 77.0000 (78.4331)  loss_ce_1_unscaled: 1.1676 (1.2380)  loss_bbox_1_unscaled: 0.2660 (0.5836)  loss_giou_1_unscaled: 0.5956 (0.7761)  cardinality_error_1_unscaled: 48.0000 (48.6237)  time: 0.1111  data: 0.0068  max mem: 594\n",
      "Test:  [4290/4410]  eta: 0:00:13  class_error: 100.00  loss: 5.3087 (7.5120)  loss_ce: 2.7512 (2.8486)  loss_bbox: 1.3300 (3.0545)  loss_giou: 1.2152 (1.6089)  loss_ce_unscaled: 1.3756 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.2660 (0.6109)  loss_giou_unscaled: 0.6076 (0.8045)  cardinality_error_unscaled: 28.0000 (27.5376)  loss_ce_0_unscaled: 1.2712 (1.2713)  loss_bbox_0_unscaled: 0.2567 (0.5544)  loss_giou_0_unscaled: 0.6057 (0.7462)  cardinality_error_0_unscaled: 77.0000 (78.4297)  loss_ce_1_unscaled: 1.1689 (1.2379)  loss_bbox_1_unscaled: 0.2567 (0.5829)  loss_giou_1_unscaled: 0.6063 (0.7758)  cardinality_error_1_unscaled: 48.0000 (48.6225)  time: 0.1100  data: 0.0067  max mem: 594\n",
      "Test:  [4300/4410]  eta: 0:00:12  class_error: 100.00  loss: 7.1750 (7.5119)  loss_ce: 2.8118 (2.8486)  loss_bbox: 2.3724 (3.0538)  loss_giou: 1.7857 (1.6095)  loss_ce_unscaled: 1.4059 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4745 (0.6108)  loss_giou_unscaled: 0.8928 (0.8048)  cardinality_error_unscaled: 28.0000 (27.5387)  loss_ce_0_unscaled: 1.2712 (1.2713)  loss_bbox_0_unscaled: 0.4745 (0.5544)  loss_giou_0_unscaled: 0.8928 (0.7466)  cardinality_error_0_unscaled: 78.0000 (78.4306)  loss_ce_1_unscaled: 1.2143 (1.2379)  loss_bbox_1_unscaled: 0.4745 (0.5828)  loss_giou_1_unscaled: 0.8928 (0.7762)  cardinality_error_1_unscaled: 49.0000 (48.6233)  time: 0.1080  data: 0.0066  max mem: 594\n",
      "Test:  [4310/4410]  eta: 0:00:11  class_error: 100.00  loss: 7.5032 (7.5120)  loss_ce: 2.8378 (2.8486)  loss_bbox: 2.8154 (3.0533)  loss_giou: 1.8581 (1.6101)  loss_ce_unscaled: 1.4189 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5631 (0.6107)  loss_giou_unscaled: 0.9290 (0.8051)  cardinality_error_unscaled: 28.0000 (27.5398)  loss_ce_0_unscaled: 1.2612 (1.2712)  loss_bbox_0_unscaled: 0.5631 (0.5544)  loss_giou_0_unscaled: 0.9257 (0.7471)  cardinality_error_0_unscaled: 79.0000 (78.4312)  loss_ce_1_unscaled: 1.2433 (1.2379)  loss_bbox_1_unscaled: 0.5624 (0.5827)  loss_giou_1_unscaled: 0.9290 (0.7765)  cardinality_error_1_unscaled: 49.0000 (48.6242)  time: 0.1095  data: 0.0067  max mem: 594\n",
      "Test:  [4320/4410]  eta: 0:00:10  class_error: 100.00  loss: 7.5204 (7.5122)  loss_ce: 2.8372 (2.8485)  loss_bbox: 2.8155 (3.0531)  loss_giou: 1.8376 (1.6106)  loss_ce_unscaled: 1.4186 (1.4243)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5631 (0.6106)  loss_giou_unscaled: 0.9188 (0.8053)  cardinality_error_unscaled: 29.0000 (27.5383)  loss_ce_0_unscaled: 1.2665 (1.2712)  loss_bbox_0_unscaled: 0.5631 (0.5545)  loss_giou_0_unscaled: 0.9195 (0.7474)  cardinality_error_0_unscaled: 79.0000 (78.4305)  loss_ce_1_unscaled: 1.2324 (1.2379)  loss_bbox_1_unscaled: 0.5631 (0.5828)  loss_giou_1_unscaled: 0.9195 (0.7768)  cardinality_error_1_unscaled: 49.0000 (48.6251)  time: 0.1109  data: 0.0067  max mem: 594\n",
      "Test:  [4330/4410]  eta: 0:00:08  class_error: 100.00  loss: 7.8328 (7.5137)  loss_ce: 2.7206 (2.8482)  loss_bbox: 3.3997 (3.0548)  loss_giou: 1.7607 (1.6108)  loss_ce_unscaled: 1.3603 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6799 (0.6110)  loss_giou_unscaled: 0.8804 (0.8054)  cardinality_error_unscaled: 15.0000 (27.4950)  loss_ce_0_unscaled: 1.2159 (1.2711)  loss_bbox_0_unscaled: 0.6379 (0.5549)  loss_giou_0_unscaled: 0.8804 (0.7477)  cardinality_error_0_unscaled: 79.0000 (78.4318)  loss_ce_1_unscaled: 1.2540 (1.2379)  loss_bbox_1_unscaled: 0.6379 (0.5831)  loss_giou_1_unscaled: 0.8804 (0.7770)  cardinality_error_1_unscaled: 49.0000 (48.6260)  time: 0.1106  data: 0.0066  max mem: 594\n",
      "Test:  [4340/4410]  eta: 0:00:07  class_error: 100.00  loss: 8.2453 (7.5161)  loss_ce: 2.6892 (2.8478)  loss_bbox: 3.7725 (3.0570)  loss_giou: 1.6120 (1.6113)  loss_ce_unscaled: 1.3446 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7545 (0.6114)  loss_giou_unscaled: 0.8060 (0.8056)  cardinality_error_unscaled: 6.0000 (27.4506)  loss_ce_0_unscaled: 1.1984 (1.2709)  loss_bbox_0_unscaled: 0.7227 (0.5553)  loss_giou_0_unscaled: 0.8564 (0.7480)  cardinality_error_0_unscaled: 79.0000 (78.4331)  loss_ce_1_unscaled: 1.2599 (1.2380)  loss_bbox_1_unscaled: 0.7243 (0.5835)  loss_giou_1_unscaled: 0.8356 (0.7773)  cardinality_error_1_unscaled: 49.0000 (48.6268)  time: 0.1093  data: 0.0065  max mem: 594\n",
      "Test:  [4350/4410]  eta: 0:00:06  class_error: 100.00  loss: 8.7302 (7.5190)  loss_ce: 2.6822 (2.8475)  loss_bbox: 4.0681 (3.0596)  loss_giou: 1.9438 (1.6119)  loss_ce_unscaled: 1.3411 (1.4237)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8136 (0.6119)  loss_giou_unscaled: 0.9719 (0.8059)  cardinality_error_unscaled: 9.0000 (27.4192)  loss_ce_0_unscaled: 1.1967 (1.2708)  loss_bbox_0_unscaled: 0.7419 (0.5556)  loss_giou_0_unscaled: 0.8356 (0.7482)  cardinality_error_0_unscaled: 79.0000 (78.4344)  loss_ce_1_unscaled: 1.2599 (1.2380)  loss_bbox_1_unscaled: 0.7426 (0.5839)  loss_giou_1_unscaled: 0.8169 (0.7774)  cardinality_error_1_unscaled: 49.0000 (48.6277)  time: 0.1082  data: 0.0066  max mem: 594\n",
      "Test:  [4360/4410]  eta: 0:00:05  class_error: 100.00  loss: 7.0885 (7.5169)  loss_ce: 2.8335 (2.8476)  loss_bbox: 2.8737 (3.0581)  loss_giou: 1.4615 (1.6112)  loss_ce_unscaled: 1.4167 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5747 (0.6116)  loss_giou_unscaled: 0.7307 (0.8056)  cardinality_error_unscaled: 29.0000 (27.4228)  loss_ce_0_unscaled: 1.2362 (1.2708)  loss_bbox_0_unscaled: 0.5510 (0.5554)  loss_giou_0_unscaled: 0.7054 (0.7480)  cardinality_error_0_unscaled: 79.0000 (78.4357)  loss_ce_1_unscaled: 1.2689 (1.2381)  loss_bbox_1_unscaled: 0.5490 (0.5836)  loss_giou_1_unscaled: 0.7370 (0.7772)  cardinality_error_1_unscaled: 49.0000 (48.6285)  time: 0.1098  data: 0.0067  max mem: 594\n",
      "Test:  [4370/4410]  eta: 0:00:04  class_error: 100.00  loss: 6.5869 (7.5151)  loss_ce: 2.9034 (2.8477)  loss_bbox: 2.3946 (3.0568)  loss_giou: 1.3037 (1.6106)  loss_ce_unscaled: 1.4517 (1.4238)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4789 (0.6114)  loss_giou_unscaled: 0.6519 (0.8053)  cardinality_error_unscaled: 29.0000 (27.4264)  loss_ce_0_unscaled: 1.2739 (1.2708)  loss_bbox_0_unscaled: 0.4789 (0.5553)  loss_giou_0_unscaled: 0.6381 (0.7477)  cardinality_error_0_unscaled: 79.0000 (78.4370)  loss_ce_1_unscaled: 1.2925 (1.2382)  loss_bbox_1_unscaled: 0.4789 (0.5834)  loss_giou_1_unscaled: 0.6490 (0.7769)  cardinality_error_1_unscaled: 49.0000 (48.6294)  time: 0.1096  data: 0.0065  max mem: 594\n",
      "Test:  [4380/4410]  eta: 0:00:03  class_error: 100.00  loss: 6.7667 (7.5136)  loss_ce: 2.9192 (2.8479)  loss_bbox: 2.4790 (3.0557)  loss_giou: 1.3518 (1.6100)  loss_ce_unscaled: 1.4596 (1.4239)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4958 (0.6111)  loss_giou_unscaled: 0.6759 (0.8050)  cardinality_error_unscaled: 29.0000 (27.4296)  loss_ce_0_unscaled: 1.2778 (1.2709)  loss_bbox_0_unscaled: 0.4792 (0.5551)  loss_giou_0_unscaled: 0.6235 (0.7475)  cardinality_error_0_unscaled: 79.0000 (78.4378)  loss_ce_1_unscaled: 1.2902 (1.2383)  loss_bbox_1_unscaled: 0.4958 (0.5832)  loss_giou_1_unscaled: 0.6438 (0.7767)  cardinality_error_1_unscaled: 49.0000 (48.6298)  time: 0.1093  data: 0.0066  max mem: 594\n",
      "Test:  [4390/4410]  eta: 0:00:02  class_error: 100.00  loss: 6.7991 (7.5118)  loss_ce: 2.9640 (2.8482)  loss_bbox: 2.1176 (3.0533)  loss_giou: 1.6231 (1.6104)  loss_ce_unscaled: 1.4820 (1.4241)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4235 (0.6107)  loss_giou_unscaled: 0.8115 (0.8052)  cardinality_error_unscaled: 27.0000 (27.4286)  loss_ce_0_unscaled: 1.2947 (1.2709)  loss_bbox_0_unscaled: 0.4282 (0.5548)  loss_giou_0_unscaled: 0.8115 (0.7478)  cardinality_error_0_unscaled: 77.0000 (78.4345)  loss_ce_1_unscaled: 1.2632 (1.2384)  loss_bbox_1_unscaled: 0.4235 (0.5828)  loss_giou_1_unscaled: 0.8115 (0.7769)  cardinality_error_1_unscaled: 47.0000 (48.6261)  time: 0.1102  data: 0.0065  max mem: 594\n",
      "Test:  [4400/4410]  eta: 0:00:01  class_error: 100.00  loss: 6.5431 (7.5096)  loss_ce: 2.9775 (2.8485)  loss_bbox: 1.9704 (3.0507)  loss_giou: 1.6520 (1.6104)  loss_ce_unscaled: 1.4888 (1.4242)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3941 (0.6101)  loss_giou_unscaled: 0.8260 (0.8052)  cardinality_error_unscaled: 27.0000 (27.4276)  loss_ce_0_unscaled: 1.2969 (1.2710)  loss_bbox_0_unscaled: 0.3941 (0.5544)  loss_giou_0_unscaled: 0.8260 (0.7480)  cardinality_error_0_unscaled: 77.0000 (78.4313)  loss_ce_1_unscaled: 1.2410 (1.2384)  loss_bbox_1_unscaled: 0.3941 (0.5823)  loss_giou_1_unscaled: 0.8260 (0.7770)  cardinality_error_1_unscaled: 47.0000 (48.6224)  time: 0.1091  data: 0.0065  max mem: 594\n",
      "Test:  [4409/4410]  eta: 0:00:00  class_error: 100.00  loss: 6.5218 (7.5078)  loss_ce: 2.9702 (2.8487)  loss_bbox: 1.8993 (3.0485)  loss_giou: 1.6355 (1.6106)  loss_ce_unscaled: 1.4851 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3799 (0.6097)  loss_giou_unscaled: 0.8178 (0.8053)  cardinality_error_unscaled: 27.0000 (27.4268)  loss_ce_0_unscaled: 1.2978 (1.2711)  loss_bbox_0_unscaled: 0.3799 (0.5540)  loss_giou_0_unscaled: 0.8178 (0.7481)  cardinality_error_0_unscaled: 77.0000 (78.4283)  loss_ce_1_unscaled: 1.2386 (1.2384)  loss_bbox_1_unscaled: 0.3799 (0.5820)  loss_giou_1_unscaled: 0.8178 (0.7772)  cardinality_error_1_unscaled: 47.0000 (48.6190)  time: 0.1085  data: 0.0064  max mem: 594\n",
      "Test: Total time: 0:08:10 (0.1112 s / it)\n",
      "Averaged stats: class_error: 100.00  loss: 6.5218 (7.5078)  loss_ce: 2.9702 (2.8487)  loss_bbox: 1.8993 (3.0485)  loss_giou: 1.6355 (1.6106)  loss_ce_unscaled: 1.4851 (1.4244)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3799 (0.6097)  loss_giou_unscaled: 0.8178 (0.8053)  cardinality_error_unscaled: 27.0000 (27.4268)  loss_ce_0_unscaled: 1.2978 (1.2711)  loss_bbox_0_unscaled: 0.3799 (0.5540)  loss_giou_0_unscaled: 0.8178 (0.7481)  cardinality_error_0_unscaled: 77.0000 (78.4283)  loss_ce_1_unscaled: 1.2386 (1.2384)  loss_bbox_1_unscaled: 0.3799 (0.5820)  loss_giou_1_unscaled: 0.8178 (0.7772)  cardinality_error_1_unscaled: 47.0000 (48.6190)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.92s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "Job Finished, eval cocoevaluator\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if args[\"output_dir\"]:\n",
    "    Path(args[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(args[\"dataset_file\"], 11111111)\n",
    "print(args[\"dataset_file\"])\n",
    "device = torch.device(args[\"device\"])\n",
    "#utils.init_distributed_mode(args) #probably remove (need for multi gpus)\n",
    "#print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "\n",
    "if args[\"frozen_weights\"] is not None:\n",
    "    assert args[\"masks\"], \"Frozen training is meant for segmentation only\"\n",
    "\n",
    "\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args[\"seed\"] #+ utils.get_rank() #no need to get rank as it is a single gpu\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train_vid', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args[\"batch_size\"], drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                            collate_fn=collate_fn, num_workers=args[\"num_workers\"],\n",
    "                            pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, args[\"batch_size\"], sampler=sampler_val,\n",
    "                            drop_last=False, collate_fn=collate_fn, num_workers=args[\"num_workers\"],\n",
    "                            pin_memory=True)\n",
    "\n",
    "# lr_backbone_names = [\"backbone.0\", \"backbone.neck\", \"input_proj\", \"transformer.encoder\"]\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n",
    "\n",
    "for n, p in model_without_ddp.named_parameters():\n",
    "    print(n)\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "            if not match_name_keywords(n, args[\"lr_backbone_names\"]) and not match_name_keywords(n, args[\"lr_linear_proj_names\"]) and p.requires_grad],\n",
    "        \"lr\": args[\"lr\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args[\"lr_backbone_names\"]) and p.requires_grad],\n",
    "        \"lr\": args[\"lr_backbone\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args[\"lr_linear_proj_names\"]) and p.requires_grad],\n",
    "        \"lr\": args[\"lr\"] * args[\"lr_linear_proj_mult\"],\n",
    "    }\n",
    "]\n",
    "if args[\"sgd\"]:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args[\"lr\"], momentum=0.9,\n",
    "                                weight_decay=args[\"weight_decay\"])\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args[\"lr\"],\n",
    "                                weight_decay=args[\"weight_decay\"])\n",
    "print(args[\"lr_drop_epochs\"])\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, args[\"lr_drop_epochs\"])\n",
    "\n",
    "base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args[\"frozen_weights\"] is not None:\n",
    "    checkpoint = torch.load(args[\"frozen_weights\"], map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args[\"output_dir\"])\n",
    "\n",
    "#IF WE WANT TO RESUME THE TRAINING\n",
    "if args[\"resume\"]:\n",
    "    if args[\"resume\"].startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args[\"resume\"], map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args[\"resume\"], map_location='cpu')\n",
    "\n",
    "    if args[\"eval\"]:\n",
    "        missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    else:\n",
    "        tmp_dict = model_without_ddp.state_dict().copy()\n",
    "        if args[\"coco_pretrain\"]: # singleBaseline\n",
    "            for k, v in checkpoint['model'].items():\n",
    "                if ('class_embed' not in k) :\n",
    "                    tmp_dict[k] = v \n",
    "                else:\n",
    "                    print('k', k)\n",
    "        \n",
    "        else:\n",
    "            tmp_dict = checkpoint['model']\n",
    "            for name, param in model_without_ddp.named_parameters():\n",
    "\n",
    "                if ('temp' in name):\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "        missing_keys, unexpected_keys = model_without_ddp.load_state_dict(tmp_dict, strict=False)\n",
    "        \n",
    "    unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "    if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "\n",
    "#If we want to evaluate a model\n",
    "if args[\"eval\"]:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                        data_loader_val, base_ds, device, args[\"output_dir\"])\n",
    "\n",
    "    if args[\"output_dir\"]:\n",
    "        save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    print(\"Job Finished, eval cocoevaluator\")\n",
    "    exit()\n",
    "\n",
    "print(\"Model ready to start Training !\")\n",
    "#Train the model and save checkpoint and final\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args[\"start_epoch\"], args[\"epochs\"]):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch, args[\"clip_max_norm\"])\n",
    "    lr_scheduler.step()\n",
    "    print('output_dir', args[\"output_dir\"])\n",
    "    if args[\"output_dir\"]:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 5 epochs\n",
    "        # if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 1 == 0:\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    #test_stats, coco_evaluator = evaluate(\n",
    "    #   model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "    #)\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                'epoch': epoch,\n",
    "                'n_parameters': n_parameters}\n",
    "\n",
    "    if args[\"output_dir\"]:\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'optimizer', 'lr_scheduler', 'epoch', 'args'])\n"
     ]
    }
   ],
   "source": [
    "trained=torch.load(\"./Final_output/checkpoint0006.pth\", map_location='cpu')\n",
    "print(trained.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_layers 4\n"
     ]
    }
   ],
   "source": [
    "trained_model=trained[\"model\"]\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "missing_keys, unexpected_keys= model.load_state_dict(trained_model, strict=False)\n",
    "if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'boxes': tensor([[0.6815, 0.5112, 0.5504, 0.5798]]), 'labels': tensor([1]), 'image_id': tensor([8]), 'area': tensor([18604.1914]), 'iscrowd': tensor([False]), 'orig_size': tensor([166, 166]), 'size': tensor([600, 600]), 'path': 'JET_train_000\\\\07.JPEG'}]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(targets)\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Forward pass: compute predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to track gradients during validation\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for inputs, targets in data_loader_val:\n",
    "    print(targets)\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass: compute predictions\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        outputs = model(inputs)\n",
    "        print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "Test:  [   0/4410]  eta: 1:41:55  class_error: 0.00  loss: 4.9126 (4.9126)  loss_ce: 1.3114 (1.3114)  loss_bbox: 1.9806 (1.9806)  loss_giou: 1.6206 (1.6206)  loss_ce_unscaled: 0.6557 (0.6557)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.3961 (0.3961)  loss_giou_unscaled: 0.8103 (0.8103)  cardinality_error_unscaled: 29.0000 (29.0000)  loss_ce_0_unscaled: 1.0261 (1.0261)  loss_bbox_0_unscaled: 0.3510 (0.3510)  loss_giou_0_unscaled: 0.5951 (0.5951)  cardinality_error_0_unscaled: 79.0000 (79.0000)  loss_ce_1_unscaled: 1.3566 (1.3566)  loss_bbox_1_unscaled: 0.3510 (0.3510)  loss_giou_1_unscaled: 0.5951 (0.5951)  cardinality_error_1_unscaled: 49.0000 (49.0000)  time: 1.3867  data: 0.0090  max mem: 7872\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n",
      "dict_keys(['aux_outputs', 'pred_logits', 'pred_boxes'])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     18\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 145\u001b[0m, in \u001b[0;36mDeformableDETR.forward\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtwo_stage:\n\u001b[0;32m    144\u001b[0m     query_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_embed\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m--> 145\u001b[0m hs, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact, final_hs, final_references_out, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_embed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_class_embed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_bbox_embed_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m outputs_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    149\u001b[0m outputs_coords \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adrie\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 154\u001b[0m, in \u001b[0;36mDeformableTransformer.forward\u001b[1;34m(self, srcs, masks, pos_embeds, query_embed, class_embed, temp_class_embed_list, temp_bbox_embed_list)\u001b[0m\n\u001b[0;32m    152\u001b[0m mask_flatten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(mask_flatten, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    153\u001b[0m lvl_pos_embed_flatten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(lvl_pos_embed_flatten, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m spatial_shapes \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_flatten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m level_start_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((spatial_shapes\u001b[38;5;241m.\u001b[39mnew_zeros((\u001b[38;5;241m1\u001b[39m, )), spatial_shapes\u001b[38;5;241m.\u001b[39mprod(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m0\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    156\u001b[0m valid_ratios \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_valid_ratio(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m masks], \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "criterion.eval()\n",
    "\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "header = 'Test:'\n",
    "\n",
    "iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
    "\n",
    "for samples, targets  in metric_logger.log_every(data_loader_val, 50, header):\n",
    "    samples = samples.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items() if k!='path'} for t in targets[0]]\n",
    "\n",
    "    outputs = model(samples)\n",
    "    print(outputs.keys())\n",
    "    loss_dict = criterion(outputs, targets)\n",
    "    weight_dict = criterion.weight_dict\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = reduce_dict(loss_dict)\n",
    "    loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "    loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "    metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                        **loss_dict_reduced_scaled,\n",
    "                        **loss_dict_reduced_unscaled)\n",
    "    metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "\n",
    "    orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "    results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
    "    if 'segm' in postprocessors.keys():\n",
    "        target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
    "        results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
    "    res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.update(res)\n",
    "\n",
    "        \n",
    "# gather the stats from all processes\n",
    "metric_logger.synchronize_between_processes()\n",
    "print(\"Averaged stats:\", metric_logger)\n",
    "if coco_evaluator is not None:\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    \n",
    "\n",
    "# accumulate predictions from all images\n",
    "if coco_evaluator is not None:\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    \n",
    "stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "if coco_evaluator is not None:\n",
    "    if 'bbox' in postprocessors.keys():\n",
    "        stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, coco_evaluator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
