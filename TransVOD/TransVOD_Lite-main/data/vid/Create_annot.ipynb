{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is made to create annotation file in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "import mmcv\n",
    "from tqdm import tqdm\n",
    "\n",
    "CLASSES = ('airplane', 'antelope', 'bear', 'bicycle', 'bird', 'bus', 'car',\n",
    "           'cattle', 'dog', 'domestic_cat', 'elephant', 'fox', 'giant_panda',\n",
    "           'hamster', 'horse', 'lion', 'lizard', 'monkey', 'motorcycle',\n",
    "           'rabbit', 'red_panda', 'sheep', 'snake', 'squirrel', 'tiger',\n",
    "           'train', 'turtle', 'watercraft', 'whale', 'zebra')\n",
    "\n",
    "CLASSES_ENCODES = ('n02691156', 'n02419796', 'n02131653', 'n02834778',\n",
    "                   'n01503061', 'n02924116', 'n02958343', 'n02402425',\n",
    "                   'n02084071', 'n02121808', 'n02503517', 'n02118333',\n",
    "                   'n02510455', 'n02342885', 'n02374451', 'n02129165',\n",
    "                   'n01674464', 'n02484322', 'n03790512', 'n02324045',\n",
    "                   'n02509815', 'n02411705', 'n01726692', 'n02355227',\n",
    "                   'n02129604', 'n04468005', 'n01662784', 'n04530566',\n",
    "                   'n02062744', 'n02391049')\n",
    "\n",
    "cats_id_maps = {}\n",
    "for k, v in enumerate(CLASSES_ENCODES, 1):\n",
    "    cats_id_maps[v] = k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_train_list(ann_dir):\n",
    "    \"\"\"Parse the txt file of ImageNet VID train dataset.\"\"\"\n",
    "    img_list = osp.join(ann_dir, 'Lists/VID_train_15frames.txt')\n",
    "    print(img_list)\n",
    "    print(\"OK\")\n",
    "    img_list = mmcv.list_from_file(img_list)\n",
    "    train_infos = defaultdict(list)\n",
    "    for info in img_list:\n",
    "        info = info.split(' ')\n",
    "        if info[0] not in train_infos:\n",
    "            train_infos[info[0]] = dict(\n",
    "                vid_train_frames=[int(info[2]) - 1], num_frames=int(info[-1]))\n",
    "        else:\n",
    "            train_infos[info[0]]['vid_train_frames'].append(int(info[2]) - 1)\n",
    "    return train_infos\n",
    "\n",
    "\n",
    "def parse_val_list(ann_dir):\n",
    "    \"\"\"Parse the txt file of ImageNet VID val dataset.\"\"\"\n",
    "    img_list = osp.join(ann_dir, 'Lists/VID_val_videos.txt')\n",
    "    img_list = mmcv.list_from_file(img_list)\n",
    "    val_infos = defaultdict(list)\n",
    "    for info in img_list:\n",
    "        info = info.split(' ')\n",
    "        val_infos[info[0]] = dict(num_frames=int(info[-1]))\n",
    "    return val_infos\n",
    "\n",
    "\n",
    "def convert_vid(VID, ann_dir, save_dir, mode='train'):\n",
    "    \"\"\"Convert ImageNet VID dataset in COCO style.\n",
    "\n",
    "    Args:\n",
    "        VID (dict): The converted COCO style annotations.\n",
    "        ann_dir (str): The path of ImageNet VID dataset.\n",
    "        save_dir (str): The path to save `VID`.\n",
    "        mode (str): Convert train dataset or validation dataset. Options are\n",
    "            'train', 'val'. Default: 'train'.\n",
    "    \"\"\"\n",
    "    assert mode in ['train', 'val']\n",
    "    records = dict(\n",
    "        vid_id=1,\n",
    "        img_id=1,\n",
    "        ann_id=1,\n",
    "        global_instance_id=1,\n",
    "        num_vid_train_frames=0,\n",
    "        num_no_objects=0)\n",
    "    obj_num_classes = dict()\n",
    "    xml_dir = osp.join(ann_dir, 'Annotations/VID/')\n",
    "    if mode == 'train':\n",
    "        vid_infos = parse_train_list(ann_dir)\n",
    "    else:\n",
    "        vid_infos = parse_val_list(ann_dir)\n",
    "    for vid_info in tqdm(vid_infos):\n",
    "        instance_id_maps = dict()\n",
    "        vid_train_frames = vid_infos[vid_info].get('vid_train_frames', [])\n",
    "        records['num_vid_train_frames'] += len(vid_train_frames)\n",
    "        video = dict(\n",
    "            id=records['vid_id'],\n",
    "            name=vid_info,\n",
    "            vid_train_frames=vid_train_frames)\n",
    "        VID['videos'].append(video)\n",
    "        num_frames = vid_infos[vid_info]['num_frames']\n",
    "        for frame_id in range(num_frames):\n",
    "            is_vid_train_frame = True if frame_id in vid_train_frames \\\n",
    "                else False\n",
    "            img_prefix = osp.join(vid_info, '%06d' % frame_id)\n",
    "            xml_name = osp.join(xml_dir, f'{img_prefix}.xml')\n",
    "            # parse XML annotation file\n",
    "            tree = ET.parse(xml_name)\n",
    "            root = tree.getroot()\n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            image = dict(\n",
    "                file_name=f'{img_prefix}.JPEG',\n",
    "                height=height,\n",
    "                width=width,\n",
    "                id=records['img_id'],\n",
    "                frame_id=frame_id,\n",
    "                video_id=records['vid_id'],\n",
    "                is_vid_train_frame=is_vid_train_frame)\n",
    "            VID['images'].append(image)\n",
    "            if root.findall('object') == []:\n",
    "                print(xml_name, 'has no objects.')\n",
    "                records['num_no_objects'] += 1\n",
    "                records['img_id'] += 1\n",
    "                continue\n",
    "            for obj in root.findall('object'):\n",
    "                name = obj.find('name').text\n",
    "                if name not in cats_id_maps:\n",
    "                    continue\n",
    "                category_id = cats_id_maps[name]\n",
    "                bnd_box = obj.find('bndbox')\n",
    "                x1, y1, x2, y2 = [\n",
    "                    int(bnd_box.find('xmin').text),\n",
    "                    int(bnd_box.find('ymin').text),\n",
    "                    int(bnd_box.find('xmax').text),\n",
    "                    int(bnd_box.find('ymax').text)\n",
    "                ]\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                track_id = obj.find('trackid').text\n",
    "                if track_id in instance_id_maps:\n",
    "                    instance_id = instance_id_maps[track_id]\n",
    "                else:\n",
    "                    instance_id = records['global_instance_id']\n",
    "                    records['global_instance_id'] += 1\n",
    "                    instance_id_maps[track_id] = instance_id\n",
    "                occluded = obj.find('occluded').text\n",
    "                generated = obj.find('generated').text\n",
    "                ann = dict(\n",
    "                    id=records['ann_id'],\n",
    "                    video_id=records['vid_id'],\n",
    "                    image_id=records['img_id'],\n",
    "                    category_id=category_id,\n",
    "                    instance_id=instance_id,\n",
    "                    bbox=[x1, y1, w, h],\n",
    "                    area=w * h,\n",
    "                    iscrowd=False,\n",
    "                    occluded=occluded == '1',\n",
    "                    generated=generated == '1')\n",
    "                if category_id not in obj_num_classes:\n",
    "                    obj_num_classes[category_id] = 1\n",
    "                else:\n",
    "                    obj_num_classes[category_id] += 1\n",
    "                VID['annotations'].append(ann)\n",
    "                records['ann_id'] += 1\n",
    "            records['img_id'] += 1\n",
    "        records['vid_id'] += 1\n",
    "    if not osp.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    mmcv.dump(VID, osp.join(save_dir, f'imagenet_vid_{mode}.json'))\n",
    "    print(f'-----ImageNet VID {mode}------')\n",
    "    print(f'{records[\"vid_id\"]- 1} videos')\n",
    "    print(f'{records[\"img_id\"]- 1} images')\n",
    "    print(\n",
    "        f'{records[\"num_vid_train_frames\"]} train frames for video detection')\n",
    "    print(f'{records[\"num_no_objects\"]} images have no objects')\n",
    "    print(f'{records[\"ann_id\"] - 1} objects')\n",
    "    print('-----------------------')\n",
    "    for i in range(1, len(CLASSES) + 1):\n",
    "        print(f'Class {i} {CLASSES[i - 1]} has {obj_num_classes[i]} objects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./TransVOD/TransVOD_Lite-main/data/vid/Data/VID\\ILSVRC2015_train_00005003.mp4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m VID_train \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     26\u001b[0m VID_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m categories\n\u001b[1;32m---> 27\u001b[0m \u001b[43mconvert_vid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVID_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m VID_val \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     30\u001b[0m VID_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m categories\n",
      "Cell \u001b[1;32mIn[8], line 82\u001b[0m, in \u001b[0;36mconvert_vid\u001b[1;34m(VID, ann_dir, save_dir, mode)\u001b[0m\n\u001b[0;32m     80\u001b[0m xml_dir \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(ann_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotations/VID/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m     vid_infos \u001b[38;5;241m=\u001b[39m \u001b[43mparse_train_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     vid_infos \u001b[38;5;241m=\u001b[39m parse_val_list(ann_dir)\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mparse_train_list\u001b[1;34m(ann_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m info \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(info[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_infos:\n\u001b[0;32m     12\u001b[0m     train_infos[info[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     13\u001b[0m         vid_train_frames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mint\u001b[39m(info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], num_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(info[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#ImageNet VID to COCO Video format\n",
    "def parse_train_list(ann_dir):\n",
    "    \"\"\"Parse the txt file of ImageNet VID train dataset.\"\"\"\n",
    "    img_list = osp.join(ann_dir, 'Lists/VID_train_15frames.txt')\n",
    "    img_list = mmcv.list_from_file(img_list)\n",
    "    train_infos = defaultdict(list)\n",
    "    for info in img_list:\n",
    "        info = info.split(' ')\n",
    "        print(info[0])\n",
    "        print(info[1])\n",
    "        if info[0] not in train_infos:\n",
    "            train_infos[info[0]] = dict(\n",
    "                vid_train_frames=[int(info[2]) - 1], num_frames=int(info[-1]))\n",
    "        else:\n",
    "            train_infos[info[0]]['vid_train_frames'].append(int(info[2]) - 1)\n",
    "    return train_infos\n",
    "\n",
    "args=dict(\n",
    "input=\"\",#root directory of ImageNet VID annotations\n",
    "output=\"\")#directory to save coco formatted label file\n",
    "categories = []\n",
    "for k, v in enumerate(CLASSES, 1):\n",
    "    categories.append(\n",
    "        dict(id=k, name=v, encode_name=CLASSES_ENCODES[k - 1]))\n",
    "VID_train = defaultdict(list)\n",
    "VID_train['categories'] = categories\n",
    "convert_vid(VID_train, args[\"input\"], args[\"output\"], 'train')\n",
    "\n",
    "VID_val = defaultdict(list)\n",
    "VID_val['categories'] = categories\n",
    "convert_vid(VID_val, args[\"input\"], args[\"output\"], 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
